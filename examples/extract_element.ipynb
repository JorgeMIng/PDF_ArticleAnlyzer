{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Escritorio\\upm\\open_science\\PDF_ArticleAnlyzer\\examples\n",
      "e:\\Escritorio\\upm\\open_science\\PDF_ArticleAnlyzer\n"
     ]
    }
   ],
   "source": [
    "# add the local libraries to the path\n",
    "\n",
    "## in case it has been added with setup.py this cell doenst need to be executed\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "print(os.getcwd())\n",
    "father_path = Path(os.path.dirname(os.getcwd()))\n",
    "print(father_path)\n",
    "sys.path.append(os.path.join(father_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                       Version\n",
      "----------------------------- ---------------\n",
      "absl-py                       2.0.0\n",
      "accelerate                    0.24.1\n",
      "aiobotocore                   2.5.0\n",
      "aiofiles                      22.1.0\n",
      "aiohttp                       3.8.5\n",
      "aioitertools                  0.7.1\n",
      "aiosignal                     1.2.0\n",
      "aiosqlite                     0.18.0\n",
      "alabaster                     0.7.12\n",
      "altair                        5.1.1\n",
      "anaconda-anon-usage           0.4.3\n",
      "anaconda-catalogs             0.2.0\n",
      "anaconda-client               1.12.1\n",
      "anaconda-project              0.11.1\n",
      "annotated-types               0.6.0\n",
      "antlr4-python3-runtime        4.9.3\n",
      "anyascii                      0.3.2\n",
      "anyio                         3.5.0\n",
      "appdirs                       1.4.4\n",
      "argon2-cffi                   21.3.0\n",
      "argon2-cffi-bindings          21.2.0\n",
      "arrow                         1.2.3\n",
      "astroid                       2.14.2\n",
      "astropy                       5.3.4\n",
      "asttokens                     2.0.5\n",
      "async-timeout                 4.0.2\n",
      "atomicwrites                  1.4.0\n",
      "attrs                         23.1.0\n",
      "Automat                       20.2.0\n",
      "autopep8                      1.6.0\n",
      "Babel                         2.11.0\n",
      "backcall                      0.2.0\n",
      "backports.functools-lru-cache 1.6.4\n",
      "backports.tempfile            1.0\n",
      "backports.weakref             1.0.post1\n",
      "bcrypt                        3.2.0\n",
      "beautifulsoup4                4.9.1\n",
      "binaryornot                   0.4.4\n",
      "black                         0.0\n",
      "bleach                        4.1.0\n",
      "blinker                       1.6.2\n",
      "blis                          0.7.9\n",
      "bokeh                         3.2.1\n",
      "boltons                       23.0.0\n",
      "boto3                         1.26.76\n",
      "botocore                      1.29.76\n",
      "Bottleneck                    1.3.5\n",
      "brotlipy                      0.7.0\n",
      "bs4                           0.0.2\n",
      "build                         0.5.0\n",
      "cachetools                    5.3.1\n",
      "catalogue                     2.0.7\n",
      "certifi                       2023.7.22\n",
      "cffi                          1.15.1\n",
      "chardet                       4.0.0\n",
      "charset-normalizer            2.0.4\n",
      "click                         7.1.2\n",
      "cloudpickle                   2.2.1\n",
      "clyent                        1.2.2\n",
      "colorama                      0.4.6\n",
      "colorcet                      3.0.1\n",
      "comm                          0.1.2\n",
      "conda                         23.9.0\n",
      "conda-build                   3.27.0\n",
      "conda-content-trust           0.2.0\n",
      "conda_index                   0.3.0\n",
      "conda-libmamba-solver         23.9.2\n",
      "conda-pack                    0.6.0\n",
      "conda-package-handling        2.2.0\n",
      "conda_package_streaming       0.9.0\n",
      "conda-token                   0.4.0\n",
      "conda-verify                  3.4.2\n",
      "confection                    0.0.4\n",
      "constantly                    15.1.0\n",
      "contourpy                     1.0.5\n",
      "contractions                  0.1.73\n",
      "cookiecutter                  1.7.3\n",
      "cryptography                  41.0.3\n",
      "cssselect                     1.1.0\n",
      "cupy-cuda12x                  12.2.0\n",
      "cycler                        0.11.0\n",
      "cymem                         2.0.6\n",
      "cytoolz                       0.12.0\n",
      "daal4py                       2023.1.1\n",
      "dask                          2023.6.0\n",
      "dataclasses-json              0.6.1\n",
      "datashader                    0.15.2\n",
      "datashape                     0.5.4\n",
      "debugpy                       1.6.7\n",
      "decorator                     5.1.1\n",
      "defusedxml                    0.7.1\n",
      "diff-match-patch              20200713\n",
      "dill                          0.3.7\n",
      "distlib                       0.3.8\n",
      "distributed                   2023.6.0\n",
      "docstring-to-markdown         0.11\n",
      "docutils                      0.18.1\n",
      "dominate                      2.5.1\n",
      "duckdb                        0.7.1\n",
      "elementpath                   4.1.5\n",
      "en-core-web-lg                3.5.0\n",
      "en-core-web-md                3.5.0\n",
      "en-core-web-sm                3.5.0\n",
      "entrypoints                   0.4\n",
      "et-xmlfile                    1.1.0\n",
      "executing                     0.8.3\n",
      "Faker                         19.4.0\n",
      "falcon                        3.1.1\n",
      "fastjsonschema                2.16.2\n",
      "fastrlock                     0.8.2\n",
      "favicon                       0.7.0\n",
      "filelock                      3.13.1\n",
      "flake8                        6.0.0\n",
      "Flask                         1.1.2\n",
      "Flask-Cors                    3.0.10\n",
      "Flask-Script                  2.0.6\n",
      "Flask-SQLAlchemy              2.5.1\n",
      "Flask-Uploads                 0.2.1\n",
      "fonttools                     4.25.0\n",
      "frozendict                    2.3.8\n",
      "frozenlist                    1.3.3\n",
      "fsspec                        2023.10.0\n",
      "fst-pso                       1.8.1\n",
      "future                        0.18.3\n",
      "FuzzyTM                       2.0.5\n",
      "gensim                        4.3.0\n",
      "gh                            0.0.4\n",
      "gitdb                         4.0.10\n",
      "GitPython                     3.1.35\n",
      "glob2                         0.7\n",
      "google-auth                   2.23.3\n",
      "google-auth-oauthlib          1.1.0\n",
      "graphviz                      0.20.1\n",
      "greenlet                      2.0.1\n",
      "grpcio                        1.59.0\n",
      "gsitk                         0.2.5\n",
      "h5py                          3.9.0\n",
      "HeapDict                      1.0.1\n",
      "holoviews                     1.17.1\n",
      "htbuilder                     0.6.2\n",
      "huggingface                   0.0.1\n",
      "huggingface-hub               0.17.3\n",
      "hvplot                        0.9.0\n",
      "hyperlink                     21.0.0\n",
      "idna                          3.4\n",
      "imagecodecs                   2023.1.23\n",
      "imageio                       2.31.4\n",
      "imagesize                     1.4.1\n",
      "imbalanced-learn              0.10.1\n",
      "importlib-metadata            6.0.0\n",
      "importlib-resources           6.1.0\n",
      "incremental                   21.3.0\n",
      "inexactsearch                 1.0.2\n",
      "inflection                    0.5.1\n",
      "iniconfig                     1.1.1\n",
      "inquirerpy                    0.3.4\n",
      "intake                        0.6.8\n",
      "intervaltree                  3.1.0\n",
      "ipykernel                     6.25.0\n",
      "ipython                       8.15.0\n",
      "ipython-genutils              0.2.0\n",
      "ipywidgets                    8.0.4\n",
      "isodate                       0.6.0\n",
      "isort                         5.9.3\n",
      "itemadapter                   0.3.0\n",
      "itemloaders                   1.0.4\n",
      "itsdangerous                  1.1.0\n",
      "jaraco.classes                3.2.1\n",
      "jedi                          0.18.1\n",
      "jellyfish                     1.0.1\n",
      "Jinja2                        3.1.3\n",
      "jinja2-time                   0.2.0\n",
      "jmespath                      0.10.0\n",
      "joblib                        1.2.0\n",
      "json-normalize                1.0.1\n",
      "json5                         0.9.6\n",
      "jsonpatch                     1.33\n",
      "jsonpath-python               1.0.6\n",
      "jsonpointer                   2.1\n",
      "jsonref                       1.1.0\n",
      "jsonschema                    4.17.3\n",
      "jupyter                       1.0.0\n",
      "jupyter_client                7.4.9\n",
      "jupyter-console               6.6.3\n",
      "jupyter_core                  5.3.0\n",
      "jupyter-events                0.6.3\n",
      "jupyter-server                1.23.4\n",
      "jupyter_server_fileid         0.9.0\n",
      "jupyter_server_ydoc           0.8.0\n",
      "jupyter-ydoc                  0.2.4\n",
      "jupyterlab                    3.6.3\n",
      "jupyterlab-pygments           0.1.2\n",
      "jupyterlab_server             2.22.0\n",
      "jupyterlab-widgets            3.0.5\n",
      "kaleido                       0.2.1\n",
      "keyring                       23.13.1\n",
      "kiwisolver                    1.4.4\n",
      "langchain                     0.0.331\n",
      "langcodes                     3.3.0\n",
      "langsmith                     0.0.60\n",
      "LatLon                        1.0.2\n",
      "latlon3                       1.0.4\n",
      "lazy_loader                   0.3\n",
      "lazy-object-proxy             1.6.0\n",
      "Levenshtein                   0.21.1\n",
      "libarchive-c                  2.9\n",
      "libmambapy                    1.5.1\n",
      "lime                          0.2.0.1\n",
      "linkify-it-py                 2.0.0\n",
      "llvmlite                      0.41.0\n",
      "lmdb                          1.4.1\n",
      "locket                        1.0.0\n",
      "lxml                          4.9.3\n",
      "lz4                           4.3.2\n",
      "Markdown                      3.4.1\n",
      "markdown-it-py                3.0.0\n",
      "markdownlit                   0.0.7\n",
      "MarkupSafe                    2.1.5\n",
      "marshmallow                   3.20.1\n",
      "matplotlib                    3.7.2\n",
      "matplotlib-inline             0.1.6\n",
      "mccabe                        0.7.0\n",
      "mdit-py-plugins               0.4.0\n",
      "mdurl                         0.1.0\n",
      "menuinst                      1.4.19\n",
      "miniful                       0.0.6\n",
      "mistune                       2.0.5\n",
      "mkl-fft                       1.3.8\n",
      "mkl-random                    1.2.4\n",
      "mkl-service                   2.4.0\n",
      "more-itertools                8.12.0\n",
      "morph_kgc                     2.6.3\n",
      "mpmath                        1.3.0\n",
      "msgpack                       1.0.3\n",
      "multidict                     6.0.2\n",
      "multipledispatch              0.6.0\n",
      "munkres                       1.1.4\n",
      "murmurhash                    1.0.7\n",
      "mypy-extensions               1.0.0\n",
      "myst-parser                   2.0.0\n",
      "names-dataset                 2.1.0\n",
      "nbclassic                     0.5.5\n",
      "nbclient                      0.5.13\n",
      "nbconvert                     6.5.4\n",
      "nbformat                      5.9.2\n",
      "nest-asyncio                  1.5.6\n",
      "networkx                      3.1\n",
      "nltk                          3.8.1\n",
      "notebook                      6.5.4\n",
      "notebook_shim                 0.2.2\n",
      "numba                         0.58.0\n",
      "numexpr                       2.8.7\n",
      "numpy                         1.24.3\n",
      "numpydoc                      1.5.0\n",
      "oauthlib                      3.2.2\n",
      "omegaconf                     2.3.0\n",
      "opencv-python                 4.9.0.80\n",
      "openpyxl                      3.0.10\n",
      "packaging                     23.2\n",
      "pandas                        1.5.2\n",
      "pandocfilters                 1.5.0\n",
      "panel                         1.2.3\n",
      "param                         1.13.0\n",
      "paramiko                      2.8.1\n",
      "parsel                        1.6.0\n",
      "parso                         0.8.3\n",
      "partd                         1.4.0\n",
      "pathspec                      0.10.3\n",
      "pathy                         0.10.1\n",
      "patsy                         0.5.3\n",
      "pep517                        0.10.0\n",
      "pep8                          1.7.1\n",
      "pexpect                       4.8.0\n",
      "pfzy                          0.3.4\n",
      "pickleshare                   0.7.5\n",
      "Pillow                        9.5.0\n",
      "pip                           23.3\n",
      "pkginfo                       1.9.6\n",
      "platformdirs                  3.10.0\n",
      "plotly                        5.9.0\n",
      "pluggy                        1.0.0\n",
      "ply                           3.11\n",
      "poyo                          0.5.0\n",
      "preshed                       3.0.6\n",
      "prettytable                   3.9.0\n",
      "prometheus-client             0.14.1\n",
      "prompt-toolkit                3.0.36\n",
      "Protego                       0.1.16\n",
      "protobuf                      4.23.4\n",
      "psutil                        5.9.0\n",
      "psycopg2                      2.9.5\n",
      "ptyprocess                    0.7.0\n",
      "pure-eval                     0.2.2\n",
      "py-cpuinfo                    9.0.0\n",
      "pyahocorasick                 2.0.0\n",
      "pyarrow                       11.0.0\n",
      "pyasn1                        0.4.8\n",
      "pyasn1-modules                0.2.8\n",
      "pycodestyle                   2.10.0\n",
      "pycosat                       0.6.6\n",
      "pycparser                     2.21\n",
      "pyct                          0.5.0\n",
      "pycurl                        7.45.2\n",
      "pydantic                      1.10.13\n",
      "pydantic_core                 2.10.1\n",
      "pydantic-extra-types          2.1.0\n",
      "pydantic-secret-decimal       0.1.2\n",
      "pydeck                        0.8.0\n",
      "PyDispatcher                  2.0.5\n",
      "pydocstyle                    6.3.0\n",
      "pyecharts                     2.0.4\n",
      "pyerfa                        2.0.0\n",
      "pyflakes                      3.0.1\n",
      "pyFUME                        0.2.25\n",
      "pygame                        2.5.2\n",
      "Pygments                      2.15.1\n",
      "PyLD                          2.0.3\n",
      "pylint                        2.16.2\n",
      "pylint-venv                   2.3.0\n",
      "pyls-spyder                   0.4.0\n",
      "pymdown-extensions            10.3\n",
      "Pympler                       1.0.1\n",
      "PyNaCl                        1.5.0\n",
      "pyodbc                        4.0.39\n",
      "pyOpenSSL                     23.2.0\n",
      "pyoxigraph                    0.3.19\n",
      "pyparsing                     2.4.7\n",
      "pyproj                        3.6.1\n",
      "PyQt5                         5.15.7\n",
      "PyQt5-sip                     12.11.0\n",
      "PyQtWebEngine                 5.15.4\n",
      "pyrsistent                    0.18.0\n",
      "PySocks                       1.7.1\n",
      "pyspellchecker                0.7.2\n",
      "pytest                        7.4.0\n",
      "python-dateutil               2.8.1\n",
      "python-decouple               3.8\n",
      "python-json-logger            2.0.7\n",
      "python-Levenshtein            0.21.1\n",
      "python-lsp-black              1.2.1\n",
      "python-lsp-jsonrpc            1.0.0\n",
      "python-lsp-server             1.7.2\n",
      "python-slugify                5.0.2\n",
      "python-snappy                 0.6.1\n",
      "pytoolconfig                  1.2.5\n",
      "pytreebank                    0.2.7\n",
      "pytz                          2020.1\n",
      "pytz-deprecation-shim         0.1.0.post0\n",
      "pyviz-comms                   2.3.0\n",
      "PyWavelets                    1.4.1\n",
      "pywin32                       305.1\n",
      "pywin32-ctypes                0.2.0\n",
      "pywinpty                      2.0.10\n",
      "PyYAML                        6.0.1\n",
      "pyzmq                         23.2.0\n",
      "QDarkStyle                    3.0.2\n",
      "qstylizer                     0.2.2\n",
      "QtAwesome                     1.2.2\n",
      "qtconsole                     5.4.2\n",
      "QtPy                          2.2.0\n",
      "queuelib                      1.6.2\n",
      "rapidfuzz                     3.2.0\n",
      "rdflib                        5.0.0\n",
      "readability                   0.3.1\n",
      "regex                         2023.10.3\n",
      "requests                      2.28.2\n",
      "requests-file                 1.5.1\n",
      "requests-oauthlib             1.3.1\n",
      "requests-toolbelt             1.0.0\n",
      "responses                     0.23.3\n",
      "rfc3339-validator             0.1.4\n",
      "rfc3986-validator             0.1.1\n",
      "rich                          13.5.2\n",
      "rope                          1.7.0\n",
      "rsa                           4.9\n",
      "Rtree                         1.0.1\n",
      "ruamel.yaml                   0.17.21\n",
      "ruamel-yaml-conda             0.17.21\n",
      "s3fs                          2023.4.0\n",
      "s3transfer                    0.6.0\n",
      "sacremoses                    0.0.43\n",
      "safetensors                   0.4.0\n",
      "scikit-image                  0.20.0\n",
      "scikit-learn                  1.3.2\n",
      "scikit-learn-intelex          20230426.121932\n",
      "scipy                         1.11.3\n",
      "Scrapy                        2.8.0\n",
      "seaborn                       0.12.2\n",
      "Send2Trash                    1.8.0\n",
      "senpy                         1.0.6\n",
      "sentence-transformers         2.2.2\n",
      "sentencepiece                 0.1.99\n",
      "service-identity              18.1.0\n",
      "setuptools                    68.0.0\n",
      "shap                          0.42.1\n",
      "shellingham                   1.5.0\n",
      "silpa-common                  0.3\n",
      "simpful                       2.11.0\n",
      "simplejson                    3.19.1\n",
      "sip                           6.6.2\n",
      "six                           1.14.0\n",
      "slicer                        0.0.7\n",
      "smart-open                    5.2.1\n",
      "smmap                         5.0.0\n",
      "sniffio                       1.2.0\n",
      "snowballstemmer               2.2.0\n",
      "sortedcontainers              2.4.0\n",
      "soundex                       1.1.3\n",
      "soupsieve                     2.0.1\n",
      "spacy                         3.5.3\n",
      "spacy-legacy                  3.0.12\n",
      "spacy-loggers                 1.0.4\n",
      "spellchecker                  0.4\n",
      "Sphinx                        7.2.6\n",
      "sphinx_mdinclude              0.5.3\n",
      "sphinx-rtd-theme              2.0.0\n",
      "sphinxcontrib-applehelp       1.0.2\n",
      "sphinxcontrib-devhelp         1.0.2\n",
      "sphinxcontrib-htmlhelp        2.0.0\n",
      "sphinxcontrib-jquery          4.1\n",
      "sphinxcontrib-jsmath          1.0.1\n",
      "sphinxcontrib-qthelp          1.0.3\n",
      "sphinxcontrib-serializinghtml 1.1.10\n",
      "spyder                        5.4.3\n",
      "spyder-kernels                2.4.4\n",
      "sql_metadata                  2.9.0\n",
      "SQLAlchemy                    1.4.46\n",
      "sqlparse                      0.4.4\n",
      "srsly                         2.4.6\n",
      "st-annotated-text             4.0.1\n",
      "stack-data                    0.2.0\n",
      "statsmodels                   0.14.0\n",
      "streamlit                     1.28.0\n",
      "streamlit-aggrid              0.3.4.post3\n",
      "streamlit-awesome-table       0.1.0\n",
      "streamlit-camera-input-live   0.2.0\n",
      "streamlit-card                0.0.61\n",
      "streamlit-echarts             0.4.0\n",
      "streamlit-elements            0.1.0\n",
      "streamlit-embedcode           0.1.2\n",
      "streamlit-extras              0.3.1\n",
      "streamlit-faker               0.0.2\n",
      "streamlit-image-coordinates   0.1.6\n",
      "streamlit-keyup               0.2.0\n",
      "streamlit-lottie              0.0.5\n",
      "streamlit-plotly-events       0.0.6\n",
      "streamlit-pydantic            0.6.0\n",
      "streamlit-toggle-switch       1.0.2\n",
      "streamlit-vertical-slider     1.0.2\n",
      "sympy                         1.11.1\n",
      "syntok                        1.4.4\n",
      "tables                        3.8.0\n",
      "tabulate                      0.8.10\n",
      "TBB                           0.2\n",
      "tblib                         1.7.0\n",
      "tenacity                      8.2.2\n",
      "tensorboard                   2.15.0\n",
      "tensorboard-data-server       0.7.2\n",
      "terminado                     0.17.1\n",
      "text-preprocessing            0.1.1\n",
      "text-unidecode                1.3\n",
      "textdistance                  4.2.1\n",
      "textsearch                    0.0.24\n",
      "thinc                         8.1.10\n",
      "threadpoolctl                 2.2.0\n",
      "three-merge                   0.1.1\n",
      "tifffile                      2023.4.12\n",
      "tinycss2                      1.2.1\n",
      "tldextract                    3.2.0\n",
      "tokenizers                    0.14.1\n",
      "toml                          0.10.2\n",
      "tomlkit                       0.11.1\n",
      "toolz                         0.12.0\n",
      "torch                         2.1.0\n",
      "torchaudio                    2.1.0\n",
      "torchsummary                  1.5.1\n",
      "torchvision                   0.16.0\n",
      "torchviz                      0.0.2\n",
      "tornado                       6.3.3\n",
      "tqdm                          4.65.0\n",
      "traitlets                     5.7.1\n",
      "transformers                  4.34.1\n",
      "truststore                    0.8.0\n",
      "Twisted                       22.10.0\n",
      "twisted-iocpsupport           1.0.2\n",
      "typer                         0.4.1\n",
      "types-PyYAML                  6.0.12.11\n",
      "typing_extensions             4.7.1\n",
      "typing-inspect                0.9.0\n",
      "tzdata                        2023.3\n",
      "tzlocal                       4.3.1\n",
      "uc-micro-py                   1.0.1\n",
      "ujson                         5.4.0\n",
      "Unidecode                     1.2.0\n",
      "unittest-xml-reporting        3.2.0\n",
      "urllib3                       1.26.16\n",
      "validators                    0.22.0\n",
      "virtualenv                    20.25.0\n",
      "visitor                       0.1.3\n",
      "w3lib                         1.21.0\n",
      "wasabi                        0.9.1\n",
      "watchdog                      2.1.6\n",
      "wcwidth                       0.2.5\n",
      "webencodings                  0.5.1\n",
      "websocket-client              0.58.0\n",
      "Werkzeug                      0.16.1\n",
      "wget                          3.2\n",
      "whatthepatch                  1.0.2\n",
      "wheel                         0.41.2\n",
      "widgetsnbextension            4.0.5\n",
      "win-inet-pton                 1.1.0\n",
      "wordcloud                     1.9.3\n",
      "wrapt                         1.14.1\n",
      "xarray                        2023.6.0\n",
      "xgboost                       2.0.0\n",
      "xlwings                       0.29.1\n",
      "xyzservices                   2022.9.0\n",
      "y-py                          0.5.9\n",
      "yapf                          0.31.0\n",
      "yarl                          1.8.1\n",
      "ypy-websocket                 0.8.2\n",
      "zict                          3.0.0\n",
      "zipp                          3.11.0\n",
      "zope.interface                5.4.0\n",
      "zstandard                     0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "from pdf_analyzer.api.extract.elements import extract_elements\n",
    "from pdf_analyzer.config_load import load_config\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVER_CONFIG\n",
      "url:\n",
      "  protocol: http\n",
      "  api_domain: yordi111nas.synology.me\n",
      "  port: 8070\n",
      "\n",
      "CLOUD_CONFIG\n",
      "data:\n",
      "  data_dir: data/PDFs\n",
      "  format: .pdf\n",
      "  recursive: true\n",
      "grobid:\n",
      "  cache: true\n",
      "  cache_dir: data/xmls\n",
      "  operation_key: processFulltextDocument\n",
      "  format: .grobid.tei.xml\n",
      "  recursive: true\n",
      "extract:\n",
      "  dir: null\n",
      "  element: null\n",
      "  type: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "server_config = load_config(\"config/api/grobid-server-config.yaml\")\n",
    "extract_config = load_config(\"config/api/extract_element.yaml\")\n",
    "print(\"SERVER_CONFIG\\n\"+OmegaConf.to_yaml(server_config))\n",
    "print(\"CLOUD_CONFIG\\n\"+OmegaConf.to_yaml(extract_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_config.grobid.recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the config files before starting or here with code\n",
    " For example\n",
    " \n",
    " server_config.protocol = https   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://yordi111nas.synology.me:8070/api/isalive\n",
      "GROBID server is up and running\n",
      "data/xmls\\nlp\\Bert.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\DistillBERT.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\Dont_stop_pretraining.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\GPT-3.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\LIME.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\LoRA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\RoBERTa.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\SORA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\Transformers.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\word2vec.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "grobid_logger: 2024-04-27 17:40:28,806 | INFO | API.py:44 | 19024 >>> All files have been process by the api\n",
      "grobid_logger: 2024-04-27 17:40:28,924 | INFO | elements.py:69 | 19024 >>> All elements of of the xml_file Bert have been extracted\n",
      "grobid_logger: 2024-04-27 17:40:28,953 | INFO | elements.py:69 | 19024 >>> All elements of of the xml_file DistillBERT have been extracted\n",
      "grobid_logger: 2024-04-27 17:40:29,083 | INFO | elements.py:69 | 19024 >>> All elements of of the xml_file Dont_stop_pretraining have been extracted\n",
      "grobid_logger: 2024-04-27 17:40:29,279 | INFO | elements.py:69 | 19024 >>> All elements of of the xml_file GPT-3 have been extracted\n",
      "grobid_logger: 2024-04-27 17:40:29,318 | INFO | elements.py:69 | 19024 >>> All elements of of the xml_file LIME have been extracted\n",
      "grobid_logger: 2024-04-27 17:40:29,404 | INFO | elements.py:69 | 19024 >>> All elements of of the xml_file LoRA have been extracted\n",
      "grobid_logger: 2024-04-27 17:40:29,544 | INFO | elements.py:69 | 19024 >>> All elements of of the xml_file RoBERTa have been extracted\n",
      "grobid_logger: 2024-04-27 17:40:29,813 | INFO | elements.py:69 | 19024 >>> All elements of of the xml_file SORA have been extracted\n",
      "grobid_logger: 2024-04-27 17:40:29,866 | INFO | elements.py:69 | 19024 >>> All elements of of the xml_file Transformers have been extracted\n",
      "grobid_logger: 2024-04-27 17:40:29,915 | INFO | elements.py:69 | 19024 >>> All elements of of the xml_file word2vec have been extracted\n"
     ]
    }
   ],
   "source": [
    "extract_config.extract.dir = \"teiHeader.fileDesc.sourceDesc.biblStruct.analytic\"\n",
    "extract_config.extract.element = \"author\"\n",
    "authors = extract_elements(extract_config,server_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing elements author in paper Bert\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Jacob</forename><surname>Devlin</surname></persName>\n",
      "<email>jacobdevlin@google.com</email>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Ming-Wei</forename><surname>Chang</surname></persName>\n",
      "<email>mingweichang@google.com</email>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n",
      "<email>kentonl@google.com</email>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Kristina</forename><surname>Toutanova</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Showing elements author in paper DistillBERT\n",
      "\n",
      "\n",
      "<author role=\"corresp\">\n",
      "<persName><forename type=\"first\">Victor</forename><surname>Sanh</surname></persName>\n",
      "<email>victor@huggingface.co</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"department\">Lysandre DEBUT</orgName>\n",
      "<orgName key=\"instit1\" type=\"institution\">Julien CHAUMOND</orgName>\n",
      "<orgName key=\"instit2\" type=\"institution\">Thomas WOLF Hugging Face</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Showing elements author in paper Dont_stop_pretraining\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Suchin</forename><surname>Gururangan</surname></persName>\n",
      "<email>suching@allenai.org</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
      "<address>\n",
      "<settlement>Seattle</settlement>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Ana</forename><surname>MarasoviÄ‡</surname></persName>\n",
      "<email>anam@allenai.org</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
      "<address>\n",
      "<settlement>Seattle</settlement>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "<affiliation key=\"aff1\">\n",
      "<orgName type=\"department\">Paul G</orgName>\n",
      "<orgName key=\"instit1\" type=\"institution\">Allen School of Computer Science &amp; Engineering</orgName>\n",
      "<orgName key=\"instit2\" type=\"institution\">University of Washington</orgName>\n",
      "<address>\n",
      "<settlement>Seattle</settlement>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Swabha</forename><surname>Swayamdipta</surname></persName>\n",
      "<email>swabhas@allenai.org</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
      "<address>\n",
      "<settlement>Seattle</settlement>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Kyle</forename><surname>Lo</surname></persName>\n",
      "<email>kylel@allenai.org</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
      "<address>\n",
      "<settlement>Seattle</settlement>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Iz</forename><surname>Beltagy</surname></persName>\n",
      "<email>beltagy@allenai.org</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
      "<address>\n",
      "<settlement>Seattle</settlement>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Doug</forename><surname>Downey</surname></persName>\n",
      "<email>dougd@allenai.org</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
      "<address>\n",
      "<settlement>Seattle</settlement>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Noah</forename><forename type=\"middle\">A</forename><surname>Smith</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
      "<address>\n",
      "<settlement>Seattle</settlement>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "<affiliation key=\"aff1\">\n",
      "<orgName type=\"department\">Paul G</orgName>\n",
      "<orgName key=\"instit1\" type=\"institution\">Allen School of Computer Science &amp; Engineering</orgName>\n",
      "<orgName key=\"instit2\" type=\"institution\">University of Washington</orgName>\n",
      "<address>\n",
      "<settlement>Seattle</settlement>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Showing elements author in paper GPT-3\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Tom</forename><forename type=\"middle\">B</forename><surname>Brown</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Benjamin</forename><surname>Mann</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Nick</forename><surname>Ryder</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Melanie</forename><surname>Subbiah</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Jared</forename><surname>Kaplan</surname></persName>\n",
      "<affiliation key=\"aff1\">\n",
      "<orgName type=\"institution\">Johns Hopkins University</orgName>\n",
      "<address>\n",
      "<settlement>OpenAI</settlement>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Prafulla</forename><surname>Dhariwal</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Arvind</forename><surname>Neelakantan</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Pranav</forename><surname>Shyam</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Girish</forename><surname>Sastry</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Amanda</forename><surname>Askell</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Sandhini</forename><surname>Agarwal</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Ariel</forename><surname>Herbert-Voss</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Gretchen</forename><surname>Krueger</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Tom</forename><surname>Henighan</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Rewon</forename><surname>Child</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Aditya</forename><surname>Ramesh</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Daniel</forename><forename type=\"middle\">M</forename><surname>Ziegler</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Jeffrey</forename><surname>Wu</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Christopher</forename><surname>Hesse</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Mark</forename><surname>Chen</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Eric</forename><surname>Sigler</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Mateusz</forename><surname>Litwin</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Scott</forename><surname>Gray</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Benjamin</forename><surname>Chess</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Jack</forename><surname>Clark</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Christopher</forename><surname>Berner</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Sam</forename><surname>Mccandlish</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Alec</forename><surname>Radford</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Ilya</forename><surname>Sutskever</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Dario</forename><forename type=\"middle\">Amodei</forename><surname>Openai</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"department\">Clemens Winter</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Showing elements author in paper LIME\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Marco</forename><forename type=\"middle\">Tulio</forename><surname>Ribeiro</surname></persName>\n",
      "<email>marcotcr@cs.uw.edu</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">University of Washington Seattle</orgName>\n",
      "<address>\n",
      "<postCode>98105</postCode>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Sameer</forename><surname>Singh</surname></persName>\n",
      "<email>sameer@cs.uw.edu</email>\n",
      "<affiliation key=\"aff1\">\n",
      "<orgName type=\"institution\">University of Washington Seattle</orgName>\n",
      "<address>\n",
      "<postCode>98105</postCode>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Carlos</forename><surname>Guestrin</surname></persName>\n",
      "<email>guestrin@cs.uw.edu</email>\n",
      "<affiliation key=\"aff2\">\n",
      "<orgName type=\"institution\">University of Washington Seattle</orgName>\n",
      "<address>\n",
      "<postCode>98105</postCode>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Showing elements author in paper LoRA\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Edward</forename><surname>Hu</surname></persName>\n",
      "<email>edwardhu@microsoft.com</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Microsoft Corporation</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Yelong</forename><surname>Shen</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Microsoft Corporation</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Phillip</forename><surname>Wallis</surname></persName>\n",
      "<email>phwallis@microsoft.com</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Microsoft Corporation</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Zeyuan</forename><surname>Allen-Zhu</surname></persName>\n",
      "<email>zeyuana@microsoft.com</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Microsoft Corporation</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Yuanzhi</forename><surname>Li</surname></persName>\n",
      "<email>yuanzhil@microsoft.com</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Microsoft Corporation</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Shean</forename><surname>Wang</surname></persName>\n",
      "<email>swang@microsoft.com</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Microsoft Corporation</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Lu</forename><surname>Wang</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Microsoft Corporation</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Weizhu</forename><surname>Chen</surname></persName>\n",
      "<email>wzchen@microsoft.com</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Microsoft Corporation</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Showing elements author in paper RoBERTa\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Zhuang</forename><surname>Liu</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Wayne</forename><surname>Lin</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Ya</forename><surname>Shi</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Jun</forename><surname>Zhao</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Showing elements author in paper SORA\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Yixin</forename><surname>Liu</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Kai</forename><surname>Zhang</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Yuan</forename><surname>Li</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Zhiling</forename><surname>Yan</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Chujie</forename><surname>Gao</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Ruoxi</forename><surname>Chen</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Zhengqing</forename><surname>Yuan</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Yue</forename><surname>Huang</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Hanchi</forename><surname>Sun</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Jianfeng</forename><surname>Gao</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "<affiliation key=\"aff1\">\n",
      "<orgName type=\"department\">Microsoft Research</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Lifang</forename><surname>He</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Lichao</forename><surname>Sun</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Showing elements author in paper Transformers\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Ashish</forename><surname>Vaswani</surname></persName>\n",
      "<email>avaswani@google.com</email>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Noam</forename><surname>Shazeer</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Google</forename><surname>Brain</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Niki</forename><surname>Parmar</surname></persName>\n",
      "<email>nikip@google.com</email>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Jakob</forename><surname>Uszkoreit</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Llion</forename><surname>Jones</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Aidan</forename><forename type=\"middle\">N</forename><surname>Gomez</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Åukasz</forename><surname>Kaiser</surname></persName>\n",
      "<email>lukaszkaiser@google.com</email>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"department\">Google Brain</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<affiliation key=\"aff1\">\n",
      "<orgName type=\"department\">Google Research</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<affiliation key=\"aff2\">\n",
      "<orgName type=\"department\">Google Research</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<affiliation key=\"aff3\">\n",
      "<orgName type=\"department\">Google Research</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<affiliation key=\"aff4\">\n",
      "<orgName type=\"institution\">University of Toronto</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<affiliation key=\"aff5\">\n",
      "<orgName type=\"department\">Google Brain</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<affiliation key=\"aff6\">\n",
      "<orgName type=\"laboratory\">31st Conference on Neural Information Processing Systems (NIPS 2017)</orgName>\n",
      "<address>\n",
      "<settlement>Long Beach</settlement>\n",
      "<region>CA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Showing elements author in paper word2vec\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Tomas</forename><surname>Mikolov</surname></persName>\n",
      "<email>tmikolov@google.com</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Google Inc</orgName>\n",
      "<address>\n",
      "<settlement>Mountain View</settlement>\n",
      "<region>CA</region>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Kai</forename><surname>Chen</surname></persName>\n",
      "<email>kaichen@google.com</email>\n",
      "<affiliation key=\"aff1\">\n",
      "<orgName type=\"institution\">Google Inc</orgName>\n",
      "<address>\n",
      "<settlement>Mountain View</settlement>\n",
      "<region>CA</region>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Greg</forename><surname>Corrado</surname></persName>\n",
      "<email>gcorrado@google.com</email>\n",
      "<affiliation key=\"aff2\">\n",
      "<orgName type=\"institution\">Google Inc</orgName>\n",
      "<address>\n",
      "<settlement>Mountain View</settlement>\n",
      "<region>CA</region>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Jeffrey</forename><surname>Dean</surname></persName>\n",
      "<affiliation key=\"aff3\">\n",
      "<orgName type=\"institution\">Google Inc</orgName>\n",
      "<address>\n",
      "<settlement>Mountain View</settlement>\n",
      "<region>CA</region>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "authors.print_elements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(authors.proccesed_files[0][\"elements\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jacob Devlin'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(authors.proccesed_files[0][\"elements\"][0].persName.strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jacob'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors.proccesed_files[0][\"elements\"][0].persName.forename.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n",
      "data/xmls\\nlp\\Bert.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\DistillBERT.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\Dont_stop_pretraining.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\GPT-3.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\LIME.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\LoRA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\RoBERTa.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\SORA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\Transformers.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\word2vec.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "grobid_logger: 2024-04-26 17:41:59,266 | INFO | API.py:44 | 50960 >>> All files have been process by the api\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grobid_logger: 2024-04-26 17:41:59,350 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Bert have been extracted\n",
      "grobid_logger: 2024-04-26 17:41:59,382 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file DistillBERT have been extracted\n",
      "grobid_logger: 2024-04-26 17:41:59,478 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Dont_stop_pretraining have been extracted\n",
      "grobid_logger: 2024-04-26 17:41:59,853 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file GPT-3 have been extracted\n",
      "grobid_logger: 2024-04-26 17:41:59,897 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file LIME have been extracted\n",
      "grobid_logger: 2024-04-26 17:41:59,989 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file LoRA have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:00,069 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file RoBERTa have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:00,342 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file SORA have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:00,398 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Transformers have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:00,461 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file word2vec have been extracted\n"
     ]
    }
   ],
   "source": [
    "extract_config.extract.dir = None\n",
    "extract_config.extract.element = \"abstract\"\n",
    "abstract = extract_elements(extract_config,server_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a;Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract.proccesed_files[0][\"elements\"][0].p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n",
      "data/xmls\\nlp\\Bert.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\DistillBERT.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\Dont_stop_pretraining.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\GPT-3.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\LIME.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\LoRA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\RoBERTa.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\SORA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\Transformers.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\word2vec.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "grobid_logger: 2024-04-26 17:42:00,602 | INFO | API.py:44 | 50960 >>> All files have been process by the api\n",
      "grobid_logger: 2024-04-26 17:42:00,678 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Bert have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:00,949 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file DistillBERT have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:01,046 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Dont_stop_pretraining have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:01,271 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file GPT-3 have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:01,330 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file LIME have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:01,430 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file LoRA have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:01,499 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file RoBERTa have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:01,772 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file SORA have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:01,825 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Transformers have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:02,202 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file word2vec have been extracted\n"
     ]
    }
   ],
   "source": [
    "extract_config.extract.dir = None\n",
    "extract_config.extract.element = \"div\"\n",
    "extract_config.extract.type = \"acknowledgement\"\n",
    "ackno = extract_elements(extract_config,server_config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'LIME',\n",
       " 'elements': [<div type=\"acknowledgement\">\n",
       "  <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Acknowledgements</head><p>We would like to thank Scott Lundberg, Tianqi Chen, and Tyler Johnson for helpful discussions and feedback. This work was supported in part by ONR awards #W911NF-13-1-0246 and #N00014-13-1-0023, and in part by TerraSwarm, one of six centers of STARnet, a Semiconductor Research Corporation program sponsored by MARCO and DARPA.</p></div>\n",
       "  </div>]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ackno.proccesed_files[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigAttributeError",
     "evalue": "Missing key extract\n    full_key: extract\n    object_type=dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConfigAttributeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m extract_config\u001b[38;5;241m.\u001b[39mextract\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JorgeM\\anaconda3\\Lib\\site-packages\\omegaconf\\dictconfig.py:355\u001b[0m, in \u001b[0;36mDictConfig.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_impl(\n\u001b[0;32m    352\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey, default_value\u001b[38;5;241m=\u001b[39m_DEFAULT_MARKER_, validate_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    353\u001b[0m     )\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConfigKeyError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_and_raise(\n\u001b[0;32m    356\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cause\u001b[38;5;241m=\u001b[39me, type_override\u001b[38;5;241m=\u001b[39mConfigAttributeError\n\u001b[0;32m    357\u001b[0m     )\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_and_raise(key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cause\u001b[38;5;241m=\u001b[39me)\n",
      "File \u001b[1;32mc:\\Users\\JorgeM\\anaconda3\\Lib\\site-packages\\omegaconf\\base.py:231\u001b[0m, in \u001b[0;36mNode._format_and_raise\u001b[1;34m(self, key, value, cause, msg, type_override)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_and_raise\u001b[39m(\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    225\u001b[0m     key: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m     type_override: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 231\u001b[0m     format_and_raise(\n\u001b[0;32m    232\u001b[0m         node\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    233\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey,\n\u001b[0;32m    234\u001b[0m         value\u001b[38;5;241m=\u001b[39mvalue,\n\u001b[0;32m    235\u001b[0m         msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(cause) \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m msg,\n\u001b[0;32m    236\u001b[0m         cause\u001b[38;5;241m=\u001b[39mcause,\n\u001b[0;32m    237\u001b[0m         type_override\u001b[38;5;241m=\u001b[39mtype_override,\n\u001b[0;32m    238\u001b[0m     )\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JorgeM\\anaconda3\\Lib\\site-packages\\omegaconf\\_utils.py:899\u001b[0m, in \u001b[0;36mformat_and_raise\u001b[1;34m(node, key, value, msg, cause, type_override)\u001b[0m\n\u001b[0;32m    896\u001b[0m     ex\u001b[38;5;241m.\u001b[39mref_type \u001b[38;5;241m=\u001b[39m ref_type\n\u001b[0;32m    897\u001b[0m     ex\u001b[38;5;241m.\u001b[39mref_type_str \u001b[38;5;241m=\u001b[39m ref_type_str\n\u001b[1;32m--> 899\u001b[0m _raise(ex, cause)\n",
      "File \u001b[1;32mc:\\Users\\JorgeM\\anaconda3\\Lib\\site-packages\\omegaconf\\_utils.py:797\u001b[0m, in \u001b[0;36m_raise\u001b[1;34m(ex, cause)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    796\u001b[0m     ex\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 797\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\JorgeM\\anaconda3\\Lib\\site-packages\\omegaconf\\dictconfig.py:351\u001b[0m, in \u001b[0;36mDictConfig.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m()\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_impl(\n\u001b[0;32m    352\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey, default_value\u001b[38;5;241m=\u001b[39m_DEFAULT_MARKER_, validate_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    353\u001b[0m     )\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConfigKeyError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_and_raise(\n\u001b[0;32m    356\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cause\u001b[38;5;241m=\u001b[39me, type_override\u001b[38;5;241m=\u001b[39mConfigAttributeError\n\u001b[0;32m    357\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\JorgeM\\anaconda3\\Lib\\site-packages\\omegaconf\\dictconfig.py:442\u001b[0m, in \u001b[0;36mDictConfig._get_impl\u001b[1;34m(self, key, default_value, validate_key)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_impl\u001b[39m(\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28mself\u001b[39m, key: DictKeyType, default_value: Any, validate_key: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    440\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 442\u001b[0m         node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_child(\n\u001b[0;32m    443\u001b[0m             key\u001b[38;5;241m=\u001b[39mkey, throw_on_missing_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, validate_key\u001b[38;5;241m=\u001b[39mvalidate_key\n\u001b[0;32m    444\u001b[0m         )\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (ConfigAttributeError, ConfigKeyError):\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m default_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _DEFAULT_MARKER_:\n",
      "File \u001b[1;32mc:\\Users\\JorgeM\\anaconda3\\Lib\\site-packages\\omegaconf\\basecontainer.py:73\u001b[0m, in \u001b[0;36mBaseContainer._get_child\u001b[1;34m(self, key, validate_access, validate_key, throw_on_missing_value, throw_on_missing_key)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_child\u001b[39m(\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     66\u001b[0m     key: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m     throw_on_missing_key: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     71\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Optional[Node], List[Optional[Node]]]:\n\u001b[0;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Like _get_node, passing through to the nearest concrete Node.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     child \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_node(\n\u001b[0;32m     74\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey,\n\u001b[0;32m     75\u001b[0m         validate_access\u001b[38;5;241m=\u001b[39mvalidate_access,\n\u001b[0;32m     76\u001b[0m         validate_key\u001b[38;5;241m=\u001b[39mvalidate_key,\n\u001b[0;32m     77\u001b[0m         throw_on_missing_value\u001b[38;5;241m=\u001b[39mthrow_on_missing_value,\n\u001b[0;32m     78\u001b[0m         throw_on_missing_key\u001b[38;5;241m=\u001b[39mthrow_on_missing_key,\n\u001b[0;32m     79\u001b[0m     )\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child, UnionNode) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_special(child):\n\u001b[0;32m     81\u001b[0m         value \u001b[38;5;241m=\u001b[39m child\u001b[38;5;241m.\u001b[39m_value()\n",
      "File \u001b[1;32mc:\\Users\\JorgeM\\anaconda3\\Lib\\site-packages\\omegaconf\\dictconfig.py:480\u001b[0m, in \u001b[0;36mDictConfig._get_node\u001b[1;34m(self, key, validate_access, validate_key, throw_on_missing_value, throw_on_missing_key)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m throw_on_missing_key:\n\u001b[1;32m--> 480\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConfigKeyError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m throw_on_missing_value \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39m_is_missing():\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingMandatoryValue(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing mandatory value: $KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mConfigAttributeError\u001b[0m: Missing key extract\n    full_key: extract\n    object_type=dict"
     ]
    }
   ],
   "source": [
    "extract_config.extract.type ==None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n",
      "data/xmls\\nlp\\Bert.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\DistillBERT.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\Dont_stop_pretraining.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\GPT-3.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\LIME.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\LoRA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\RoBERTa.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\SORA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\Transformers.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\word2vec.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "grobid_logger: 2024-04-26 17:44:50,208 | INFO | API.py:44 | 50960 >>> All files have been process by the api\n",
      "grobid_logger: 2024-04-26 17:44:50,300 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Bert have been extracted\n",
      "grobid_logger: 2024-04-26 17:44:50,336 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file DistillBERT have been extracted\n",
      "grobid_logger: 2024-04-26 17:44:50,475 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Dont_stop_pretraining have been extracted\n",
      "grobid_logger: 2024-04-26 17:44:50,665 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file GPT-3 have been extracted\n",
      "grobid_logger: 2024-04-26 17:44:50,701 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file LIME have been extracted\n",
      "grobid_logger: 2024-04-26 17:44:51,815 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file LoRA have been extracted\n",
      "grobid_logger: 2024-04-26 17:44:51,899 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file RoBERTa have been extracted\n",
      "grobid_logger: 2024-04-26 17:44:52,116 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file SORA have been extracted\n",
      "grobid_logger: 2024-04-26 17:44:52,164 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Transformers have been extracted\n",
      "grobid_logger: 2024-04-26 17:44:52,212 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file word2vec have been extracted\n"
     ]
    }
   ],
   "source": [
    "extract_config.extract.dir = \"teiHeader.fileDesc\"\n",
    "extract_config.extract.element = None\n",
    "papers_info = extract_elements(extract_config,server_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fileDesc>\n",
       "<titleStmt>\n",
       "<title level=\"a\" type=\"main\">Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>\n",
       "</titleStmt>\n",
       "<publicationStmt>\n",
       "<publisher>Association for Computational Linguistics</publisher>\n",
       "<availability status=\"unknown\"><p>Copyright Association for Computational Linguistics</p>\n",
       "</availability>\n",
       "<date type=\"published\" when=\"2020-05-05\">5 May 2020</date>\n",
       "</publicationStmt>\n",
       "<sourceDesc>\n",
       "<biblStruct>\n",
       "<analytic>\n",
       "<author>\n",
       "<persName><forename type=\"first\">Suchin</forename><surname>Gururangan</surname></persName>\n",
       "<email>suching@allenai.org</email>\n",
       "<affiliation key=\"aff0\">\n",
       "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       "<address>\n",
       "<settlement>Seattle</settlement>\n",
       "<region>WA</region>\n",
       "<country key=\"US\">USA</country>\n",
       "</address>\n",
       "</affiliation>\n",
       "</author>\n",
       "<author>\n",
       "<persName><forename type=\"first\">Ana</forename><surname>MarasoviÄ‡</surname></persName>\n",
       "<email>anam@allenai.org</email>\n",
       "<affiliation key=\"aff0\">\n",
       "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       "<address>\n",
       "<settlement>Seattle</settlement>\n",
       "<region>WA</region>\n",
       "<country key=\"US\">USA</country>\n",
       "</address>\n",
       "</affiliation>\n",
       "<affiliation key=\"aff1\">\n",
       "<orgName type=\"department\">Paul G</orgName>\n",
       "<orgName key=\"instit1\" type=\"institution\">Allen School of Computer Science &amp; Engineering</orgName>\n",
       "<orgName key=\"instit2\" type=\"institution\">University of Washington</orgName>\n",
       "<address>\n",
       "<settlement>Seattle</settlement>\n",
       "<region>WA</region>\n",
       "<country key=\"US\">USA</country>\n",
       "</address>\n",
       "</affiliation>\n",
       "</author>\n",
       "<author>\n",
       "<persName><forename type=\"first\">Swabha</forename><surname>Swayamdipta</surname></persName>\n",
       "<email>swabhas@allenai.org</email>\n",
       "<affiliation key=\"aff0\">\n",
       "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       "<address>\n",
       "<settlement>Seattle</settlement>\n",
       "<region>WA</region>\n",
       "<country key=\"US\">USA</country>\n",
       "</address>\n",
       "</affiliation>\n",
       "</author>\n",
       "<author>\n",
       "<persName><forename type=\"first\">Kyle</forename><surname>Lo</surname></persName>\n",
       "<email>kylel@allenai.org</email>\n",
       "<affiliation key=\"aff0\">\n",
       "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       "<address>\n",
       "<settlement>Seattle</settlement>\n",
       "<region>WA</region>\n",
       "<country key=\"US\">USA</country>\n",
       "</address>\n",
       "</affiliation>\n",
       "</author>\n",
       "<author>\n",
       "<persName><forename type=\"first\">Iz</forename><surname>Beltagy</surname></persName>\n",
       "<email>beltagy@allenai.org</email>\n",
       "<affiliation key=\"aff0\">\n",
       "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       "<address>\n",
       "<settlement>Seattle</settlement>\n",
       "<region>WA</region>\n",
       "<country key=\"US\">USA</country>\n",
       "</address>\n",
       "</affiliation>\n",
       "</author>\n",
       "<author>\n",
       "<persName><forename type=\"first\">Doug</forename><surname>Downey</surname></persName>\n",
       "<email>dougd@allenai.org</email>\n",
       "<affiliation key=\"aff0\">\n",
       "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       "<address>\n",
       "<settlement>Seattle</settlement>\n",
       "<region>WA</region>\n",
       "<country key=\"US\">USA</country>\n",
       "</address>\n",
       "</affiliation>\n",
       "</author>\n",
       "<author>\n",
       "<persName><forename type=\"first\">Noah</forename><forename type=\"middle\">A</forename><surname>Smith</surname></persName>\n",
       "<affiliation key=\"aff0\">\n",
       "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       "<address>\n",
       "<settlement>Seattle</settlement>\n",
       "<region>WA</region>\n",
       "<country key=\"US\">USA</country>\n",
       "</address>\n",
       "</affiliation>\n",
       "<affiliation key=\"aff1\">\n",
       "<orgName type=\"department\">Paul G</orgName>\n",
       "<orgName key=\"instit1\" type=\"institution\">Allen School of Computer Science &amp; Engineering</orgName>\n",
       "<orgName key=\"instit2\" type=\"institution\">University of Washington</orgName>\n",
       "<address>\n",
       "<settlement>Seattle</settlement>\n",
       "<region>WA</region>\n",
       "<country key=\"US\">USA</country>\n",
       "</address>\n",
       "</affiliation>\n",
       "</author>\n",
       "<title level=\"a\" type=\"main\">Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>\n",
       "</analytic>\n",
       "<monogr>\n",
       "<title level=\"m\">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>\n",
       "<meeting>the 58th Annual Meeting of the Association for Computational Linguistics\t\t\t\t\t\t</meeting>\n",
       "<imprint>\n",
       "<publisher>Association for Computational Linguistics</publisher>\n",
       "<date type=\"published\" when=\"2020-05-05\">5 May 2020</date>\n",
       "</imprint>\n",
       "</monogr>\n",
       "<idno type=\"DOI\">10.18653/v1/2020.acl-main.740</idno>\n",
       "<idno type=\"arXiv\">arXiv:2004.10964v3[cs.CL]</idno>\n",
       "</biblStruct>\n",
       "</sourceDesc>\n",
       "</fileDesc>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_info.proccesed_files[2][\"elements\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVER_CONFIG\n",
      "url:\n",
      "  protocol: http\n",
      "  api_domain: yordi111nas.synology.me\n",
      "  port: 8070\n",
      "\n",
      "CLOUD_CONFIG\n",
      "data:\n",
      "  data_dir: data/PDFs\n",
      "  format: .pdf\n",
      "  recursive: true\n",
      "grobid:\n",
      "  cache: true\n",
      "  cache_dir: data/xmls\n",
      "  operation_key: processFulltextDocument\n",
      "  format: .grobid.tei.xml\n",
      "  recursive: true\n",
      "\n",
      "GROBID server is up and running\n",
      "data/xmls/nlp/word2vec.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls/nlp/Dont_stop_pretraining.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls/nlp/LIME.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls/nlp/LoRA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls/nlp/Bert.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls/nlp/SORA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls/nlp/Transformers.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls/nlp/DistillBERT.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls/nlp/RoBERTa.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls/nlp/GPT-3.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "grobid_logger: 2024-04-27 18:31:28,060 | INFO | API.py:44 | 74856 >>> All files have been process by the api\n"
     ]
    }
   ],
   "source": [
    "from pdf_analyzer.api import API\n",
    "from pdf_analyzer.api.extract.elements import extract_element_soup\n",
    "from pdf_analyzer.config_load import load_config\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "server_config = load_config(\"config/api/grobid-server-config.yaml\")\n",
    "extract_config = load_config(\"config/api/api-base-config.yaml\")\n",
    "print(\"SERVER_CONFIG\\n\"+OmegaConf.to_yaml(server_config))\n",
    "print(\"CLOUD_CONFIG\\n\"+OmegaConf.to_yaml(extract_config))\n",
    "\n",
    "base_api = API.BaseAPI(extract_config,server_config)\n",
    "\n",
    "files = base_api.proccesed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       " <TEI xml:space=\"preserve\" xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd\">\n",
       " <teiHeader xml:lang=\"en\">\n",
       " <fileDesc>\n",
       " <titleStmt>\n",
       " <title level=\"a\" type=\"main\">A Robustly Optimized BERT Pre-training Approach with Post-training</title>\n",
       " </titleStmt>\n",
       " <publicationStmt>\n",
       " <publisher>Springer International Publishing</publisher>\n",
       " <availability status=\"unknown\"><p>Copyright Springer International Publishing</p>\n",
       " </availability>\n",
       " <date type=\"published\" when=\"2019-07-26\">26 Jul 2019</date>\n",
       " </publicationStmt>\n",
       " <sourceDesc>\n",
       " <biblStruct>\n",
       " <analytic>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhuang</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wayne</forename><surname>Lin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ya</forename><surname>Shi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jun</forename><surname>Zhao</surname></persName>\n",
       " </author>\n",
       " <title level=\"a\" type=\"main\">A Robustly Optimized BERT Pre-training Approach with Post-training</title>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Lecture Notes in Computer Science</title>\n",
       " <idno type=\"ISSN\">0302-9743</idno>\n",
       " <idno type=\"eISSN\">1611-3349</idno>\n",
       " <imprint>\n",
       " <publisher>Springer International Publishing</publisher>\n",
       " <biblScope from=\"471\" to=\"484\" unit=\"page\"/>\n",
       " <date type=\"published\" when=\"2019-07-26\">26 Jul 2019</date>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <idno type=\"DOI\">10.1007/978-3-030-84186-7_31</idno>\n",
       " <idno type=\"arXiv\">arXiv:1907.11692v1[cs.CL]</idno>\n",
       " </biblStruct>\n",
       " </sourceDesc>\n",
       " </fileDesc>\n",
       " <encodingDesc>\n",
       " <appInfo>\n",
       " <application ident=\"GROBID\" version=\"0.7.0-SNAPSHOT\" when=\"2024-04-27T16:15+0000\">\n",
       " <desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>\n",
       " <ref target=\"https://github.com/kermitt2/grobid\"/>\n",
       " </application>\n",
       " </appInfo>\n",
       " </encodingDesc>\n",
       " <profileDesc>\n",
       " <abstract>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   </p></div>\n",
       " </abstract>\n",
       " </profileDesc>\n",
       " </teiHeader>\n",
       " <text xml:lang=\"en\">\n",
       " <body>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>Self-training methods such as ELMo <ref target=\"#b32\" type=\"bibr\">(Peters et al., 2018)</ref>, GPT <ref target=\"#b33\" type=\"bibr\">(Radford et al., 2018)</ref>, BERT <ref target=\"#b8\" type=\"bibr\">(Devlin et al., 2019)</ref>, XLM <ref target=\"#b22\" type=\"bibr\">(Lample and Conneau, 2019)</ref>, and XLNet <ref target=\"#b48\" type=\"bibr\">(Yang et al., 2019)</ref> have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most. Training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances.</p><p>We present a replication study of BERT pretraining <ref target=\"#b8\" type=\"bibr\">(Devlin et al., 2019)</ref>, which includes a careful evaluation of the effects of hyperparmeter tuning and training set size. We find that BERT was significantly undertrained and propose an improved recipe for training BERT models, which we call RoBERTa, that can match or exceed the performance of all of the post-BERT methods. Our modifications are simple, they include: (1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data. We also collect a large new dataset (CC-NEWS) of comparable size to other privately used datasets, to better control for training set size effects.</p><p>When controlling for training data, our improved training procedure improves upon the published BERT results on both GLUE and SQuAD. When trained for longer over additional data, our model achieves a score of 88.5 on the public GLUE leaderboard, matching the 88.4 reported by <ref target=\"#b48\" type=\"bibr\">Yang et al. (2019)</ref>. Our model establishes a new state-of-the-art on 4/9 of the GLUE tasks: MNLI, QNLI, RTE and STS-B. We also match state-of-the-art results on SQuAD and RACE. Overall, we re-establish that BERT's masked language model training objective is competitive with other recently proposed training objectives such as perturbed autoregressive language modeling <ref target=\"#b48\" type=\"bibr\">(Yang et al., 2019)</ref>. <ref type=\"bibr\">2</ref> In summary, the contributions of this paper are: (1) We present a set of important BERT design choices and training strategies and introduce alternatives that lead to better downstream task performance; (2) We use a novel dataset, CC-NEWS, and confirm that using more data for pretraining further improves performance on downstream tasks; (3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods. We release our model, pretraining and fine-tuning code implemented in PyTorch <ref target=\"#b31\" type=\"bibr\">(Paszke et al., 2017)</ref>.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">Background</head><p>In this section, we give a brief overview of the BERT <ref target=\"#b8\" type=\"bibr\">(Devlin et al., 2019)</ref> pretraining approach and some of the training choices that we will examine experimentally in the following section.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.1\">Setup</head><p>BERT takes as input a concatenation of two segments (sequences of tokens), x 1 , . . . , x N and y 1 , . . . , y M . Segments usually consist of more than one natural sentence. The two segments are presented as a single input sequence to BERT with special tokens delimiting them:</p><formula xml:id=\"formula_0\">[CLS ], x 1 , . . . , x N , [SEP ], y 1 , . . . , y M , [EOS ]. M and N are constrained such that M + N &lt; T ,</formula><p>where T is a parameter that controls the maximum sequence length during training.</p><p>The model is first pretrained on a large unlabeled text corpus and subsequently finetuned using end-task labeled data.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Architecture</head><p>BERT uses the now ubiquitous transformer architecture <ref target=\"#b42\" type=\"bibr\">(Vaswani et al., 2017)</ref>, which we will not review in detail. We use a transformer architecture with L layers. Each block uses A self-attention heads and hidden dimension H.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.3\">Training Objectives</head><p>During pretraining, BERT uses two objectives: masked language modeling and next sentence prediction.</p><p>Masked Language Model (MLM) A random sample of the tokens in the input sequence is selected and replaced with the special token <ref type=\"bibr\">[MASK ]</ref>. The MLM objective is a cross-entropy loss on predicting the masked tokens. BERT uniformly selects 15% of the input tokens for possible replacement. Of the selected tokens, 80% are replaced with [MASK ], 10% are left unchanged, and 10% are replaced by a randomly selected vocabulary token.</p><p>In the original implementation, random masking and replacement is performed once in the beginning and saved for the duration of training, although in practice, data is duplicated so the mask is not always the same for every training sentence (see Section 4.1).</p><p>Next Sentence Prediction (NSP) NSP is a binary classification loss for predicting whether two segments follow each other in the original text. Positive examples are created by taking consecutive sentences from the text corpus. Negative examples are created by pairing segments from different documents. Positive and negative examples are sampled with equal probability.</p><p>The NSP objective was designed to improve performance on downstream tasks, such as Natural Language Inference <ref target=\"#b4\" type=\"bibr\">(Bowman et al., 2015)</ref>, which require reasoning about the relationships between pairs of sentences.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.4\">Optimization</head><p>BERT is optimized with Adam <ref target=\"#b19\" type=\"bibr\">(Kingma and Ba, 2015)</ref> using the following parameters: Î² 1 = 0.9, Î² 2 = 0.999, Ç« = 1e-6 and L 2 weight decay of 0.01. The learning rate is warmed up over the first 10,000 steps to a peak value of 1e-4, and then linearly decayed. BERT trains with a dropout of 0.1 on all layers and attention weights, and a GELU activation function <ref target=\"#b14\" type=\"bibr\">(Hendrycks and Gimpel, 2016)</ref>. Models are pretrained for S = 1,000,000 updates, with minibatches containing B = 256 sequences of maximum length T = 512 tokens.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.5\">Data</head><p>BERT is trained on a combination of BOOKCOR-PUS <ref target=\"#b50\" type=\"bibr\">(Zhu et al., 2015)</ref> plus English WIKIPEDIA, which totals 16GB of uncompressed text. 3</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3\">Experimental Setup</head><p>In this section, we describe the experimental setup for our replication study of BERT.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1\">Implementation</head><p>We reimplement BERT in FAIRSEQ <ref target=\"#b29\" type=\"bibr\">(Ott et al., 2019)</ref>. We primarily follow the original BERT optimization hyperparameters, given in Section 2, except for the peak learning rate and number of warmup steps, which are tuned separately for each setting. We additionally found training to be very sensitive to the Adam epsilon term, and in some cases we obtained better performance or improved stability after tuning it. Similarly, we found setting Î² 2 = 0.98 to improve stability when training with large batch sizes.</p><p>We pretrain with sequences of at most T = 512 tokens. Unlike <ref target=\"#b8\" type=\"bibr\">Devlin et al. (2019)</ref>, we do not randomly inject short sequences, and we do not train with a reduced sequence length for the first 90% of updates. We train only with full-length sequences.</p><p>We train with mixed precision floating point arithmetic on DGX-1 machines, each with 8 Ã— 32GB Nvidia V100 GPUs interconnected by Infiniband <ref target=\"#b27\" type=\"bibr\">(Micikevicius et al., 2018)</ref>.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Data</head><p>BERT-style pretraining crucially relies on large quantities of text.  demonstrate that increasing data size can result in improved end-task performance. Several efforts have trained on datasets larger and more diverse than the original BERT <ref target=\"#b34\" type=\"bibr\">(Radford et al., 2019;</ref><ref target=\"#b48\" type=\"bibr\">Yang et al., 2019;</ref><ref target=\"#b49\" type=\"bibr\">Zellers et al., 2019)</ref>. Unfortunately, not all of the additional datasets can be publicly released. For our study, we focus on gathering as much data as possible for experimentation, allowing us to match the overall quality and quantity of data as appropriate for each comparison.</p><p>We consider five English-language corpora of varying sizes and domains, totaling over 160GB of uncompressed text. We use the following text corpora:</p><p>â€¢ BOOKCORPUS <ref target=\"#b50\" type=\"bibr\">(Zhu et al., 2015)</ref> plus English WIKIPEDIA. This is the original data used to train BERT. (16GB).</p><p>â€¢ CC-NEWS, which we collected from the English portion of the CommonCrawl News dataset <ref target=\"#b28\" type=\"bibr\">(Nagel, 2016)</ref>. The data contains 63 million English news articles crawled between September 2016 and February 2019. (76GB after filtering). 4</p><p>â€¢ OPENWEBTEXT <ref target=\"#b12\" type=\"bibr\">(Gokaslan and Cohen, 2019)</ref>, an open-source recreation of the WebText cor-pus described in <ref target=\"#b34\" type=\"bibr\">Radford et al. (2019)</ref>. The text is web content extracted from URLs shared on Reddit with at least three upvotes. (38GB). 5</p><p>â€¢ STORIES, a dataset introduced in Trinh and Le (2018) containing a subset of CommonCrawl data filtered to match the story-like style of Winograd schemas. (31GB).</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.3\">Evaluation</head><p>Following previous work, we evaluate our pretrained models on downstream tasks using the following three benchmarks.</p><p>GLUE The General Language Understanding Evaluation (GLUE) benchmark <ref target=\"#b44\" type=\"bibr\">(Wang et al., 2019b</ref>) is a collection of 9 datasets for evaluating natural language understanding systems. <ref type=\"bibr\">6</ref> Tasks are framed as either single-sentence classification or sentence-pair classification tasks. The GLUE organizers provide training and development data splits as well as a submission server and leaderboard that allows participants to evaluate and compare their systems on private held-out test data.</p><p>For the replication study in Section 4, we report results on the development sets after finetuning the pretrained models on the corresponding singletask training data (i.e., without multi-task training or ensembling). Our finetuning procedure follows the original BERT paper <ref target=\"#b8\" type=\"bibr\">(Devlin et al., 2019)</ref>.</p><p>In Section 5 we additionally report test set results obtained from the public leaderboard. These results depend on a several task-specific modifications, which we describe in Section 5.1.</p><p>SQuAD The Stanford Question Answering Dataset (SQuAD) provides a paragraph of context and a question. The task is to answer the question by extracting the relevant span from the context. We evaluate on two versions of SQuAD: V1.1 and V2.0 <ref target=\"#b36\" type=\"bibr\">(Rajpurkar et al., 2016</ref><ref target=\"#b35\" type=\"bibr\">(Rajpurkar et al., , 2018</ref>. In V1.1 the context always contains an answer, whereas in V2.0 some questions are not answered in the provided context, making the task more challenging.</p><p>For SQuAD V1.1 we adopt the same span prediction method as BERT <ref target=\"#b8\" type=\"bibr\">(Devlin et al., 2019)</ref>. For SQuAD V2.0, we add an additional binary classifier to predict whether the question is answerable, which we train jointly by summing the classification and span loss terms. During evaluation, we only predict span indices on pairs that are classified as answerable.</p><p>RACE The ReAding Comprehension from Examinations (RACE) <ref target=\"#b21\" type=\"bibr\">(Lai et al., 2017)</ref> task is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle and high school students. In RACE, each passage is associated with multiple questions. For every question, the task is to select one correct answer from four options. RACE has significantly longer context than other popular reading comprehension datasets and the proportion of questions that requires reasoning is very large.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4\">Training Procedure Analysis</head><p>This section explores and quantifies which choices are important for successfully pretraining BERT models. We keep the model architecture fixed. 7 Specifically, we begin by training BERT models with the same configuration as BERT BASE (L = 12, H = 768, A = 12, 110M params).</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1\">Static vs. Dynamic Masking</head><p>As discussed in Section 2, BERT relies on randomly masking and predicting tokens. The original BERT implementation performed masking once during data preprocessing, resulting in a single static mask. To avoid using the same mask for each training instance in every epoch, training data was duplicated 10 times so that each sequence is masked in 10 different ways over the 40 epochs of training. Thus, each training sequence was seen with the same mask four times during training.</p><p>We compare this strategy with dynamic masking where we generate the masking pattern every time we feed a sequence to the model. This becomes crucial when pretraining for more steps or with larger datasets. Results Table <ref type=\"table\">1</ref> compares the published BERT BASE results from <ref target=\"#b8\" type=\"bibr\">Devlin et al. (2019)</ref> to our reimplementation with either static or dynamic masking. We find that our reimplementation with static masking performs similar to the original BERT model, and dynamic masking is comparable or slightly better than static masking. Given these results and the additional efficiency benefits of dynamic masking, we use dynamic masking in the remainder of the experiments.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2\">Model Input Format and Next Sentence Prediction</head><p>In the original BERT pretraining procedure, the model observes two concatenated document segments, which are either sampled contiguously from the same document (with p = 0.5) or from distinct documents. In addition to the masked language modeling objective, the model is trained to predict whether the observed document segments come from the same or distinct documents via an auxiliary Next Sentence Prediction (NSP) loss.</p><p>The NSP loss was hypothesized to be an important factor in training the original BERT model. <ref target=\"#b8\" type=\"bibr\">Devlin et al. (2019)</ref> observe that removing NSP hurts performance, with significant performance degradation on QNLI, MNLI, and SQuAD 1.1. However, some recent work has questioned the necessity of the NSP loss <ref target=\"#b22\" type=\"bibr\">(Lample and Conneau, 2019;</ref><ref target=\"#b48\" type=\"bibr\">Yang et al., 2019;</ref><ref target=\"#b18\" type=\"bibr\">Joshi et al., 2019)</ref>.</p><p>To better understand this discrepancy, we compare several alternative training formats:</p><p>â€¢ SEGMENT-PAIR+NSP: This follows the original input format used in BERT <ref target=\"#b8\" type=\"bibr\">(Devlin et al., 2019)</ref>, with the NSP loss. Each input has a pair of segments, which can each contain multiple natural sentences, but the total combined length must be less than 512 tokens. â€¢ SENTENCE-PAIR+NSP: Each input contains a pair of natural sentences, either sampled from a contiguous portion of one document or from separate documents. Since these inputs are significantly shorter than 512 tokens, we increase the batch size so that the total number of tokens remains similar to SEGMENT-PAIR+NSP. We retain the NSP loss.</p><p>â€¢ FULL-SENTENCES: Each input is packed with full sentences sampled contiguously from one or more documents, such that the total length is at most 512 tokens. Inputs may cross document boundaries. When we reach the end of one document, we begin sampling sentences from the next document and add an extra separator token between documents. We remove the NSP loss.</p><p>â€¢ DOC-SENTENCES: Inputs are constructed similarly to FULL-SENTENCES, except that they may not cross document boundaries. Inputs sampled near the end of a document may be shorter than 512 tokens, so we dynamically increase the batch size in these cases to achieve a similar number of total tokens as FULL-SENTENCES. We remove the NSP loss.</p><p>Results Table <ref type=\"table\">2</ref> shows results for the four different settings. We first compare the original SEGMENT-PAIR input format from <ref target=\"#b8\" type=\"bibr\">Devlin et al. (2019)</ref> to the SENTENCE-PAIR format; both formats retain the NSP loss, but the latter uses single sentences. We find that using individual sentences hurts performance on downstream tasks, which we hypothesize is because the model is not able to learn long-range dependencies.</p><p>We next compare training without the NSP loss and training with blocks of text from a single document (DOC-SENTENCES). We find that this setting outperforms the originally published BERT BASE results and that removing the NSP loss matches or slightly improves downstream task performance, in contrast to <ref target=\"#b8\" type=\"bibr\">Devlin et al. (2019)</ref>. It is possible that the original BERT implementation may only have removed the loss term while still retaining the SEGMENT-PAIR input format.</p><p>Finally we find that restricting sequences to come from a single document (DOC-SENTENCES) performs slightly better than packing sequences from multiple documents (FULL-SENTENCES). However, because the DOC-SENTENCES format results in variable batch sizes, we use FULL-SENTENCES in the remainder of our experiments for easier comparison with related work.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.3\">Training with large batches</head><p>Past work in Neural Machine Translation has shown that training with very large mini-batches can both improve optimization speed and end-task performance when the learning rate is increased appropriately <ref target=\"#b30\" type=\"bibr\">(Ott et al., 2018)</ref>. Recent work has shown that BERT is also amenable to large batch training <ref target=\"#b48\" type=\"bibr\">(You et al., 2019</ref> </p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.4\">Text Encoding</head><p>Byte-Pair Encoding (BPE) <ref target=\"#b37\" type=\"bibr\">(Sennrich et al., 2016)</ref> is a hybrid between character-and word-level representations that allows handling the large vocabularies common in natural language corpora. Instead of full words, BPE relies on subwords units, which are extracted by performing statistical analysis of the training corpus.</p><p>BPE vocabulary sizes typically range from 10K-100K subword units. However, unicode characters can account for a sizeable portion of this vocabulary when modeling large and diverse corpora, such as the ones considered in this work. <ref target=\"#b34\" type=\"bibr\">Radford et al. (2019)</ref> introduce a clever implementation of BPE that uses bytes instead of unicode characters as the base subword units. Using bytes makes it possible to learn a subword vocabulary of a modest size (50K units) that can still encode any input text without introducing any \"unknown\" tokens. <ref type=\"bibr\">8</ref> Large batch training can improve training efficiency even without large scale parallel hardware through gradient accumulation, whereby gradients from multiple mini-batches are accumulated locally before each optimization step. This functionality is supported natively in FAIRSEQ <ref target=\"#b29\" type=\"bibr\">(Ott et al., 2019)</ref>.</p><p>The original BERT implementation <ref target=\"#b8\" type=\"bibr\">(Devlin et al., 2019)</ref> uses a character-level BPE vocabulary of size 30K, which is learned after preprocessing the input with heuristic tokenization rules. Following <ref target=\"#b34\" type=\"bibr\">Radford et al. (2019)</ref>, we instead consider training BERT with a larger byte-level BPE vocabulary containing 50K subword units, without any additional preprocessing or tokenization of the input. This adds approximately 15M and 20M additional parameters for BERT BASE and BERT LARGE , respectively.</p><p>Early experiments revealed only slight differences between these encodings, with the Radford et al. ( <ref type=\"formula\">2019</ref>) BPE achieving slightly worse end-task performance on some tasks. Nevertheless, we believe the advantages of a universal encoding scheme outweighs the minor degredation in performance and use this encoding in the remainder of our experiments. A more detailed comparison of these encodings is left to future work.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5\">RoBERTa</head><p>In the previous section we propose modifications to the BERT pretraining procedure that improve end-task performance. We now aggregate these improvements and evaluate their combined impact. We call this configuration RoBERTa for Robustly optimized BERT approach. Specifically, RoBERTa is trained with dynamic masking (Section 4.1), FULL-SENTENCES without NSP loss (Section 4.2), large mini-batches (Section 4.3) and a larger byte-level BPE (Section 4.4).</p><p>Additionally, we investigate two other important factors that have been under-emphasized in previous work: (1) the data used for pretraining, and (2) the number of training passes through the data. For example, the recently proposed XLNet architecture <ref target=\"#b48\" type=\"bibr\">(Yang et al., 2019</ref>) is pretrained using nearly 10 times more data than the original BERT <ref target=\"#b8\" type=\"bibr\">(Devlin et al., 2019)</ref>. It is also trained with a batch size eight times larger for half as many optimization steps, thus seeing four times as many sequences in pretraining compared to BERT.</p><p>To help disentangle the importance of these factors from other modeling choices (e.g., the pretraining objective), we begin by training RoBERTa following the BERT LARGE architecture (L = 24, H = 1024, A = 16, 355M parameters). We pretrain for 100K steps over a comparable BOOK-CORPUS plus WIKIPEDIA dataset as was used in   <ref target=\"#b48\" type=\"bibr\">Yang et al. (2019)</ref>, respectively. Complete results on all GLUE tasks can be found in the Appendix. <ref target=\"#b8\" type=\"bibr\">Devlin et al. (2019)</ref>. We pretrain our model using 1024 V100 GPUs for approximately one day.</p><p>Results We present our results in Table <ref target=\"#tab_5\" type=\"table\">4</ref>. When controlling for training data, we observe that RoBERTa provides a large improvement over the originally reported BERT LARGE results, reaffirming the importance of the design choices we explored in Section 4.</p><p>Next, we combine this data with the three additional datasets described in Section 3.2. We train RoBERTa over the combined data with the same number of training steps as before (100K). In total, we pretrain over 160GB of text. We observe further improvements in performance across all downstream tasks, validating the importance of data size and diversity in pretraining. <ref type=\"bibr\">9</ref> Finally, we pretrain RoBERTa for significantly longer, increasing the number of pretraining steps from 100K to 300K, and then further to 500K. We again observe significant gains in downstream task performance, and the 300K and 500K step models outperform XLNet LARGE across most tasks. We note that even our longest-trained model does not appear to overfit our data and would likely benefit from additional training.</p><p>In the rest of the paper, we evaluate our best RoBERTa model on the three different benchmarks: GLUE, SQuaD and RACE. Specifically we consider RoBERTa trained for 500K steps over all five of the datasets introduced in Section 3.2.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">GLUE Results</head><p>For GLUE we consider two finetuning settings. In the first setting (single-task, dev) we finetune RoBERTa separately for each of the GLUE tasks, using only the training data for the corresponding task. We consider a limited hyperparameter sweep for each task, with batch sizes âˆˆ {16, 32} and learning rates âˆˆ {1eâˆ’5, 2eâˆ’5, 3eâˆ’5}, with a linear warmup for the first 6% of steps followed by a linear decay to 0. We finetune for 10 epochs and perform early stopping based on each task's evaluation metric on the dev set. The rest of the hyperparameters remain the same as during pretraining. In this setting, we report the median development set results for each task over five random initializations, without model ensembling.</p><p>In the second setting (ensembles, test), we compare RoBERTa to other approaches on the test set via the GLUE leaderboard. While many submissions to the GLUE leaderboard depend on multitask finetuning, our submission depends only on single-task finetuning. For RTE, STS and MRPC we found it helpful to finetune starting from the MNLI single-task model, rather than the baseline pretrained RoBERTa. We explore a slightly wider hyperparameter space, described in the Appendix, and ensemble between 5 and 7 models per task.  Task-specific modifications Two of the GLUE tasks require task-specific finetuning approaches to achieve competitive leaderboard results. QNLI: Recent submissions on the GLUE leaderboard adopt a pairwise ranking formulation for the QNLI task, in which candidate answers are mined from the training set and compared to one another, and a single (question, candidate) pair is classified as positive <ref type=\"bibr\">(Liu et al., 2019b,a;</ref><ref target=\"#b48\" type=\"bibr\">Yang et al., 2019)</ref>. This formulation significantly simplifies the task, but is not directly comparable to BERT <ref target=\"#b8\" type=\"bibr\">(Devlin et al., 2019)</ref>. Following recent work, we adopt the ranking approach for our test submission, but for direct comparison with BERT we report development set results based on a pure classification approach. WNLI: We found the provided NLI-format data to be challenging to work with. Instead we use the reformatted WNLI data from Super-GLUE <ref target=\"#b43\" type=\"bibr\">(Wang et al., 2019a)</ref>, which indicates the span of the query pronoun and referent. We finetune RoBERTa using the margin ranking loss from <ref target=\"#b20\" type=\"bibr\">Kocijan et al. (2019)</ref>. For a given input sentence, we use spaCy <ref type=\"bibr\">(Honnibal and Montani, 2017)</ref> to extract additional candidate noun phrases from the sentence and finetune our model so that it assigns higher scores to positive referent phrases than for any of the generated negative candidate phrases. One unfortunate consequence of this formulation is that we can only make use Results We present our results in Table <ref target=\"#tab_7\" type=\"table\">5</ref>. In the first setting (single-task, dev), RoBERTa achieves state-of-the-art results on all 9 of the GLUE task development sets. Crucially, RoBERTa uses the same masked language modeling pretraining objective and architecture as BERT LARGE , yet consistently outperforms both BERT LARGE and XLNet LARGE . This raises questions about the relative importance of model architecture and pretraining objective, compared to more mundane details like dataset size and training time that we explore in this work.</p><p>In the second setting (ensembles, test), we submit RoBERTa to the GLUE leaderboard and achieve state-of-the-art results on 4 out of 9 tasks and the highest average score to date. This is especially exciting because RoBERTa does not depend on multi-task finetuning, unlike most of the other top submissions. We expect future work may further improve these results by incorporating more sophisticated multi-task finetuning procedures.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.2\">SQuAD Results</head><p>We adopt a much simpler approach for SQuAD compared to past work. In particular, while both BERT <ref target=\"#b8\" type=\"bibr\">(Devlin et al., 2019)</ref> and XL-Net <ref target=\"#b48\" type=\"bibr\">(Yang et al., 2019)</ref>   XLNet, while we use the same learning rate for all layers. For SQuAD v1.1 we follow the same finetuning procedure as <ref target=\"#b8\" type=\"bibr\">Devlin et al. (2019)</ref>. For SQuAD v2.0, we additionally classify whether a given question is answerable; we train this classifier jointly with the span predictor by summing the classification and span loss terms.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Results</head><p>We present our results in Table <ref target=\"#tab_9\" type=\"table\">6</ref>. On the SQuAD v1.1 development set, RoBERTa matches the state-of-the-art set by XLNet. On the SQuAD v2.0 development set, RoBERTa sets a new state-of-the-art, improving over XLNet by 0.4 points (EM) and 0.6 points (F1).</p><p>We also submit RoBERTa to the public SQuAD 2.0 leaderboard and evaluate its performance relative to other systems. Most of the top systems build upon either BERT <ref target=\"#b8\" type=\"bibr\">(Devlin et al., 2019)</ref> or XLNet <ref target=\"#b48\" type=\"bibr\">(Yang et al., 2019)</ref>, both of which rely on additional external training data. In contrast, our submission does not use any additional data.</p><p>Our single RoBERTa model outperforms all but one of the single model submissions, and is the top scoring system among those that do not rely on data augmentation.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.3\">RACE Results</head><p>In RACE, systems are provided with a passage of text, an associated question, and four candidate answers. Systems are required to classify which of the four candidate answers is correct.</p><p>We modify RoBERTa for this task by concate-   <ref target=\"#b48\" type=\"bibr\">Yang et al. (2019)</ref>.</p><p>nating each candidate answer with the corresponding question and passage. We then encode each of these four sequences and pass the resulting <ref type=\"bibr\">[CLS]</ref> representations through a fully-connected layer, which is used to predict the correct answer. We truncate question-answer pairs that are longer than 128 tokens and, if needed, the passage so that the total length is at most 512 tokens.</p><p>Results on the RACE test sets are presented in Table <ref target=\"#tab_11\" type=\"table\">7</ref>. RoBERTa achieves state-of-the-art results on both middle-school and high-school settings.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6\">Related Work</head><p>Pretraining methods have been designed with different training objectives, including language modeling <ref target=\"#b7\" type=\"bibr\">(Dai and Le, 2015;</ref><ref target=\"#b32\" type=\"bibr\">Peters et al., 2018;</ref><ref target=\"#b16\" type=\"bibr\">Howard and Ruder, 2018)</ref>, machine translation <ref target=\"#b26\" type=\"bibr\">(McCann et al., 2017)</ref>, and masked language modeling <ref target=\"#b8\" type=\"bibr\">(Devlin et al., 2019;</ref><ref target=\"#b22\" type=\"bibr\">Lample and Conneau, 2019)</ref>.</p><p>Many recent papers have used a basic recipe of finetuning models for each end task <ref target=\"#b16\" type=\"bibr\">(Howard and Ruder, 2018;</ref><ref target=\"#b33\" type=\"bibr\">Radford et al., 2018)</ref>, and pretraining with some variant of a masked language model objective.</p><p>However, newer methods have improved performance by multi-task fine tuning <ref target=\"#b10\" type=\"bibr\">(Dong et al., 2019)</ref>, incorporating entity embeddings <ref target=\"#b40\" type=\"bibr\">(Sun et al., 2019)</ref>, span prediction <ref target=\"#b18\" type=\"bibr\">(Joshi et al., 2019)</ref>, and multiple variants of autoregressive pretraining <ref target=\"#b5\" type=\"bibr\">Chan et al., 2019;</ref><ref target=\"#b48\" type=\"bibr\">Yang et al., 2019)</ref>. Performance is also typically improved by training bigger models on more data <ref target=\"#b8\" type=\"bibr\">(Devlin et al., 2019;</ref><ref target=\"#b48\" type=\"bibr\">Yang et al., 2019;</ref><ref target=\"#b34\" type=\"bibr\">Radford et al., 2019)</ref>. Our goal was to replicate, simplify, and better tune the training of BERT, as a reference point for better understanding the relative performance of all of these methods.</p><p>We carefully evaluate a number of design decisions when pretraining BERT models. We find that performance can be substantially improved by training the model longer, with bigger batches over more data; removing the next sentence prediction objective; training on longer sequences; and dynamically changing the masking pattern applied to the training data. Our improved pretraining procedure, which we call RoBERTa, achieves state-of-the-art results on GLUE, RACE and SQuAD, without multi-task finetuning for GLUE or additional data for SQuAD. These results illustrate the importance of these previously overlooked design decisions and suggest that BERT's pretraining objective remains competitive with recently proposed alternatives.</p><p>We additionally use a novel dataset, CC-NEWS, and release our models and code for pretraining and finetuning at: https://github.com/pytorch/fairseq.</p></div><figure xml:id=\"fig_0\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>of the positive training examples, which excludes over half of the provided training examples. 10 10 While we only use the provided WNLI training data, our</figDesc></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_1\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>Development set results for base models pretrained over BOOKCORPUS and WIKIPEDIA. All models are trained for 1M steps with a batch size of 256 sequences. We report F1 for SQuAD and accuracy for MNLI-m, SST-2 and RACE. Reported results are medians over five random initializations (seeds). Results for BERT BASE and XLNet BASE are from<ref target=\"#b48\" type=\"bibr\">Yang et al. (2019)</ref>.</figDesc><table><row><cell>Model</cell><cell cols=\"4\">SQuAD 1.1/2.0 MNLI-m SST-2 RACE</cell></row><row><cell cols=\"2\">Our reimplementation (with NSP loss):</cell><cell/><cell/><cell/></row><row><cell>SEGMENT-PAIR</cell><cell>90.4/78.7</cell><cell>84.0</cell><cell>92.9</cell><cell>64.2</cell></row><row><cell>SENTENCE-PAIR</cell><cell>88.7/76.2</cell><cell>82.9</cell><cell>92.1</cell><cell>63.0</cell></row><row><cell cols=\"2\">Our reimplementation (without NSP loss):</cell><cell/><cell/><cell/></row><row><cell>FULL-SENTENCES</cell><cell>90.4/79.1</cell><cell>84.7</cell><cell>92.5</cell><cell>64.8</cell></row><row><cell>DOC-SENTENCES</cell><cell>90.6/79.7</cell><cell>84.7</cell><cell>92.7</cell><cell>65.6</cell></row><row><cell>BERT BASE</cell><cell>88.5/76.3</cell><cell>84.3</cell><cell>92.8</cell><cell>64.3</cell></row><row><cell>XLNet BASE (K = 7)</cell><cell>-/81.3</cell><cell>85.8</cell><cell>92.7</cell><cell>66.1</cell></row><row><cell>XLNet BASE (K = 6)</cell><cell>-/81.0</cell><cell>85.6</cell><cell>93.4</cell><cell>66.7</cell></row><row><cell>Table 2:</cell><cell/><cell/><cell/><cell/></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_3\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 3</head><label>3</label><figDesc/><table><row><cell>: Perplexity on held-out training data (ppl) and</cell></row><row><cell>development set accuracy for base models trained over</cell></row><row><cell>BOOKCORPUS and WIKIPEDIA with varying batch</cell></row><row><cell>sizes (bsz). We tune the learning rate (lr) for each set-</cell></row><row><cell>ting. Models make the same number of passes over the</cell></row><row><cell>data (epochs) and have the same computational cost.</cell></row><row><cell>task performance of BERT BASE as we increase the</cell></row><row><cell>batch size, controlling for the number of passes</cell></row><row><cell>through the training data. We observe that train-</cell></row><row><cell>ing with large batches improves perplexity for the</cell></row><row><cell>masked language modeling objective, as well as</cell></row><row><cell>end-task accuracy. Large batches are also easier to</cell></row><row><cell>parallelize via distributed data parallel training, 8</cell></row><row><cell>and in later experiments we train with batches of</cell></row><row><cell>8K sequences.</cell></row><row><cell>Notably You et al. (2019) train BERT with even</cell></row><row><cell>larger batche sizes, up to 32K sequences. We leave</cell></row><row><cell>further exploration of the limits of large batch</cell></row><row><cell>training to future work.</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_5\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 4 :</head><label>4</label><figDesc>Development set results for RoBERTa as we pretrain over more data (16GB â†’ 160GB of text) and pretrain for longer (100K â†’ 300K â†’ 500K steps). Each row accumulates improvements from the rows above. RoBERTa matches the architecture and training objective of BERT LARGE . Results for BERT LARGE and XLNet LARGE are from Devlin et al. (2019) and</figDesc><table/></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_7\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 5 :</head><label>5</label><figDesc>Results on GLUE. All results are based on a 24-layer architecture. BERT LARGE and XLNet LARGE results are from Devlin et al. (2019) and Yang et al. (2019), respectively. RoBERTa results on the development set are a median over five runs. RoBERTa results on the test set are ensembles of single-task models. For RTE, STS and MRPC we finetune starting from the MNLI model instead of the baseline pretrained model. Averages are obtained from the GLUE leaderboard.</figDesc><table/></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_8\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>augment their training data with additional QA datasets, we only finetune RoBERTa using the provided SQuAD training data.<ref target=\"#b48\" type=\"bibr\">Yang et al. (2019)</ref> also employed a custom layer-wise learning rate schedule to finetune results could potentially be improved by augmenting this with additional pronoun disambiguation datasets.</figDesc><table><row><cell>Model</cell><cell>SQuAD 1.1 EM F1</cell><cell cols=\"2\">SQuAD 2.0 EM F1</cell></row><row><cell cols=\"4\">Single models on dev, w/o data augmentation</cell></row><row><cell cols=\"3\">BERT LARGE 84.1 90.9 79.0</cell><cell>81.8</cell></row><row><cell cols=\"3\">XLNet LARGE 89.0 94.5 86.1</cell><cell>88.8</cell></row><row><cell>RoBERTa</cell><cell cols=\"2\">88.9 94.6 86.5</cell><cell>89.4</cell></row><row><cell cols=\"4\">Single models on test (as of July 25, 2019)</cell></row><row><cell>XLNet LARGE</cell><cell/><cell>86.3  â€ </cell><cell>89.1  â€ </cell></row><row><cell>RoBERTa</cell><cell/><cell>86.8</cell><cell>89.8</cell></row><row><cell cols=\"2\">XLNet + SG-Net Verifier</cell><cell>87.0  â€ </cell><cell>89.9  â€ </cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_9\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 6 :</head><label>6</label><figDesc>Results on SQuAD. â€  indicates results that depend on additional external training data. RoBERTa uses only the provided SQuAD data in both dev and test settings. BERT LARGE and XLNet LARGE results are from Devlin et al. (2019) and Yang et al. (2019), respectively.</figDesc><table/></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_11\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 7 :</head><label>7</label><figDesc>Results on the RACE test set. BERT LARGE and XLNet LARGE results are from</figDesc><table/></figure>\n",
       " <note n=\"2\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">It is possible that these other methods could also improve with more tuning. We leave this exploration to future work.</note>\n",
       " <note n=\"3\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\"><ref target=\"#b48\" type=\"bibr\">Yang et al. (2019)</ref> use the same dataset but report having only 13GB of text after data cleaning. This is most likely due to subtle differences in cleaning of the Wikipedia data.</note>\n",
       " <note n=\"4\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">We use news-please<ref target=\"#b13\" type=\"bibr\">(Hamborg et al., 2017)</ref> to collect and extract CC-NEWS. CC-NEWS is similar to the RE-ALNEWS dataset described in<ref target=\"#b49\" type=\"bibr\">Zellers et al. (2019)</ref>.</note>\n",
       " <note n=\"7\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">Studying architectural changes, including larger architectures, is an important area for future work.</note>\n",
       " <note n=\"9\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">Our experiments conflate increases in data size and diversity. We leave a more careful analysis of these two dimensions to future work.</note>\n",
       " </body>\n",
       " <back>\n",
       " <div type=\"annex\">\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Appendix for \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\"</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A Full results on GLUE</head><p>In Table <ref type=\"table\">8</ref> we present the full set of development set results for RoBERTa. We present results for a LARGE configuration that follows BERT LARGE , as well as a BASE configuration that follows BERT BASE .    </p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>B Pretraining Hyperparameters</head></div> </div>\n",
       " <div type=\"references\">\n",
       " <listBibl>\n",
       " <biblStruct xml:id=\"b0\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Eneko</forename><surname>Agirre</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Richard</forename><surname>Llu'is M'arquez</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Wicentowski</surname></persName>\n",
       " </author>\n",
       " <title level=\"m\">Proceedings of the Fourth International Workshop on Semantic Evaluations</title>\n",
       " <meeting>the Fourth International Workshop on Semantic Evaluations</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2007\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b1\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Clozedriven pretraining of self-attention networks</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alexei</forename><surname>Baevski</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sergey</forename><surname>Edunov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yinhan</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Michael</forename><surname>Auli</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1903.07785</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b2\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">The second PASCAL recognising textual entailment challenge</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Roy</forename><surname>Bar-Haim</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ido</forename><surname>Dagan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bill</forename><surname>Dolan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lisa</forename><surname>Ferro</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Danilo</forename><surname>Giampiccolo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bernardo</forename><surname>Magnini</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Idan</forename><surname>Szpektor</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the second PASCAL challenges workshop on recognising textual entailment</title>\n",
       " <meeting>the second PASCAL challenges workshop on recognising textual entailment</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2006\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b3\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">The fifth PASCAL recognizing textual entailment challenge</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luisa</forename><surname>Bentivogli</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ido</forename><surname>Dagan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hoa</forename><forename type=\"middle\">Trang</forename><surname>Dang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Danilo</forename><surname>Giampiccolo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bernardo</forename><surname>Magnini</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2009\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b4\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A large annotated corpus for learning natural language inference</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Gabor</forename><surname>Samuel R Bowman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><surname>Angeli</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher D</forename><surname>Potts</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Manning</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Empirical Methods in Natural Language Processing (EMNLP)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b5\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">KERMIT: Generative insertion-based modeling for sequences</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">William</forename><surname>Chan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nikita</forename><surname>Kitaev</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kelvin</forename><surname>Guu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mitchell</forename><surname>Stern</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jakob</forename><surname>Uszkoreit</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1906.01604</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b6\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">The PASCAL recognising textual entailment challenge</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oren</forename><surname>Ido Dagan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bernardo</forename><surname>Glickman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Magnini</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2006\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b7\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Semi-supervised sequence learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Andrew</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Quoc V</forename><surname>Dai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Le</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in Neural Information Processing Systems (NIPS)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b8\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">BERT: Pre-training of deep bidirectional transformers for language understanding</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jacob</forename><surname>Devlin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ming-Wei</forename><surname>Chang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kristina</forename><surname>Toutanova</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">North American Association for Computational Linguistics (NAACL)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b9\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Automatically constructing a corpus of sentential paraphrases</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>William</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chris</forename><surname>Dolan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Brockett</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the International Workshop on Paraphrasing</title>\n",
       " <meeting>the International Workshop on Paraphrasing</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2005\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b10\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Unified language model pre-training for natural language understanding and generation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Li</forename><surname>Dong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nan</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wenhui</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Furu</forename><surname>Wei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xiaodong</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yu</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jianfeng</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ming</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hsiao-Wuen</forename><surname>Hon</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1905.03197</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b11\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">The third PASCAL recognizing textual entailment challenge</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Danilo</forename><surname>Giampiccolo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bernardo</forename><surname>Magnini</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ido</forename><surname>Dagan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bill</forename><surname>Dolan</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing</title>\n",
       " <meeting>the ACL-PASCAL workshop on textual entailment and paraphrasing</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2007\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b12\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Openwebtext corpus</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Aaron</forename><surname>Gokaslan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Vanya</forename><surname>Cohen</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"http://web.archive.org/save/http://Skylion007.github.io/OpenWebTextCorpus\"/>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b13\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">news-please: A generic news crawler and extractor</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Felix</forename><surname>Hamborg</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Norman</forename><surname>Meuschke</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Corinna</forename><surname>Breitinger</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bela</forename><surname>Gipp</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 15th International Symposium of Information Science</title>\n",
       " <meeting>the 15th International Symposium of Information Science</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b14\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dan</forename><surname>Hendrycks</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kevin</forename><surname>Gimpel</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1606.08415</idno>\n",
       " <title level=\"m\">Gaussian error linear units (gelus)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b15\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matthew</forename><surname>Honnibal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ines</forename><surname>Montani</surname></persName>\n",
       " </author>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " <note>To appear</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b16\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Universal language model fine-tuning for text classification</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeremy</forename><surname>Howard</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sebastian</forename><surname>Ruder</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1801.06146</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b17\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">First quora dataset release: Question pairs. https://data</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Shankar</forename><surname>Iyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nikhil</forename><surname>Dandekar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kornl</forename><surname>Csernai</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"s\">quora.com/First-Quora-Dataset-Release-Question-Pairs</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b18\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mandar</forename><surname>Joshi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Danqi</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yinhan</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Daniel</forename><forename type=\"middle\">S</forename><surname>Weld</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Omer</forename><surname>Levy</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1907.10529</idno>\n",
       " <title level=\"m\">SpanBERT: Improving pre-training by representing and predicting spans</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b19\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Adam: A method for stochastic optimization</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Diederik</forename><surname>Kingma</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jimmy</forename><surname>Ba</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">International Conference on Learning Representations (ICLR)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b20\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">A surprisingly robust trick for winograd schema challenge</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ana-Maria</forename><surname>Vid Kocijan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oana-Maria</forename><surname>Cretu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yordan</forename><surname>Camburu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Thomas</forename><surname>Yordanov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Lukasiewicz</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1905.06290</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b21\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Guokun</forename><surname>Lai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Qizhe</forename><surname>Xie</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hanxiao</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yiming</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Eduard</forename><surname>Hovy</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1704.04683</idno>\n",
       " <title level=\"m\">Race: Large-scale reading comprehension dataset from examinations</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b22\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Guillaume</forename><surname>Lample</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alexis</forename><surname>Conneau</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1901.07291</idno>\n",
       " <title level=\"m\">Crosslingual language model pretraining</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b23\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">The Winograd schema challenge</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Hector</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ernest</forename><surname>Levesque</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Leora</forename><surname>Davis</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Morgenstern</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2011\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b24\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Improving multi-task deep neural networks via knowledge distillation for natural language understanding</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xiaodong</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pengcheng</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Weizhu</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jianfeng</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1904.09482</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b25\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xiaodong</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pengcheng</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Weizhu</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jianfeng</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1901.11504</idno>\n",
       " <title level=\"m\">Multi-task deep neural networks for natural language understanding</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b26\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Learned in translation: Contextualized word vectors</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bryan</forename><surname>Mccann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">James</forename><surname>Bradbury</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Caiming</forename><surname>Xiong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Richard</forename><surname>Socher</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in Neural Information Processing Systems (NIPS)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " <biblScope from=\"6297\" to=\"6308\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b27\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Mixed precision training</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Paulius</forename><surname>Micikevicius</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sharan</forename><surname>Narang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jonah</forename><surname>Alben</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Gregory</forename><surname>Diamos</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Erich</forename><surname>Elsen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">David</forename><surname>Garcia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Boris</forename><surname>Ginsburg</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Michael</forename><surname>Houston</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oleksii</forename><surname>Kuchaiev</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ganesh</forename><surname>Venkatesh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hao</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">International Conference on Learning Representations</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b28\">\n",
       " <monogr>\n",
       " <title/>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sebastian</forename><surname>Nagel</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"http://web.archive.org/save/http://commoncrawl.org/2016/10/news-dataset-available\"/>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b29\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">FAIRSEQ: A fast, extensible toolkit for sequence modeling</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Myle</forename><surname>Ott</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sergey</forename><surname>Edunov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alexei</forename><surname>Baevski</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Angela</forename><surname>Fan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sam</forename><surname>Gross</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nathan</forename><surname>Ng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">David</forename><surname>Grangier</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Michael</forename><surname>Auli</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">North American Association for Computational Linguistics (NAACL): System Demonstrations</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b30\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Scaling neural machine translation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Myle</forename><surname>Ott</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sergey</forename><surname>Edunov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">David</forename><surname>Grangier</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Michael</forename><surname>Auli</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the Third Conference on Machine Translation (WMT)</title>\n",
       " <meeting>the Third Conference on Machine Translation (WMT)</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b31\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Automatic differentiation in PyTorch</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Adam</forename><surname>Paszke</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sam</forename><surname>Gross</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Soumith</forename><surname>Chintala</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Gregory</forename><surname>Chanan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Edward</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zachary</forename><surname>Devito</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zeming</forename><surname>Lin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alban</forename><surname>Desmaison</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luca</forename><surname>Antiga</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Adam</forename><surname>Lerer</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NIPS Autodiff Workshop</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b32\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Deep contextualized word representations</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matthew</forename><surname>Peters</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mark</forename><surname>Neumann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mohit</forename><surname>Iyyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matt</forename><surname>Gardner</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><surname>Clark</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">North American Association for Computational Linguistics (NAACL)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b33\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Improving language understanding with unsupervised learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alec</forename><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Karthik</forename><surname>Narasimhan</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " <pubPlace>OpenAI</pubPlace>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">Technical report</note>\n",
       " <note>Time Salimans, and Ilya Sutskever</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b34\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Language models are unsupervised multitask learners</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alec</forename><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeffrey</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rewon</forename><surname>Child</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">David</forename><surname>Luan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dario</forename><surname>Amodei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ilya</forename><surname>Sutskever</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " <pubPlace>OpenAI</pubPlace>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">Technical report</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b35\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Know what you don't know: Unanswerable questions for squad</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pranav</forename><surname>Rajpurkar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Robin</forename><surname>Jia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Percy</forename><surname>Liang</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Association for Computational Linguistics (ACL)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b36\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">SQuAD: 100,000+ questions for machine comprehension of text</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pranav</forename><surname>Rajpurkar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jian</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Konstantin</forename><surname>Lopyrev</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Percy</forename><surname>Liang</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Empirical Methods in Natural Language Processing (EMNLP)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b37\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Neural machine translation of rare words with subword units</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rico</forename><surname>Sennrich</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Barry</forename><surname>Haddow</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alexandra</forename><surname>Birch</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Association for Computational Linguistics (ACL)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " <biblScope from=\"1715\" to=\"1725\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b38\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Recursive deep models for semantic compositionality over a sentiment treebank</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Richard</forename><surname>Socher</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Perelygin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jean</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jason</forename><surname>Chuang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Christopher</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrew</forename><surname>Manning</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><surname>Ng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Potts</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Empirical Methods in Natural Language Processing</title>\n",
       " <imprint>\n",
       " <publisher>EMNLP</publisher>\n",
       " <date type=\"published\" when=\"2013\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b39\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">MASS: Masked sequence to sequence pre-training for language generation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kaitao</forename><surname>Song</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xu</forename><surname>Tan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tao</forename><surname>Qin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jianfeng</forename><surname>Lu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tie-Yan</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">International Conference on Machine Learning (ICML)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b40\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Shuohuan</forename><surname>Yu Stephanie Sun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yukun</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Shikun</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xuyi</forename><surname>Feng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Han</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xinlun</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Danxiang</forename><surname>Tian</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Zhu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hua</forename><surname>Hao Tian</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1904.09223</idno>\n",
       " <title level=\"m\">ERNIE: Enhanced representation through knowledge integration</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b41\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">A simple method for commonsense reasoning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Trieu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Quoc V</forename><surname>Trinh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Le</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1806.02847</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b42\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Attention is all you need</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ashish</forename><surname>Vaswani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noam</forename><surname>Shazeer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Niki</forename><surname>Parmar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jakob</forename><surname>Uszkoreit</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Llion</forename><surname>Jones</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Aidan</forename><forename type=\"middle\">N</forename><surname>Gomez</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Åukasz</forename><surname>Kaiser</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Illia</forename><surname>Polosukhin</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in neural information processing systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b43\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">SuperGLUE: A stickier benchmark for general-purpose language understanding systems</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yada</forename><surname>Pruksachatkun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nikita</forename><surname>Nangia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Amanpreet</forename><surname>Singh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Julian</forename><surname>Michael</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Felix</forename><surname>Hill</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Omer</forename><surname>Levy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samuel</forename><forename type=\"middle\">R</forename><surname>Bowman</surname></persName>\n",
       " </author>\n",
       " <idno>1905.00537</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b44\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Amanpreet</forename><surname>Singh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Julian</forename><surname>Michael</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Felix</forename><surname>Hill</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Omer</forename><surname>Levy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samuel</forename><forename type=\"middle\">R</forename><surname>Bowman</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">International Conference on Learning Representations</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>ICLR</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b45\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Bowman</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Warstadt</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Amanpreet</forename><surname>Singh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samuel</forename><forename type=\"middle\">R</forename></persName>\n",
       " </author>\n",
       " <idno>1805.12471</idno>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Neural network acceptability judgments</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b46\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A broad-coverage challenge corpus for sentence understanding through inference</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Adina</forename><surname>Williams</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nikita</forename><surname>Nangia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samuel</forename><surname>Bowman</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">North American Association for Computational Linguistics (NAACL)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b47\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhilin</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zihang</forename><surname>Dai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yiming</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jaime</forename><surname>Carbonell</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ruslan</forename><surname>Salakhutdinov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Quoc V</forename><surname>Le</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1906.08237</idno>\n",
       " <title level=\"m\">Xlnet: Generalized autoregressive pretraining for language understanding</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b48\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yang</forename><surname>You</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jing</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jonathan</forename><surname>Hseu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xiaodan</forename><surname>Song</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">James</forename><surname>Demmel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Cho-Jui</forename><surname>Hsieh</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1904.00962</idno>\n",
       " <title level=\"m\">Reducing bert pre-training time from 3 days to 76 minutes</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b49\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rowan</forename><surname>Zellers</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ari</forename><surname>Holtzman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hannah</forename><surname>Rashkin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yonatan</forename><surname>Bisk</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ali</forename><surname>Farhadi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Franziska</forename><surname>Roesner</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yejin</forename><surname>Choi</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1905.12616</idno>\n",
       " <title level=\"m\">Defending against neural fake news</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b50\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yukun</forename><surname>Zhu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ryan</forename><surname>Kiros</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Richard</forename><surname>Zemel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ruslan</forename><surname>Salakhutdinov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Raquel</forename><surname>Urtasun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Antonio</forename><surname>Torralba</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sanja</forename><surname>Fidler</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXivpreprintarXiv:1506.06724</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " </listBibl>\n",
       " </div>\n",
       " </back>\n",
       " </text>\n",
       " </TEI>,\n",
       " <?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       " <TEI xml:space=\"preserve\" xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd\">\n",
       " <teiHeader xml:lang=\"en\">\n",
       " <fileDesc>\n",
       " <titleStmt>\n",
       " <title level=\"a\" type=\"main\">Pronunciation and good language learners</title>\n",
       " </titleStmt>\n",
       " <publicationStmt>\n",
       " <publisher>Cambridge University Press</publisher>\n",
       " <availability status=\"unknown\"><p>Copyright Cambridge University Press</p>\n",
       " </availability>\n",
       " <date type=\"published\" when=\"2020-07-22\">22 Jul 2020</date>\n",
       " </publicationStmt>\n",
       " <sourceDesc>\n",
       " <biblStruct>\n",
       " <analytic>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tom</forename><forename type=\"middle\">B</forename><surname>Brown</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Benjamin</forename><surname>Mann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nick</forename><surname>Ryder</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Melanie</forename><surname>Subbiah</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jared</forename><surname>Kaplan</surname></persName>\n",
       " <affiliation key=\"aff1\">\n",
       " <orgName type=\"institution\">Johns Hopkins University</orgName>\n",
       " <address>\n",
       " <settlement>OpenAI</settlement>\n",
       " </address>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Prafulla</forename><surname>Dhariwal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Arvind</forename><surname>Neelakantan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pranav</forename><surname>Shyam</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Girish</forename><surname>Sastry</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Amanda</forename><surname>Askell</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sandhini</forename><surname>Agarwal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ariel</forename><surname>Herbert-Voss</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Gretchen</forename><surname>Krueger</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tom</forename><surname>Henighan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rewon</forename><surname>Child</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Aditya</forename><surname>Ramesh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Daniel</forename><forename type=\"middle\">M</forename><surname>Ziegler</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeffrey</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><surname>Hesse</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mark</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Eric</forename><surname>Sigler</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mateusz</forename><surname>Litwin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Scott</forename><surname>Gray</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Benjamin</forename><surname>Chess</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jack</forename><surname>Clark</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><surname>Berner</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sam</forename><surname>Mccandlish</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alec</forename><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ilya</forename><surname>Sutskever</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dario</forename><forename type=\"middle\">Amodei</forename><surname>Openai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"department\">Clemens Winter</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <title level=\"a\" type=\"main\">Pronunciation and good language learners</title>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Lessons from Good Language Learners</title>\n",
       " <imprint>\n",
       " <publisher>Cambridge University Press</publisher>\n",
       " <biblScope from=\"197\" to=\"207\" unit=\"page\"/>\n",
       " <date type=\"published\" when=\"2020-07-22\">22 Jul 2020</date>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <idno type=\"DOI\">10.1017/cbo9780511497667.018</idno>\n",
       " <idno type=\"arXiv\">arXiv:2005.14165v4[cs.CL]</idno>\n",
       " </biblStruct>\n",
       " </sourceDesc>\n",
       " </fileDesc>\n",
       " <encodingDesc>\n",
       " <appInfo>\n",
       " <application ident=\"GROBID\" version=\"0.7.0-SNAPSHOT\" when=\"2024-04-27T16:15+0000\">\n",
       " <desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>\n",
       " <ref target=\"https://github.com/kermitt2/grobid\"/>\n",
       " </application>\n",
       " </appInfo>\n",
       " </encodingDesc>\n",
       " <profileDesc>\n",
       " <abstract>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p></div>\n",
       " </abstract>\n",
       " </profileDesc>\n",
       " </teiHeader>\n",
       " <text xml:lang=\"en\">\n",
       " <body>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly flexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word vectors <ref target=\"#b103\" type=\"bibr\">[MCCD13,</ref><ref target=\"#b122\" type=\"bibr\">PSM14]</ref> and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations <ref target=\"#b47\" type=\"bibr\">[DL15,</ref><ref target=\"#b102\" type=\"bibr\">MBXS17,</ref><ref target=\"#b120\" type=\"bibr\">PNZtY18]</ref> (though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models [VSP + 17] have been directly fine-tuned, entirely removing the need for task-specific architectures <ref target=\"#b132\" type=\"bibr\">[RNSS18,</ref><ref target=\"#b43\" type=\"bibr\">DCLT18,</ref><ref target=\"#b65\" type=\"bibr\">HR18]</ref>.</p><p>This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms [RSR + 19, LOG + 19, YDY + 19, LCG + 19]. However, a major limitation to this approach is that while the architecture is task-agnostic, there is still a need for task-specific datasets and task-specific fine-tuning: to achieve strong performance on a desired task typically requires fine-tuning on a dataset of thousands to hundreds of thousands of examples specific to that task. Removing this limitation would be desirable, for several reasons.</p><p>First, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the applicability of language models. There exists a very wide range of possible useful language tasks, encompassing anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story. For many of these tasks it is difficult to collect a large supervised training dataset, especially when the process must be repeated for every new task.</p><p>Second, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution. This can create problems for the pre-training plus fine-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then fine-tuned on very narrow task distributions. For instance [HLW + 20] observe that larger models do not necessarily generalize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly specific to the training distribution and does not generalize well outside it [YdC + 19, MPL19]. Thus, the performance of fine-tuned models on specific benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task [GSL + 18, NK19]. Third, humans do not require large supervised datasets to learn most language tasks -a brief directive in natural language (e.g. \"please tell me if this sentence describes something happy or something sad\") or at most a tiny number of demonstrations (e.g. \"here are two examples of people acting brave; please give a third example of bravery\") is often Figure <ref target=\"#fig_11\" type=\"figure\">1</ref>.1: Language model meta-learning. During unsupervised pre-training, a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term \"in-context learning\" to describe the inner loop of this process, which occurs within the forward-pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre-training, but are intended to show that there are sometimes repeated sub-tasks embedded within a single sequence. Figure <ref target=\"#fig_11\" type=\"figure\">1</ref>.2: Larger models make increasingly efficient use of in-context information. We show in-context learning performance on a simple task requiring the model to remove random symbols from a word, both with and without a natural language task description (see Sec. 3.9.2). The steeper \"in-context learning curves\" for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks.</p><p>sufficient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages -it allows humans to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy dialogue. To be broadly useful, we would someday like our NLP systems to have this same fluidity and generality.</p><p>One potential route towards addressing these issues is meta-learning 1 -which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure <ref target=\"#fig_11\" type=\"figure\">1</ref>.1). Recent work [RWC + 19] attempts to do this via what we call \"in-context learning\", using the text input of a pretrained language model as a form of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.</p><p>While it has shown some initial promise, this approach still achieves results far inferior to fine-tuning -for example [RWC + 19] achieves only 4% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks. Another recent trend in language modeling may offer a way forward. In recent years the capacity of transformer language models has increased substantially, from 100 million parameters <ref target=\"#b132\" type=\"bibr\">[RNSS18]</ref>, to 300 million parameters <ref target=\"#b43\" type=\"bibr\">[DCLT18]</ref>, to 1.5 billion parameters [RWC + 19], to 8 billion parameters <ref type=\"bibr\">[SPP +</ref> 19], 11 billion parameters [RSR + 19], and finally 17 billion parameters <ref target=\"#b152\" type=\"bibr\">[Tur20]</ref>. Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale <ref type=\"bibr\">[KMH + 20]</ref>. Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale.</p><p>Figure <ref target=\"#fig_11\" type=\"figure\">1</ref>.3: Aggregate performance for all 42 accuracy-denominated benchmarks While zero-shot performance improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are more proficient at in-context learning. See Figure <ref target=\"#fig_1\" type=\"figure\">3</ref>.8 for a more detailed analysis on SuperGLUE, a standard NLP benchmark suite.</p><p>In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities. Specifically, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set. For each task, we evaluate GPT-3 under 3 conditions: (a) \"few-shot learning\", or in-context learning where we allow as many demonstrations as will fit into the model's context window (typically 10 to 100), (b) \"one-shot learning\", where we allow only one demonstration, and (c) \"zero-shot\" learning, where no demonstrations are allowed and only an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional fine-tuning setting, but we leave this to future work. Figure <ref target=\"#fig_11\" type=\"figure\">1</ref>.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model's context, K. Few-shot learning also improves dramatically with model size. Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study. We emphasize that these \"learning\" curves involve no gradient updates or fine-tuning, just increasing numbers of demonstrations given as conditioning.</p><p>Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by fine-tuned models). For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art relative to fine-tuned models operating in the same closed-book setting.</p><p>GPT-3 also displays one-shot and few-shot proficiency at tasks designed to test rapid adaption or on-the-fly reasoning, which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them defined only once. We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human evaluators have difficulty distinguishing from human-generated articles.</p><p>At the same time, we also find some tasks on which few-shot performance struggles, even at the scale of GPT-3. This includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE or QuAC. By presenting a broad characterization of GPT-3's strengths and weaknesses, including these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed.</p><p>A heuristic sense of the overall results can be seen in Figure <ref target=\"#fig_11\" type=\"figure\">1</ref>.3, which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself).</p><p>We also undertake a systematic study of \"data contamination\" -a growing problem when training high capacity models on datasets such as Common Crawl, which can potentially include content from test datasets simply because such content often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify its distorting effects. Although we find that data contamination has a minimal effect on GPT-3's performance on most datasets, we do identify a few datasets where it could be inflating results, and we either do not report results on these datasets or we note them with an asterisk, depending on the severity.</p><p>In addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion parameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings. Broadly, for most tasks we find relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models are more proficient meta-learners.</p><p>Finally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3's characteristics in this regard.</p><p>The remainder of this paper is organized as follows. In Section 2, we describe our approach and methods for training GPT-3 and evaluating it. Section 3 presents results on the full range of tasks in the zero-, one-and few-shot settings. Section 4 addresses questions of data contamination (train-test overlap). Section 5 discusses limitations of GPT-3. Section 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">Approach</head><p>Our basic pre-training approach, including model, data, and training, is similar to the process described in [RWC + 19], with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training. Our use of in-context learning is also similar to [RWC + 19], but in this work we systematically explore different settings for learning within the context. Therefore, we start this section by explicitly defining and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-specific data they tend to rely on. Specifically, we can identify at least four points on this spectrum (see Figure <ref target=\"#fig_28\" type=\"figure\">2</ref>.1 for an illustration):</p><p>â€¢ Fine-Tuning (FT) has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task. Typically thousands to hundreds of thousands of labeled examples are used. The main advantage of fine-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution <ref target=\"#b109\" type=\"bibr\">[MPL19]</ref>, and the potential to exploit spurious features of the training data [GSL + 18, NK19], potentially resulting in an unfair comparison with human performance. In this work we do not fine-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be fine-tuned in principle and this is a promising direction for future work.</p><p>â€¢ Few-Shot (FS) is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning [RWC + 19], but no weight updates are allowed. As shown in Figure <ref target=\"#fig_28\" type=\"figure\">2</ref>.1, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving K examples of context and completion, and then one final example of context, with the model expected to provide the completion. We typically set K in the range of 10 to 100 as this is how many examples can fit in the model's context window (n ctx = 2048). The main advantages of few-shot are a major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art fine-tuned models. Also, a small amount of task specific data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML [HYC01, VBL + 16] -both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task.</p><p>â€¢ One-Shot (1S) is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure <ref target=\"#fig_11\" type=\"figure\">1</ref>. The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans. For example, when asking humans to generate a dataset on a human worker service (for example Mechanical Turk), it is common to give one demonstration of the task. By contrast it is sometimes difficult to communicate the content or format of a task if no examples are given.</p><p>Figure <ref target=\"#fig_28\" type=\"figure\">2</ref>.1: Zero-shot, one-shot and few-shot, contrasted with traditional fine-tuning. The panels above show four methods for performing a task with a language model -fine-tuning is the traditional method, whereas zero-, one-, and few-shot, which we study in this work, require the model to perform the task with only forward passes at test time. We typically present the model with a few dozen examples in the few shot setting. Exact phrasings for all task descriptions, examples and prompts can be found in Appendix G.</p><p>â€¢ Zero-Shot (0S) is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task. This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of pre-training data), but is also the most challenging setting. In some cases it may even be difficult for humans to understand the format of the task without prior examples, so this setting is in some cases \"unfairly hard\". For example, if someone is asked to \"make a Figure <ref target=\"#fig_28\" type=\"figure\">2</ref>.1 shows the four methods using the example of translating English to French. In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specific benchmarks and sample efficiency.</p><p>We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art fine-tuned models.</p><p>Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work. Table <ref target=\"#tab_2\" type=\"table\">2</ref>.1: Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of the models which we trained. All models were trained for a total of 300 billion tokens.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.1\">Model and Architectures</head><p>We use the same model and architecture as GPT-2 [RWC + 19], including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer <ref target=\"#b38\" type=\"bibr\">[CGRS19]</ref>. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work <ref type=\"bibr\">[KMH + 20]</ref> suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks.</p><p>Table <ref target=\"#tab_2\" type=\"table\">2</ref>.1 shows the sizes and architectures of our 8 models. Here n params is the total number of trainable parameters, n layers is the total number of layers, d model is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer, d ff = 4 * d model ), and d head is the dimension of each attention head. All models use a context window of n ctx = 2048 tokens. We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models across GPU's. Previous work <ref type=\"bibr\">[KMH + 20]</ref> suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Training Dataset</head><p>Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset 2 [RSR + 19] constituting nearly a trillion words. This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice. However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets. Therefore, we took 3 steps to improve the average quality of our datasets:</p><p>(1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.</p><p>Details of the first two points (processing of Common Crawl) are described in Appendix A. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset [RWC + 19], collected by scraping links over a longer period of time, and first described in <ref type=\"bibr\">[KMH + 20]</ref>, two internet-based books corpora (Books1 and Books2) and English-language Wikipedia.</p><p>Table <ref target=\"#tab_2\" type=\"table\">2</ref>.2 shows the final mixture of datasets that we used in training. The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens. Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overfitting in exchange for higher quality training data. that are drawn from a given dataset, which we intentionally do not make proportional to the size of the dataset. As a result, when we train for 300 billion tokens, some datasets are seen up to 3.4 times during training while other datasets are seen less than once.</p><p>A major methodological concern with language models pretrained on a broad swath of internet data, particularly large models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by having their test or development sets inadvertently seen during pre-training. To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper.</p><p>Unfortunately, a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. In Section 4 we characterize the impact of the remaining overlaps, and in future work we will more aggressively remove data contamination.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.3\">Training Process</head><p>As found in <ref type=\"bibr\">[KMH + 20,</ref><ref target=\"#b106\" type=\"bibr\">MKAT18]</ref>, larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size <ref target=\"#b106\" type=\"bibr\">[MKAT18]</ref>. Table <ref target=\"#tab_2\" type=\"table\">2</ref>.1 shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network. All models were trained on V100 GPU's on part of a high-bandwidth cluster provided by Microsoft. Details of the training process and hyperparameter settings are described in Appendix B.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.4\">Evaluation</head><p>For few-shot learning, we evaluate each example in the evaluation set by randomly drawing K examples from that task's training set as conditioning, delimited by 1 or 2 newlines depending on the task. For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set. For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it.</p><p>K can be any value from 0 to the maximum amount allowed by the model's context window, which is n ctx = 2048 for all models and typically fits 10 to 100 examples. Larger values of K are usually but not always better, so when a separate development and test set are available, we experiment with a few values of K on the development set and then run the best value on the test set. For some tasks (see Appendix G) we also use a natural language prompt in addition to (or for K = 0, instead of) demonstrations.</p><p>On tasks that involve choosing one correct completion from several options (multiple choice), we provide K examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion. For most tasks we compare the per-token likelihood (to normalize for length), however on a small number of datasets (ARC, OpenBookQA, and RACE) we gain additional benefit as measured on the development set by normalizing by the unconditional probability of each completion, by computing</p><formula xml:id=\"formula_0\">P (completion|context) P (completion|answer context) ,</formula><p>where answer context is the string \"Answer: \" or \"A: \" and is used to prompt that the completion should be an answer but is otherwise generic.</p><p>On tasks that involve binary classification, we give the options more semantically meaningful names (e.g. \"True\" or \"False\" rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by [RSR + 19] (see Appendix G) for details.</p><p>On tasks with free-form completion, we use beam search with the same parameters as [RSR + 19]: a beam width of 4 and a length penalty of Î± = 0.6. We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand.</p><p>Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot). When the test set is private, our model is often too large to fit on the test server, so we report results on the development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3\">Results</head><p>In Figure <ref target=\"#fig_1\" type=\"figure\">3</ref>.1 we display training curves for the 8 models described in Section 2. For this graph we also include 6 additional extra-small models with as few as 100,000 parameters. As observed in <ref type=\"bibr\">[KMH + 20]</ref>, language modeling performance follows a power-law when making efficient use of training compute. After extending this trend by two more orders of magnitude, we observe only a slight (if any) departure from the power-law. One might worry that these improvements in cross-entropy loss come only from modeling spurious details of our training corpus. However, we will see in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks. Below, we evaluate the 8 models described in Section 2 (the 175 billion parameter parameter GPT-3 and 7 smaller models) on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks.</p><p>In Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling, such as Cloze tasks and sentence/paragraph completion tasks. In Section 3.2 we evaluate on \"closed book\" question answering tasks: tasks which require using the information stored in the model's parameters to answer general knowledge questions. In Section 3.3 we evaluate the model's ability to translate between languages (especially one-shot and few-shot). In Section 3.4 we evaluate the model's performance on Winograd Schema-like tasks. In Section 3.5 we evaluate on datasets that involve commonsense reasoning or question answering. In Section 3.6 we evaluate on reading comprehension tasks, in Section 3.7 we evaluate on the SuperGLUE benchmark suite, and in 3.8 we briefly explore NLI. Finally, in Section 3.9, we invent some additional tasks designed especially to probe in-context learning abilitiesthese tasks focus on on-the-fly reasoning, adaptation skills, or open-ended text synthesis. We evaluate all tasks in the few-shot, one-shot, and zero-shot settings.  </p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1\">Language Modeling, Cloze, and Completion Tasks</head><p>In this section we test GPT-3's performance on the traditional task of language modeling, as well as related tasks that involve predicting a single word of interest, completing a sentence or paragraph, or choosing between possible completions of a piece of text.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1.1\">Language Modeling</head><p>We  On LAMBADA, the few-shot capability of language models results in a strong boost to accuracy. GPT-3 2.7B outperforms the SOTA 17B parameter Turing-NLG <ref target=\"#b152\" type=\"bibr\">[Tur20]</ref> in this setting, and GPT-3 175B advances the state of the art by 18%. Note zero-shot uses a different format from one-shot and few-shot as described in the text.</p><p>and <ref target=\"#b152\" type=\"bibr\">[Tur20]</ref>) and argue that \"continuing to expand hardware and data sizes by orders of magnitude is not the path forward\". We find that path is still promising and in a zero-shot setting GPT-3 achieves 76% on LAMBADA, a gain of 8% over the previous state of the art.</p><p>LAMBADA is also a demonstration of the flexibility of few-shot learning as it provides a way to address a problem that classically occurs with this dataset. Although the completion in LAMBADA is always the last word in a sentence, a standard language model has no way of knowing this detail. It thus assigns probability not only to the correct ending but also to other valid continuations of the paragraph. This problem has been partially addressed in the past with stop-word filters [RWC + 19] (which ban \"continuation\" words). The few-shot setting instead allows us to \"frame\" the task as a cloze-test and allows the language model to infer from examples that a completion of exactly one word is desired. We use the following fill-in-the-blank format:</p><p>Alice was friends with Bob. Alice went to visit her friend . â†’ Bob George bought some baseball equipment, a ball, a glove, and a . â†’</p><p>When presented with examples formatted this way, GPT-3 achieves 86.4% accuracy in the few-shot setting, an increase of over 18% from the previous state-of-the-art. We observe that few-shot performance improves strongly with model size. While this setting decreases the performance of the smallest model by almost 20%, for GPT-3 it improves accuracy by 10%. Finally, the fill-in-blank method is not effective one-shot, where it always performs worse than the zero-shot setting. Perhaps this is because all models still require several examples to recognize the pattern. One note of caution is that an analysis of test set contamination identified that a significant minority of the LAMBADA dataset appears to be present in our training data -however analysis performed in Section 4 suggests negligible impact on performance.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1.3\">HellaSwag</head><p>The HellaSwag dataset [ZHB + 19] involves picking the best ending to a story or set of instructions. The examples were adversarially mined to be difficult for language models while remaining easy for humans (who achieve 95.6% accuracy). GPT-3 achieves 78.1% accuracy in the one-shot setting and 79.3% accuracy in the few-shot setting, outperforming the 75.4% accuracy of a fine-tuned 1.5B parameter language model <ref type=\"bibr\">[ZHR +</ref> 19] but still a fair amount lower than the overall SOTA of 85.6% achieved by the fine-tuned multi-task model ALUM.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1.4\">StoryCloze</head><p>We next evaluate GPT-3 on the StoryCloze 2016 dataset [MCH + 16], which involves selecting the correct ending sentence for five-sentence long stories. Here GPT-3 achieves 83.2% in the zero-shot setting and 87.7% in the few-shot setting (with K = 70). This is still 4.1% lower than the fine-tuned SOTA using a BERT based model <ref target=\"#b86\" type=\"bibr\">[LDL19]</ref> but improves over previous zero-shot results by roughly 10%.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Closed Book Question Answering</head><p>In this section we measure GPT-3's ability to answer questions about broad factual knowledge. Due to the immense amount of possible queries, this task has normally been approached by using an information retrieval system to find relevant text in combination with a model which learns to generate an answer given the question and the retrieved text. Since this setting allows a system to search for and condition on text which potentially contains the answer it is denoted \"open-book\". <ref target=\"#b135\" type=\"bibr\">[RRS20]</ref> recently demonstrated that a large language model can perform surprisingly well directly answering the questions without conditioning on auxilliary information. They denote this more restrictive evaluation setting as \"closed-book\". Their work suggests that even higher-capacity models could perform even better and we test this hypothesis with GPT-3. We evaluate GPT-3 on the 3 datasets in [RRS20]: Natural Questions [KPR + 19], WebQuestions <ref target=\"#b28\" type=\"bibr\">[BCFL13]</ref>, and TriviaQA [JCWZ17], using the same splits. Note that in addition to all results being in the closed-book setting, our use of few-shot, one-shot, and zero-shot evaluations represent an even stricter setting than previous closed-book QA work: in addition to external content not being allowed, fine-tuning on the Q&amp;A dataset itself is also not permitted.</p><p>The results for GPT-3 are shown in GPT-3's few-shot result further improves performance another 3.2% beyond this.</p><p>On WebQuestions (WebQs), GPT-3 achieves 14.4% in the zero-shot setting, 25.3% in the one-shot setting, and 41.5% in the few-shot setting. This compares to 37.4% for fine-tuned T5-11B, and 44.7% for fine-tuned T5-11B+SSM, which uses a Q&amp;A-specific pre-training procedure. GPT-3 in the few-shot setting approaches the performance of state-of-the-art fine-tuned models. Notably, compared to TriviaQA, WebQS shows a much larger gain from zero-shot to few-shot (and indeed its zero-shot and one-shot performance are poor), perhaps suggesting that the WebQs questions and/or the style of their answers are out-of-distribution for GPT-3. Nevertheless, GPT-3 appears able to adapt to this distribution, recovering strong performance in the few-shot setting.</p><p>On Natural Questions (NQs) GPT-3 achieves 14.6% in the zero-shot setting, 23.0% in the one-shot setting, and 29.9% in the few-shot setting, compared to 36.6% for fine-tuned T5 11B+SSM. Similar to WebQS, the large gain from zero-shot to few-shot may suggest a distribution shift, and may also explain the less competitive performance compared to TriviaQA and WebQS. In particular, the questions in NQs tend towards very fine-grained knowledge on Wikipedia specifically which could be testing the limits of GPT-3's capacity and broad pretraining distribution.</p><p>Overall, on one of the three datasets GPT-3's one-shot matches the open-domain fine-tuning SOTA. On the other two datasets it approaches the performance of the closed-book SOTA despite not using fine-tuning. On all 3 datasets, we find that performance scales very smoothly with model size (Figure <ref target=\"#fig_1\" type=\"figure\">3</ref>.3 and Appendix H Figure H.7), possibly reflecting the idea that model capacity translates directly to more 'knowledge' absorbed in the parameters of the model.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.3\">Translation</head><p>For GPT-2 a filter was used on a multilingual collection of documents to produce an English only dataset due to capacity concerns. Even with this filtering GPT-2 showed some evidence of multilingual capability and performed non-trivially when translating between French and English despite only training on 10 megabytes of remaining French text. Since we increase the capacity by over two orders of magnitude from GPT-2 to GPT-3, we also expand the scope of the training dataset to include more representation of other languages, though this remains an area for further improvement. As discussed in 2.2 the majority of our data is derived from raw Common Crawl with only quality-based filtering. Although GPT-3's training data is still primarily English (93% by word count), it also includes 7% of text in other languages. These languages are documented in the supplemental material. In order to better understand translation capability, we also expand our analysis to include two additional commonly studied languages, German and Romanian.</p><p>Existing unsupervised machine translation approaches often combine pretraining on a pair of monolingual datasets with back-translation <ref target=\"#b143\" type=\"bibr\">[SHB15]</ref> to bridge the two languages in a controlled way. By contrast, GPT-3 learns from a blend of training data that mixes many languages together in a natural way, combining them on a word, sentence, and document level. GPT-3 also uses a single training objective which is not customized or designed for any task in particular. However, our one / few-shot settings aren't strictly comparable to prior unsupervised work since they make use of a small amount of paired examples (1 or 64). This corresponds to up to a page or two of in-context training data.</p><p>Results are shown in  Scaling is relatively smooth with the gains to few-shot learning increasing with model size, and few-shot GPT-3 175B is competitive with a fine-tuned RoBERTA-large.</p><p>each translation task improves performance by over 7 BLEU and nears competitive performance with prior work. GPT-3 in the full few-shot setting further improves another 4 BLEU resulting in similar average performance to prior unsupervised NMT work. GPT-3 has a noticeable skew in its performance depending on language direction. For the three input languages studied, GPT-3 significantly outperforms prior unsupervised NMT work when translating into English but underperforms when translating in the other direction. Performance on En-Ro is a noticeable outlier at over 10 BLEU worse than prior unsupervised NMT work. This could be a weakness due to reusing the byte-level BPE tokenizer of GPT-2 which was developed for an almost entirely English training dataset. For both Fr-En and De-En, few shot GPT-3 outperforms the best supervised result we could find but due to our unfamiliarity with the literature and the appearance that these are un-competitive benchmarks we do not suspect those results represent true state of the art. For Ro-En, few shot GPT-3 performs within 0.5 BLEU of the overall SOTA which is achieved by a combination of unsupervised pretraining, supervised finetuning on 608K labeled examples, and backtranslation <ref target=\"#b92\" type=\"bibr\">[LHCG19b]</ref>.</p><p>Finally, across all language pairs and across all three settings (zero-, one-, and few-shot), there is a smooth trend of improvement with model capacity. This is shown in Figure <ref target=\"#fig_1\" type=\"figure\">3</ref>.4 in the case of few-shot results, and scaling for all three settings is shown in Appendix H.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.4\">Winograd-Style Tasks</head><p>The Winograd Schemas Challenge <ref target=\"#b87\" type=\"bibr\">[LDM12]</ref> is a classical task in NLP that involves determining which word a pronoun refers to, when the pronoun is grammatically ambiguous but semantically unambiguous to a human. Table <ref target=\"#tab_3\" type=\"table\">3</ref>.6: GPT-3 results on three commonsense reasoning tasks, PIQA, ARC, and OpenBookQA. GPT-3 Few-Shot PIQA result is evaluated on the test server. See Section 4 for details on potential contamination issues on the PIQA test set.</p><p>Figure <ref target=\"#fig_1\" type=\"figure\">3</ref>.6: GPT-3 results on PIQA in the zero-shot, one-shot, and few-shot settings. The largest model achieves a score on the development set in all three conditions that exceeds the best recorded score on the task.</p><p>such as the adversarially-mined Winogrande dataset <ref target=\"#b138\" type=\"bibr\">[SBBC19]</ref> still significantly lag human performance. We test GPT-3's performance on both Winograd and Winogrande, as usual in the zero-, one-, and few-shot setting.</p><p>On Winograd we test GPT-3 on the original set of 273 Winograd schemas, using the same \"partial evaluation\" method described in [RWC + 19]. Note that this setting differs slightly from the WSC task in the SuperGLUE benchmark, which is presented as binary classification and requires entity extraction to convert to the form described in this section. On Winograd GPT-3 achieves 88.3%, 89.7%, and 88.6% in the zero-shot, one-shot, and few-shot settings, showing no clear in-context learning but in all cases achieving strong results just a few points below state-of-the-art and estimated human performance. We note that contamination analysis found some Winograd schemas in the training data but this appears to have only a small effect on results (see Section 4).</p><p>On the more difficult Winogrande dataset, we do find gains to in-context learning: GPT-3 achieves 70.2% in the zero-shot setting, 73.2% in the one-shot setting, and 77.7% in the few-shot setting. For comparison a fine-tuned RoBERTA model achieves 79%, state-of-the-art is 84.6% achieved with a fine-tuned high capacity model (T5), and human performance on the task as reported by <ref target=\"#b138\" type=\"bibr\">[SBBC19]</ref> is 94.0%.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.5\">Common Sense Reasoning</head><p>Next we consider three datasets which attempt to capture physical or scientific reasoning, as distinct from sentence completion, reading comprehension, or broad knowledge question answering. The first, PhysicalQA (PIQA) <ref type=\"bibr\">[BZB +</ref> 19], asks common sense questions about how the physical world works and is intended as a probe of grounded understanding of the world. GPT-3 achieves 81.0% accuracy zero-shot, 80.5% accuracy one-shot, and 82.8% accuracy few-shot (the last measured on PIQA's test server). This compares favorably to the 79.4% accuracy prior state-of-the-art of a fine-tuned RoBERTa. PIQA shows relatively shallow scaling with model size and is still over 10% worse than human performance, but GPT-3's few-shot and even zero-shot result outperform the current state-of-the-art. Our analysis flagged PIQA for a potential data contamination issue (despite hidden test labels), and we therefore conservatively mark the result with an asterisk. See Section 4 for details.</p><p>ARC [CCE + 18] is a dataset of multiple-choice questions collected from 3rd to 9th grade science exams. On the \"Challenge\" version of the dataset which has been filtered to questions which simple statistical or information retrieval methods are unable to correctly answer, GPT-3 achieves 51.4% accuracy in the zero-shot setting, 53.2% in the one-shot setting, and 51.5% in the few-shot setting. This is approaching the performance of a fine-tuned RoBERTa baseline (55.9%) from UnifiedQA [KKS + 20]. On the \"Easy\" version of the dataset (questions which either of the mentioned baseline approaches answered correctly), GPT-3 achieves 68.8%, 71.2%, and 70.1% which slightly exceeds a fine-tuned RoBERTa baseline from [KKS + 20]. However, both of these results are still much worse than the overall SOTAs achieved by the UnifiedQA which exceeds GPT-3's few-shot results by 27% on the challenge set and 22% on the easy set.</p><p>On OpenBookQA <ref target=\"#b105\" type=\"bibr\">[MCKS18]</ref>, GPT-3 improves significantly from zero to few shot settings but is still over 20 points short of the overall SOTA. GPT-3's few-shot performance is similar to a fine-tuned BERT Large baseline on the leaderboard.</p><p>Overall, in-context learning with GPT-3 shows mixed results on commonsense reasoning tasks, with only small and inconsistent gains observed in the one and few-shot learning settings for both PIQA and ARC, but a significant improvement is observed on OpenBookQA. GPT-3 sets SOTA on the new PIQA dataset in all evaluation settings.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.6\">Reading Comprehension</head><p>Next we evaluate GPT-3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive, multiple choice, and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT-3's performance across these datasets suggestive of varying capability with different answer formats. In general we observe GPT-3 is on par with initial baselines and early results trained using contextual representations on each respective dataset.</p><p>GPT-3 performs best (within 3 points of the human baseline) on CoQA [RCM19] a free-form conversational dataset and performs worst (13 F1 below an ELMo baseline) on QuAC [CHI + 18] a dataset which requires modeling structured dialog acts and answer span selections of teacher-student interactions. On DROP [DWD + 19], a dataset testing discrete reasoning and numeracy in the context of reading comprehension, GPT-3 in a few-shot setting outperforms the fine-tuned BERT baseline from the original paper but is still well below both human performance and state-of-the-art approaches which augment neural networks with symbolic systems <ref type=\"bibr\">[RLL +</ref> 19]. On SQuAD 2.0 [RJL18], GPT-3 demonstrates its few-shot learning capabilities, improving by almost 10 F1 (to 69.8) compared to a zero-shot setting. This allows it to slightly outperform the best fine-tuned result in the original paper. On RACE <ref type=\"bibr\">[LXL +</ref> 17], a multiple choice dataset of middle school and high school english examinations, GPT-3 performs relatively weakly and is only competitive with the earliest work utilizing contextual representations and is still 45% behind SOTA.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.7\">SuperGLUE</head><p>In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark We observe a wide range in GPT-3's performance across tasks. On COPA and ReCoRD GPT-3 achieves near-SOTA performance in the one-shot and few-shot settings, with COPA falling only a couple points short and achieving second place on the leaderboard, where first place is held by a fine-tuned 11 billion parameter model (T5). On WSC, performance is still relatively strong, achieving 80.1% in the few-shot setting (note that GPT-3 achieves 88.6% on the original Winograd dataset as described in Section 3.4). On BoolQ, MultiRC, and RTE, performance is reasonable, roughly matching that of a fine-tuned BERT-Large. On CB, we see signs of life at 75.6% in the few-shot setting.</p><formula xml:id=\"formula_1\">[</formula><p>WiC is a notable weak spot with few-shot performance at 49.4% (at random chance). We tried a number of different phrasings and formulations for WiC (which involves determining if a word is being used with the same meaning in two sentences), none of which was able to achieve strong performance. This hints at a phenomenon that will become clearer in the next section (which discusses the ANLI benchmark) -GPT-3 appears to be weak in the few-shot or one-shot setting at some tasks that involve comparing two sentences or snippets, for example whether a word is used the same way in two sentences (WiC), whether one sentence is a paraphrase of another, or whether one sentence implies another. This could also explain the comparatively low scores for RTE and CB, which also follow this format. Despite these weaknesses, GPT-3 still outperforms a fine-tuned BERT-large on four of eight tasks and on two tasks GPT-3 is close to the state-of-the-art held by a fine-tuned 11 billion parameter model.</p><p>Finally, we note that the few-shot SuperGLUE score steadily improves with both model size and with number of examples in the context showing increasing benefits from in-context learning (Figure <ref target=\"#fig_1\" type=\"figure\">3</ref>.8). We scale K up to 32 examples per task, after which point additional examples will not reliably fit into our context. When sweeping over values of K, we find that GPT-3 requires less than eight total examples per task to outperform a fine-tuned BERT-Large on overall SuperGLUE score.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.8\">NLI</head><p>Natural Language Inference (NLI) <ref target=\"#b54\" type=\"bibr\">[Fyo00]</ref> concerns the ability to understand the relationship between two sentences. In practice, this task is usually structured as a two or three class classification problem where the model classifies ). We find that smaller models hover around random chance, while few-shot GPT-3 175B closes almost half the gap from random chance to SOTA. Results for ANLI rounds 1 and 2 are shown in the appendix.</p><p>whether the second sentence logically follows from the first, contradicts the first sentence, or is possibly true (neutral). SuperGLUE includes an NLI dataset, RTE, which evaluates the binary version of the task. On RTE, only the largest version of GPT-3 performs convincingly better than random (56%) in any evaluation setting, but in a few-shot setting GPT-3 performs similarly to a single-task fine-tuned BERT Large. We also evaluate on the recently introduced Adversarial Natural Language Inference (ANLI) dataset [NWD + 19]. ANLI is a difficult dataset employing a series of adversarially mined natural language inference questions in three rounds (R1, R2, and R3). Similar to RTE, all of our models smaller than GPT-3 perform at almost exactly random chance on ANLI, even in the few-shot setting (âˆ¼ 33%), whereas GPT-3 itself shows signs of life on Round 3. Results for ANLI R3 are highlighted in Figure <ref target=\"#fig_1\" type=\"figure\">3</ref>.9 and full results for all rounds can be found in Appendix H. These results on both RTE and ANLI suggest that NLI is still a very difficult task for language models and they are only just beginning to show signs of progress.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.9\">Synthetic and Qualitative Tasks</head><p>One way to probe GPT-3's range of abilities in the few-shot (or zero-and one-shot) setting is to give it tasks which require it to perform simple on-the-fly computational reasoning, recognize a novel pattern that is unlikely to have occurred in training, or adapt quickly to an unusual task. We devise several tasks to test this class of abilities. First, we test GPT-3's ability to perform arithmetic. Second, we create several tasks that involve rearranging or unscrambling the letters in a word, tasks which are unlikely to have been exactly seen during training. Third, we test GPT-3's ability to solve SAT-style analogy problems few-shot. Finally, we test GPT-3 on several qualitative tasks, including using new words in a sentence, correcting English grammar, and news article generation. We will release the synthetic datasets with the hope of stimulating further study of test-time behavior of language models.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.9.1\">Arithmetic</head><p>To test GPT-3's ability to perform simple arithmetic operations without task-specific training, we developed a small battery of 10 tests that involve asking GPT-3 a simple arithmetic problem in natural language:</p><p>â€¢ 2 digit addition (2D+) -The model is asked to add two integers sampled uniformly from [0, 100), phrased in the form of a question, e.g. \"Q: What is 48 plus 76? A: 124.\" â€¢ 2 digit subtraction (2D-) -The model is asked to subtract two integers sampled uniformly from [0, 100); the answer may be negative. Example: \"Q: What is 34 minus 53? A: -19\". â€¢ 3 digit addition (3D+) -Same as 2 digit addition, except numbers are uniformly sampled from [0, 1000).</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 3.10:</head><p>Results on all 10 arithmetic tasks in the few-shot settings for models of different sizes. There is a significant jump from the second largest model (GPT-3 13B) to the largest model (GPT-3 175), with the latter being able to reliably accurate 2 digit arithmetic, usually accurate 3 digit arithmetic, and correct answers a significant fraction of the time on 4-5 digit arithmetic, 2 digit multiplication, and compound operations. Results for one-shot and zero-shot are shown in the appendix.</p><p>â€¢ 3 digit subtraction (3D-) -Same as 2 digit subtraction, except numbers are uniformly sampled from [0, 1000).</p><p>â€¢ 4 digit addition (4D+) -Same as 3 digit addition, except uniformly sampled from [0, 10000).</p><p>â€¢ 4 digit subtraction (4D-) -Same as 3 digit subtraction, except uniformly sampled from [0, 10000).</p><p>â€¢ 5 digit addition (5D+) -Same as 3 digit addition, except uniformly sampled from [0, 100000).</p><p>â€¢ 5 digit subtraction (5D-) -Same as 3 digit subtraction, except uniformly sampled from [0, 100000).</p><p>â€¢ 2 digit multiplication (2Dx) -The model is asked to multiply two integers sampled uniformly from [0, 100), e.g. \"Q: What is 24 times 42? A: 1008\".</p><p>â€¢ One-digit composite (1DC) -The model is asked to perform a composite operation on three 1 digit numbers, with parentheses around the last two. For example, \"Q: What is 6+(4*8)? A: 38\". The three 1 digit numbers are selected uniformly on [0, 10) and the operations are selected uniformly from {+,-,*}.</p><p>In all 10 tasks the model must generate the correct answer exactly. For each task we generate a dataset of 2,000 random instances of the task and evaluate all models on those instances.</p><p>First we evaluate GPT-3 in the few-shot setting, for which results are shown in Figure <ref target=\"#fig_1\" type=\"figure\">3</ref>.10. On addition and subtraction, GPT-3 displays strong proficiency when the number of digits is small, achieving 100% accuracy on 2 digit addition, 98.9% at 2 digit subtraction, 80.2% at 3 digit addition, and 94.2% at 3-digit subtraction. Performance decreases as the number of digits increases, but GPT-3 still achieves 25-26% accuracy on four digit operations and 9-10% accuracy on five digit operations, suggesting at least some capacity to generalize to larger numbers of digits. GPT-3 also achieves 29.2% accuracy at 2 digit multiplication, an especially computationally intensive operation. Finally, GPT-3 achieves 21.3% accuracy at single digit combined operations (for example, 9*(7+5)), suggesting that it has some robustness beyond just single operations.</p><p>As Figure <ref target=\"#fig_1\" type=\"figure\">3</ref>.10 makes clear, small models do poorly on all of these tasks -even the 13 billion parameter model (the second largest after the 175 billion full GPT-3) can solve 2 digit addition and subtraction only half the time, and all other operations less than 10% of the time.</p><p>One-shot and zero-shot performance are somewhat degraded relative to few-shot performance, suggesting that adaptation to the task (or at the very least recognition of the task) is important to performing these computations correctly. Nevertheless, one-shot performance is still quite strong, and even zero-shot performance of the full GPT-3 significantly</p><formula xml:id=\"formula_2\">Setting 2D+ 2D-3D+ 3D-4D+ 4D-5D+ 5D-2Dx 1DC</formula><p>GPT-3 Zero-shot 76.9 58.0 34.2 48.3 4.0 7.5 0.7 0.8 19.8 9.8 GPT-3 One-shot 99.6 86.4 65.5 78.7 14.0 14.0 3.5 3.8 27.4 14.3 GPT-3 Few-shot 100.0 98.9 80.4 94.2 25.5 26.8 9.3 9.9 29.2 21.3 Table <ref target=\"#tab_3\" type=\"table\">3</ref>.9: Results on basic arithmetic tasks for GPT-3 175B. {2,3,4,5}D{+,-} is 2, 3, 4, and 5 digit addition or subtraction, 2Dx is 2 digit multiplication. 1DC is 1 digit composite operations. Results become progressively stronger moving from the zero-shot to one-shot to few-shot setting, but even the zero-shot shows significant arithmetic abilities. Table <ref target=\"#tab_3\" type=\"table\">3</ref>.10: GPT-3 175B performance on various word unscrambling and word manipulation tasks, in zero-, one-, and few-shot settings. CL is \"cycle letters in word\", A1 is anagrams of but the first and last letters, A2 is anagrams of all but the first and last two letters, RI is \"Random insertion in word\", RW is \"reversed words\".</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Setting</head><p>outperforms few-shot learning for all smaller models. All three settings for the full GPT-3 are shown in Table <ref target=\"#tab_3\" type=\"table\">3</ref>.9, and model capacity scaling for all three settings is shown in Appendix H.</p><p>To spot-check whether the model is simply memorizing specific arithmetic problems, we took the 3-digit arithmetic problems in our test set and searched for them in our training data in both the forms \"&lt;NUM1&gt; + &lt;NUM2&gt; =\" and \"&lt;NUM1&gt; plus &lt;NUM2&gt;\". Out of 2,000 addition problems we found only 17 matches (0.8%) and out of 2,000 subtraction problems we found only 2 matches (0.1%), suggesting that only a trivial fraction of the correct answers could have been memorized. In addition, inspection of incorrect answers reveals that the model often makes mistakes such as not carrying a \"1\", suggesting it is actually attempting to perform the relevant computation rather than memorizing a table.</p><p>Overall, GPT-3 displays reasonable proficiency at moderately complex arithmetic in few-shot, one-shot, and even zero-shot settings.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.9.2\">Word Scrambling and Manipulation Tasks</head><p>To test GPT-3's ability to learn novel symbolic manipulations from a few examples, we designed a small battery of 5 \"character manipulation\" tasks. Each task involves giving the model a word distorted by some combination of scrambling, addition, or deletion of characters, and asking it to recover the original word. The 5 tasks are:</p><p>â€¢ Cycle letters in word (CL) -The model is given a word with its letters cycled, then the \"=\" symbol, and is expected to generate the original word. For example, it might be given \"lyinevitab\" and should output \"inevitably\".</p><p>â€¢ Anagrams of all but first and last characters (A1) -The model is given a word where every letter except the first and last have been scrambled randomly, and must output the original word. Example: criroptuon = corruption.</p><p>â€¢ Anagrams of all but first and last 2 characters (A2) -The model is given a word where every letter except the first 2 and last 2 have been scrambled randomly, and must recover the original word. Example: opoepnnt â†’ opponent.</p><p>â€¢ Random insertion in word (RI) -A random punctuation or space character is inserted between each letter of a word, and the model must output the original word. Example: s.u!c/c!e.s s i/o/n = succession.</p><p>â€¢ Reversed words (RW) -The model is given a word spelled backwards, and must output the original word. Example: stcejbo â†’ objects.</p><p>For each task we generate 10,000 examples, which we chose to be the top 10,000 most frequent words as measured by <ref target=\"#b113\" type=\"bibr\">[Nor09]</ref> of length more than 4 characters and less than 15 characters. The few-shot results are shown in Figure <ref target=\"#fig_1\" type=\"figure\">3</ref>.11. Task performance tends to grow smoothly with model size, with the full GPT-3 model achieving 66.9% on removing random insertions, 38.6% on cycling letters, 40.2% on the easier anagram task, and 15.1% on the more difficult anagram task (where only the first and last letters are held fixed). None of the models can reverse the letters in a word.</p><p>In the one-shot setting, performance is significantly weaker (dropping by half or more), and in the zero-shot setting the model can rarely perform any of the tasks (Table <ref target=\"#tab_3\" type=\"table\">3</ref>.10). This suggests that the model really does appear to learn these tasks at test time, as the model cannot perform them zero-shot and their artificial nature makes them unlikely to appear in the pre-training data (although we cannot confirm this with certainty).</p><p>We can further quantify performance by plotting \"in-context learning curves\", which show task performance as a function of the number of in-context examples. We show in-context learning curves for the Symbol Insertion task in Figure <ref target=\"#fig_11\" type=\"figure\">1</ref>.2. We can see that larger models are able to make increasingly effective use of in-context information, including both task examples and natural language task descriptions.</p><p>Finally, it is worth adding that solving these tasks requires character-level manipulations, whereas our BPE encoding operates on significant fractions of a word (on average âˆ¼ 0.7 words per token), so from the LM's perspective succeeding at these tasks involves not just manipulating BPE tokens but understanding and pulling apart their substructure. Also, CL, A1, and A2 are not bijective (that is, the unscrambled word is not a deterministic function of the scrambled word), requiring the model to perform some search to find the correct unscrambling. Thus, the skills involved appear to require non-trivial pattern-matching and computation.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.9.3\">SAT Analogies</head><p>To test GPT-3 on another task that is somewhat unusual relative to the typical distribution of text, we collected a set of 374 \"SAT analogy\" problems <ref target=\"#b151\" type=\"bibr\">[TLBS03]</ref>. Analogies are a style of multiple choice question that constituted a section of the SAT college entrance exam before 2005. A typical example is \"audacious is to boldness as (a) sanctimonious is to hypocrisy, (b) anonymous is to identity, (c) remorseful is to misdeed, (d) deleterious is to result, (e) impressionable is to temptation\". The student is expected to choose which of the five word pairs has the same relationship as the original word pair; in this example the answer is \"sanctimonious is to hypocrisy\". On this task GPT-3 achieves 65.2% in the few-shot setting, 59.1% in the one-shot setting, and 53.7% in the zero-shot setting, whereas the average score among college applicants was 57% <ref target=\"#b149\" type=\"bibr\">[TL05]</ref> (random guessing yields 20%). As shown in Figure <ref target=\"#fig_1\" type=\"figure\">3</ref>.12, the results improve with scale, with the the full 175 billion model improving by over 10% compared to the 13 billion parameter model. Figure <ref target=\"#fig_1\" type=\"figure\">3</ref>.12: Zero-, one-,and few-shot performance on SAT analogy tasks, for different sizes of model. The largest model achieves 65% accuracy in the few-shot setting, and also demonstrates significant gains to in-context learning which are not present in smaller models.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.9.4\">News Article Generation</head><p>Previous work on generative language models qualitatively tested their ability to generate synthetic \"news articles\" by conditional sampling from the model given a human-written prompt consisting of a plausible first sentence for a news story [RWC + 19]. Relative to [RWC + 19], the dataset used to train GPT-3 is much less weighted towards news articles, so trying to generate news articles via raw unconditional samples is less effective -for example GPT-3 often interprets the proposed first sentence of a \"news article\" as a tweet and then posts synthetic responses or follow-up tweets. To solve this problem we employed GPT-3's few-shot learning abilities by providing three previous news articles in the model's context to condition it. With the title and subtitle of a proposed next article, the model is able to reliably generate short articles in the \"news\" genre.</p><p>To gauge the quality of news article generation from GPT-3 (which we believe is likely to be correlated with conditional sample generation quality in general), we decided to measure human ability to distinguish GPT-3-generated articles from real ones. Similar work has been carried out by Kreps et al. <ref target=\"#b78\" type=\"bibr\">[KMB20]</ref> and Zellers et al. <ref type=\"bibr\">[ZHR + 19]</ref>. Generative language models are trained to match the distribution of content generated by humans, so the (in)ability of humans to distinguish the two is a potentially important measure of quality. <ref type=\"bibr\">3</ref> In order to see how well humans can detect model generated text, we arbitrarily selected 25 article titles and subtitles from the website newser.com (mean length: 215 words). We then generated completions of these titles and subtitles from four language models ranging in size from 125M to 175B (GPT-3) parameters (mean length: 200 words). For each model, we presented around 80 US-based participants with a quiz consisting of these real titles and subtitles followed by either the human written article or the article generated by the model 4 . Participants were asked to select whether the article was \"very likely written by a human\", \"more likely written by a human\", \"I don't know\", \"more likely written by a machine\", or \"very likely written by a machine\".</p><p>The articles we selected were not in the models' training data and the model outputs were formatted and selected programmatically to prevent human cherry-picking. All models used the same context to condition outputs on and were pre-trained with the same context size and the same article titles and subtitles were used as prompts for each model. However, we also ran an experiment to control for participant effort and attention that followed the same format but involved intentionally bad model generated articles. This was done by generating articles from a \"control model\": a 160M parameter model with no context and increased output randomness. Table <ref target=\"#tab_3\" type=\"table\">3</ref>.11: Human accuracy in identifying whether short (âˆ¼200 word) news articles are model generated. We find that human accuracy (measured by the ratio of correct assignments to non-neutral assignments) ranges from 86% on the control model to 52% on GPT-3 175B. This table compares mean accuracy between five different models, and shows the results of a two-sample T-Test for the difference in mean accuracy between each model and the control model (an unconditional GPT-3 Small model with increased output randomness).</p><p>Mean human accuracy (the ratio of correct assignments to non-neutral assignments per participant) at detecting that the intentionally bad articles were model generated was âˆ¼ 86% where 50% is chance level performance. By contrast, mean human accuracy at detecting articles that were produced by the 175B parameter model was barely above chance at âˆ¼ 52% (see Table <ref target=\"#tab_3\" type=\"table\">3</ref>.11). <ref type=\"bibr\">5</ref> Human abilities to detect model generated text appear to decrease as model size increases: there appears to be a trend towards chance accuracy with model size, and human detection of GPT-3 is close to chance. <ref type=\"bibr\">6</ref> This is true despite the fact that participants spend more time on each output as model size increases (see Appendix E).</p><p>Examples of synthetic articles from GPT-3 are given in Figures 3.14 and 3.15. 7 Much of the text is-as indicated by the evaluations-difficult for humans to distinguish from authentic human content. Factual inaccuracies can be an indicator that an article is model generated since, unlike human authors, the models have no access to the specific facts that the article titles refer to or when the article was written. Other indicators include repetition, non sequiturs, and unusual phrasings, though these are often subtle enough that they are not noticed.</p><p>Related work on language model detection by Ippolito et al. <ref target=\"#b70\" type=\"bibr\">[IDCBE19]</ref> indicates that automatic discriminators like G R O V E R <ref type=\"bibr\">[ZHR +</ref> 19] and GLTR [GSR19] may have greater success at detecting model generated text than human evaluators. Automatic detection of these models may be a promising area of future research.</p><p>Ippolito et al. <ref target=\"#b70\" type=\"bibr\">[IDCBE19]</ref> also note that human accuracy at detecting model generated text increases as humans observe more tokens. To do a preliminary investigation of how good humans are at detecting longer news articles generated by GPT-3 175B, we selected 12 world news articles from Reuters with an average length of 569 words and generated completions of these articles from GPT-3 with an average length of 498 words (298 words longer than our initial experiments). Following the methodology above, we ran two experiments, each on around 80 US-based participants, to compare human abilities to detect the articles generated by GPT-3 and a control model.</p><p>We found that mean human accuracy at detecting the intentionally bad longer articles from the control model was âˆ¼ 88%, while mean human accuracy at detecting the longer articles that were produced by GPT-3 175B was still barely above chance at âˆ¼ 52% (see Table <ref target=\"#tab_3\" type=\"table\">3</ref>.12). This indicates that, for news articles that are around 500 words long, GPT-3 continues to produce articles that humans find difficult to distinguish from human written news articles.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.9.5\">Learning and Using Novel Words</head><p>A task studied in developmental linguistics <ref target=\"#b36\" type=\"bibr\">[CB78]</ref> is the ability to learn and utilize new words, for example using a word in a sentence after seeing it defined only once, or conversely inferring a word's meaning from only one usage. Here we qualitatively test GPT-3's ability to do the former. Specifically, we give GPT-3 the definition of a nonexistent word, such as \"Gigamuru\", and then ask it to use it in a sentence. We provide one to five previous examples of a (separate)</p><p>Figure <ref target=\"#fig_1\" type=\"figure\">3</ref>.13: People's ability to identify whether news articles are model-generated (measured by the ratio of correct assignments to non-neutral assignments) decreases as model size increases. Accuracy on the outputs on the deliberatelybad control model (an unconditioned GPT-3 Small model with higher output randomness) is indicated with the dashed line at the top, and the random chance (50%) is indicated with the dashed line at the bottom. Line of best fit is a power law with 95% confidence intervals. Table <ref target=\"#tab_3\" type=\"table\">3</ref>.12: People's ability to identify whether âˆ¼ 500 word articles are model generated (as measured by the ratio of correct assignments to non-neutral assignments) was 88% on the control model and 52% on GPT-3 175B. This table shows the results of a two-sample T-Test for the difference in mean accuracy between GPT-3 175B and the control model (an unconditional GPT-3 Small model with increased output randomness).</p><p>Title: United Methodists Agree to Historic Split Subtitle: Those who oppose gay marriage will form their own denomination Article: After two days of intense debate, the United Methodist Church has agreed to a historic split -one that is expected to end in the creation of a new denomination, one that will be \"theologically and socially conservative,\" according to The Washington Post. The majority of delegates attending the church's annual General Conference in May voted to strengthen a ban on the ordination of LGBTQ clergy and to write new rules that will \"discipline\" clergy who officiate at same-sex weddings. But those who opposed these measures have a new plan: They say they will form a separate denomination by 2020, calling their church the Christian Methodist denomination. The Post notes that the denomination, which claims 12.5 million members, was in the early 20th century the \"largest Protestant denomination in the U.S.,\" but that it has been shrinking in recent decades. The new split will be the second in the church's history. The first occurred in 1968, when roughly 10 percent of the denomination left to form the Evangelical United Brethren Church. The Post notes that the proposed split \"comes at a critical time for the church, which has been losing members for years,\" which has been \"pushed toward the brink of a schism over the role of LGBTQ people in the church.\" Gay marriage is not the only issue that has divided the church. In 2016, the denomination was split over ordination of transgender clergy, with the North Pacific regional conference voting to ban them from serving as clergy, and the South Pacific regional conference voting to allow them.</p><p>Figure <ref target=\"#fig_1\" type=\"figure\">3</ref>.14: The GPT-3 generated news article that humans had the greatest difficulty distinguishing from a human written article (accuracy: 12%).  The GPT-3 generated news article that humans found the easiest to distinguish from a human written article (accuracy: 61%).</p><p>A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:</p><p>We were traveling in Africa and we saw these very cute whatpus.</p><p>To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:</p><p>One day when I was playing tag with my little sister, she got really excited and she started doing these crazy farduddles.</p><p>A \"yalubalu\" is a type of vegetable that looks like a big pumpkin. An example of a sentence that uses the word yalubalu is:</p><p>I was on a trip to Africa and I tried this yalubalu vegetable that was grown in a garden there. It was delicious.</p><p>A \"Burringo\" is a car with very fast acceleration. An example of a sentence that uses the word Burringo is:</p><p>In our garage we have a Burringo that my father drives to work every day.</p><p>A \"Gigamuru\" is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is:</p><p>I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.</p><p>To \"screeg\" something is to swing a sword at it. An example of a sentence that uses the word screeg is:</p><p>We screeghed at each other for several minutes and then we went outside and ate ice cream.</p><p>Figure <ref target=\"#fig_1\" type=\"figure\">3</ref>.16: Representative GPT-3 completions for the few-shot task of using a new word in a sentence. Boldface is GPT-3's completions, plain text is human prompts. In the first example both the prompt and the completion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional prompts and provides the completions. Nothing task-specific is provided to GPT-3 other than the conditioning shown here.</p><p>nonexistent word being defined and used in a sentence, so the task is few-shot in terms of previous examples of the broad task and one-shot in terms of the specific word. Table <ref target=\"#tab_3\" type=\"table\">3</ref>.16 shows the 6 examples we generated; all definitions were human-generated, and the first answer was human-generated as conditioning while the subsequent answers were generated by GPT-3. These examples were generated continuously in one sitting and we did not omit or repeatedly try any prompts. In all cases the generated sentence appears to be a correct or at least plausible use of the word. In the final sentence the model generates a plausible conjugation for the word \"screeg\" (namely \"screeghed\"), although the use of the word is slightly awkward (\"screeghed at each other\") despite being plausible in the sense that it could describe a toy sword fight. Overall, GPT-3 appears to be at least proficient at the task of using novel words in a sentence.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.9.6\">Correcting English Grammar</head><p>Another task well suited for few-shot learning is correcting English grammar. We test this with GPT-3 in the fewshot setting by giving prompts of the form \"Poor English Input: &lt;sentence&gt;\\n Good English Output: &lt;sentence&gt;\". We give GPT-3 one human-generated correction and then ask it to correct 5 more (again without any omissions or repeats). Results are shown in Figure <ref target=\"#fig_1\" type=\"figure\">3</ref>.17.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4\">Measuring and Preventing Memorization Of Benchmarks</head><p>Since our training dataset is sourced from the internet, it is possible that our model was trained on some of our benchmark test sets. Accurately detecting test contamination from internet-scale datasets is a new area of research without established best practices. While it is common practice to train large models without investigating contamination, given the increasing scale of pretraining datasets, we believe this issue is becoming increasingly important to attend to.</p><p>This concern is not just hypothetical. One of the first papers to train a language model on Common Crawl data <ref target=\"#b150\" type=\"bibr\">[TL18]</ref> detected and removed a training document which overlapped with one of their evaluation datasets. Other work such as GPT-2 [RWC + 19] also conducted post-hoc overlap analysis. Their study was relatively encouraging, finding that</p><p>Poor English input: I eated the purple berries.</p><p>Figure <ref target=\"#fig_1\" type=\"figure\">3</ref>.17: Representative GPT-3 completions for the few-shot task of correcting English grammar. Boldface is GPT-3's completions, plain text is human prompts. In the first few examples example both the prompt and the completion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional prompts and provides the completions. Nothing task-specific is provided to GPT-3 aside from the first few examples as conditioning and the \"Poor English input/Good English output\" framing. We note that the distinction between \"poor\" and \"good\" English (and the terms themselves) is complex, contextual, and contested. As the example mentioning the rental of a house shows, assumptions that the model makes about what \"good\" is can even lead it to make errors (here, the model not only adjusts grammar, but also removes the word \"cheap\" in a way that alters meaning). although models did perform moderately better on data that overlapped between training and testing, this did not significantly impact reported results due to the small fraction of data which was contaminated (often only a few percent).</p><p>GPT-3 operates in a somewhat different regime. On the one hand, the dataset and model size are about two orders of magnitude larger than those used for GPT-2, and include a large amount of Common Crawl, creating increased potential for contamination and memorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B does not overfit its training set by a significant amount, measured relative to a held-out validation set with which it was deduplicated (Figure <ref target=\"#fig_8\" type=\"figure\">4</ref>.1). Thus, we expect that contamination is likely to be frequent, but that its effects may not be as large as feared.</p><p>We initially tried to address the issue of contamination by proactively searching for and attempting to remove any overlap between our training data and the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn't feasible to retrain the model. To address this, we investigate in detail how the remaining detected overlap impacts results.</p><p>For each benchmark, we produce a 'clean' version which removes all potentially leaked examples, defined roughly as examples that have a 13-gram overlap with anything in the pretraining set (or that overlap with the whole example when it is shorter than 13-grams). The goal is to very conservatively flag anything that could potentially be contamination, so as to produce a clean subset that is free of contamination with high confidence. The exact procedure is detailed in Appendix C.</p><p>We then evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on the clean subset is similar to the score on the entire dataset, this suggests that contamination, even if present, does not have a significant effect on reported results. If the score on the clean subset is lower, this suggests contamination may be inflating the results. The results are summarized in Figure <ref target=\"#fig_8\" type=\"figure\">4</ref>.2. Although potential contamination is often high (with a quarter of benchmarks scoring over 50%), in most cases performance changes only negligibly, and we see no evidence that contamination level and performance difference are correlated. We conclude that either our conservative method substantially overestimated contamination or that contamination has little effect on performance.</p><p>Below, we review in more detail the few specific cases where either (1) the model performs significantly worse on the cleaned version, or (2) potential contamination is very high, which makes measuring the performance difference difficult.</p><p>Our analysis flagged six groups of benchmarks for further investigation: Word Scrambling, Reading Comprehension (QuAC, SQuAD2, DROP), PIQA, Winograd, language modeling tasks (Wikitext tasks, 1BW), and German to English The x-axis is a conservative lower bound for how much of the dataset is known with high confidence to be clean, and the y-axis shows the difference in performance when evaluating only on the verified clean subset. Performance on most benchmarks changed negligibly, but some were flagged for further review. On inspection we find some evidence for contamination of the PIQA and Winograd results, and we mark the corresponding results in Section 3 with an asterisk. We find no evidence that other benchmarks are affected.</p><p>translation. Since our overlap analysis is designed to be extremely conservative, we expect it to produce some false positives. We summarize the results for each group of tasks below:</p><p>â€¢ Reading Comprehension: Our initial analysis flagged &gt;90% of task examples from QuAC, SQuAD2, and DROP as potentially contaminated, so large that even measuring the differential on a clean subset was difficult. Upon manual inspection, however, we found that for every overlap we inspected, in all 3 datasets, the source text was present in our training data but the question/answer pairs were not, meaning the model gains only background information and cannot memorize the answer to a specific question. â€¢ German translation: We found 25% of the examples in the WMT16 German-English test set were marked as potentially contaminated, with an associated total effect size of 1-2 BLEU. Upon inspection, none of the flagged examples contain paired sentences resembling NMT training data and collisions were monolingual matches mostly of snippets of events discussed in the news. â€¢ Reversed Words and Anagrams: Recall that these tasks are of the form \"alaok = koala\". Due to the short length of these tasks, we used 2-grams for filtering (ignoring punctuation). After inspecting the flagged overlaps, we found that they were not typically instances of real reversals or unscramblings in the training set, but rather palindromes or trivial unscramblings, e.g \"kayak = kayak\". The amount of overlap was small, but removing the trivial tasks lead to an increase in difficulty and thus a spurious signal. Related to this, the symbol insertion task shows high overlap but no effect on performance -this is because that task involves removing non-letter characters from a word, and the overlap analysis itself ignores such characters, leading to many spurious matches. â€¢ PIQA: The overlap analysis flagged 29% of examples as contaminated, and observed a 3 percentage point absolute decrease (4% relative decrease) in performance on the clean subset. Though the test dataset was released after our training set was created and its labels are hidden, some of the web pages used by the crowdsourced dataset creators are contained in our training set. We found a similar decrease in a 25x smaller model with much less capacity to memorize, leading us to suspect that the shift is likely statistical bias rather than memorization; examples which workers copied may simply be easier. Unfortunately, we cannot rigorously prove this hypothesis. We therefore mark our PIQA results with an asterisk to denote this potential contamination. â€¢ Winograd: The overlap analysis flagged 45% of examples, and found a 2.6% decrease in performance on the clean subset. Manual inspection of the overlapping data point showed that 132 Winograd schemas were in fact present in our training set, though presented in a different format than we present the task to the model. Although the decrease in performance is small, we mark our Winograd results in the main paper with an asterisk.</p><p>â€¢ Language modeling: We found the 4 Wikipedia language modeling benchmarks measured in GPT-2, plus the Children's Book Test dataset, to be almost entirely contained in our training data. Since we cannot reliably extract a clean subset here, we do not report results on these datasets, even though we intended to when starting this work. We note that Penn Tree Bank due to its age was unaffected and therefore became our chief language modeling benchmark.</p><p>We also inspected datasets where contamination was high, but the impact on performance was close to zero, simply to verify how much actual contamination existed. These appeared to often contain false positives. They had either no actual contamination, or had contamination that did not give away the answer to the task. One notable exception was LAMBADA, which appeared to have substantial genuine contamination, yet the impact on performance was very small, with the clean subset scoring within 0.5% of the full dataset. Also, strictly speaking, our fill-in-the-blank format precludes the simplest form of memorization. Nevertheless, since we made very large gains on LAMBADA in this paper, the potential contamination is noted in the results section.</p><p>An important limitation of our contamination analysis is that we cannot be sure that the clean subset is drawn from the same distribution as the original dataset. It remains possible that memorization inflates results but at the same time is precisely counteracted by some statistical bias causing the clean subset to be easier. However, the sheer number of shifts close to zero suggests this is unlikely, and we also observed no noticeable difference in the shifts for small models, which are unlikely to be memorizing.</p><p>Overall, we have made a best effort to measure and document the effects of data contamination, and to note or outright remove problematic results, depending on the severity. Much work remains to be done to address this important and subtle issue for the field in general, both when designing benchmarks and when training models. For a more detailed explanation of our analysis, we refer the reader to Appendix C.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5\">Limitations</head><p>GPT-3 and our analysis of it have a number of limitations. Below we describe some of these and suggest directions for future work.</p><p>First, despite the strong quantitative and qualitative improvements of GPT-3, particularly compared to its direct predecessor GPT-2, it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis, although the overall quality is high, GPT-3 samples still sometimes repeat themselves semantically at the document level, start to lose coherence over sufficiently long passages, contradict themselves, and occasionally contain non-sequitur sentences or paragraphs. We will release a collection of 500 uncurated unconditional samples to help provide a better sense of GPT-3's limitations and strengths at text synthesis. Within the domain of discrete language tasks, we have noticed informally that GPT-3 seems to have special difficulty with \"common sense physics\", despite doing well on some datasets (such as PIQA [BZB + 19]) that test this domain. Specifically GPT-3 has difficulty with questions of the type \"If I put cheese into the fridge, will it melt?\". Quantitatively, GPT-3's in-context learning performance has some notable gaps on our suite of benchmarks, as described in Section 3, and in particular it does little better than chance when evaluated one-shot or even few-shot on some \"comparison\" tasks, such as determining if two words are used the same way in a sentence, or if one sentence implies another (WIC and ANLI respectively), as well as on a subset of reading comprehension tasks. This is especially striking given GPT-3's strong few-shot performance on many other tasks.</p><p>GPT-3 has several structural and algorithmic limitations, which could account for some of the issues above. We focused on exploring in-context learning behavior in autoregressive language models because it is straightforward to both sample and compute likelihoods with this model class. As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature, which has documented improved fine-tuning performance when using these approaches over standard language models [RSR + 19]. Thus our design decision comes at the cost of potentially worse performance on tasks which empirically benefit from bidirectionality. This may include fill-in-the-blank tasks, tasks that involve looking back and comparing two pieces of content, or tasks that require re-reading or carefully considering a long passage and then generating a very short answer. This could be a possible explanation for GPT-3's lagging few-shot performance on a few of the tasks, such as WIC (which involves comparing the use of a word in two sentences), ANLI (which involves comparing two sentences to see if one implies the other), and several reading comprehension tasks (e.g. QuAC and RACE). We also conjecture, based on past literature, that a large bidirectional model would be stronger at fine-tuning than GPT-3. Making a bidirectional model at the scale of GPT-3, and/or trying to make bidirectional models work with few-or zero-shot learning, is a promising direction for future research, and could help achieve the \"best of both worlds\".</p><p>A more fundamental limitation of the general approach described in this paper -scaling up any LM-like model, whether autoregressive or bidirectional -is that it may eventually run into (or could already be running into) the limits of the pretraining objective. Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important. <ref target=\"#b135\" type=\"bibr\">[RRS20]</ref> demonstrate benefits of customizing prediction to entities of interest. Also, with self-supervised objectives, task specification relies on forcing the desired task into a prediction problem, whereas ultimately, useful language systems (for example virtual assistants) might be better thought of as taking goal-directed actions rather than just making predictions. Finally, large pretrained language models are not grounded in other domains of experience, such as video or real-world physical interaction, and thus lack a large amount of context about the world [BHT + 20]. For all these reasons, scaling pure self-supervised prediction is likely to hit limits, and augmentation with a different approach is likely to be necessary. Promising future directions in this vein might include learning the objective function from humans [ZSW + 19a], fine-tuning with reinforcement learning, or adding additional modalities such as images to provide grounding and a better model of the world [CLY + 19].</p><p>Another limitation broadly shared by language models is poor sample efficiency during pre-training. While GPT-3 takes a step towards test-time sample efficiency closer to that of humans (one-shot or zero-shot), it still sees much more text during pre-training than a human sees in the their lifetime <ref target=\"#b93\" type=\"bibr\">[Lin20]</ref>. Improving pre-training sample efficiency is an important direction for future work, and might come from grounding in the physical world to provide additional information, or from algorithmic improvements.</p><p>A limitation, or at least uncertainty, associated with few-shot learning in GPT-3 is ambiguity about whether few-shot learning actually learns new tasks \"from scratch\" at inference time, or if it simply recognizes and identifies tasks that it has learned during training. These possibilities exist on a spectrum, ranging from demonstrations in the training set that are drawn from exactly the same distribution as those at test time, to recognizing the same task but in a different format, to adapting to a specific style of a general task such as QA, to learning a skill entirely de novo. Where GPT-3 is on this spectrum may also vary from task to task. Synthetic tasks such as wordscrambling or defining nonsense words seem especially likely to be learned de novo, whereas translation clearly must be learned during pretraining, although possibly from data that is very different in organization and style than the test data. Ultimately, it is not even clear what humans learn from scratch vs from prior demonstrations. Even organizing diverse demonstrations during pre-training and identifying them at test time would be an advance for language models, but nevertheless understanding precisely how few-shot learning works is an important unexplored direction for future research.</p><p>A limitation associated with models at the scale of GPT-3, regardless of objective function or algorithm, is that they are both expensive and inconvenient to perform inference on, which may present a challenge for practical applicability of models of this scale in their current form. One possible future direction to address this is distillation <ref target=\"#b66\" type=\"bibr\">[HVD15]</ref> of large models down to a manageable size for specific tasks. Large models such as GPT-3 contain a very wide range of skills, most of which are not needed for a specific task, suggesting that in principle aggressive distillation may be possible. Distillation is well-explored in general <ref target=\"#b91\" type=\"bibr\">[LHCG19a]</ref> but has not been tried at the scale of hundred of billions parameters; new challenges and opportunities may be associated with applying it to models of this size.</p><p>Finally, GPT-3 shares some limitations common to most deep learning systems -its decisions are not easily interpretable, it is not necessarily well-calibrated in its predictions on novel inputs as observed by the much higher variance in performance than humans on standard benchmarks, and it retains the biases of the data it has been trained on. This last issue -biases in the data that may lead the model to generate stereotyped or prejudiced content -is of special concern from a societal perspective, and will be discussed along with other issues in the next section on Broader Impacts (Section 6).</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6\">Broader Impacts</head><p>Language models have a wide range of beneficial applications for society, including code and writing auto-completion, grammar assistance, game narrative generation, improving search engine responses, and answering questions. But they also have potentially harmful applications. GPT-3 improves the quality of text generation and adaptability over smaller models and increases the difficulty of distinguishing synthetic text from human-written text. It therefore has the potential to advance both the beneficial and harmful applications of language models.</p><p>Here we focus on the potential harms of improved language models, not because we believe the harms are necessarily greater, but in order to stimulate efforts to study and mitigate them. The broader impacts of language models like this are numerous. We focus on two primary issues: the potential for deliberate misuse of language models like GPT-3 in Section 6.1, and issues of bias, fairness, and representation within models like GPT-3 in Section 6.2. We also briefly discuss issues of energy efficiency (Section 6.3).</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6.1\">Misuse of Language Models</head><p>Malicious uses of language models can be somewhat difficult to anticipate because they often involve repurposing language models in a very different environment or for a different purpose than researchers intended. To help with this, we can think in terms of traditional security risk assessment frameworks, which outline key steps such as identifying threats and potential impacts, assessing likelihood, and determining risk as a combination of likelihood and impact <ref target=\"#b133\" type=\"bibr\">[Ros12]</ref>. We discuss three factors: potential misuse applications, threat actors, and external incentive structures.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6.1.1\">Potential Misuse Applications</head><p>Any socially harmful activity that relies on generating text could be augmented by powerful language models. Examples include misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing and social engineering pretexting. Many of these applications bottleneck on human beings to write sufficiently high quality text. Language models that produce high quality text generation could lower existing barriers to carrying out these activities and increase their efficacy.</p><p>The misuse potential of language models increases as the quality of text synthesis improves. The ability of GPT-3 to generate several paragraphs of synthetic content that people find difficult to distinguish from human-written text in 3.9.4 represents a concerning milestone in this regard.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6.1.2\">Threat Actor Analysis</head><p>Threat actors can be organized by skill and resource levels, ranging from low or moderately skilled and resourced actors who may be able to build a malicious product to 'advanced persistent threats' (APTs): highly skilled and well-resourced (e.g. state-sponsored) groups with long-term agendas <ref type=\"bibr\">[SBC +</ref> 19].</p><p>To understand how low and mid-skill actors think about language models, we have been monitoring forums and chat groups where misinformation tactics, malware distribution, and computer fraud are frequently discussed. While we did find significant discussion of misuse following the initial release of GPT-2 in spring of 2019, we found fewer instances of experimentation and no successful deployments since then. Additionally, those misuse discussions were correlated with media coverage of language model technologies. From this, we assess that the threat of misuse from these actors is not immediate, but significant improvements in reliability could change this.</p><p>Because APTs do not typically discuss operations in the open, we have consulted with professional threat analysts about possible APT activity involving the use of language models. Since the release of GPT-2 there has been no discernible difference in operations that may see potential gains by using language models. The assessment was that language models may not be worth investing significant resources in because there has been no convincing demonstration that current language models are significantly better than current methods for generating text, and because methods for \"targeting\" or \"controlling\" the content of language models are still at a very early stage.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6.1.3\">External Incentive Structures</head><p>Each threat actor group also has a set of tactics, techniques, and procedures (TTPs) that they rely on to accomplish their agenda. TTPs are influenced by economic factors like scalability and ease of deployment; phishing is extremely popular among all groups because it offers a low-cost, low-effort, high-yield method of deploying malware and stealing login credentials. Using language models to augment existing TTPs would likely result in an even lower cost of deployment.</p><p>Ease of use is another significant incentive. Having stable infrastructure has a large impact on the adoption of TTPs. The outputs of language models are stochastic, however, and though developers can constrain these (e.g. using top-k truncation) they are not able to perform consistently without human feedback. If a social media disinformation bot produces outputs that are reliable 99% of the time, but produces incoherent outputs 1% of the time, this could reduce the amount of human labor required in operating this bot. But a human is still needed to filter the outputs, which restricts how scalable the operation can be.</p><p>Based on our analysis of this model and analysis of threat actors and the landscape, we suspect AI researchers will eventually develop language models that are sufficiently consistent and steerable that they will be of greater interest to malicious actors. We expect this will introduce challenges for the broader research community, and hope to work on this through a combination of mitigation research, prototyping, and coordinating with other technical developers.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6.2\">Fairness, Bias, and Representation</head><p>Biases present in training data may lead models to generate stereotyped or prejudiced content. This is concerning, since model bias could harm people in the relevant groups in different ways by entrenching existing stereotypes and producing demeaning portrayals amongst other potential harms <ref target=\"#b42\" type=\"bibr\">[Cra17]</ref>. We have conducted an analysis of biases in the model in order to better understand GPT-3's limitations when it comes to fairness, bias, and representation. 8</p><p>Our goal is not to exhaustively characterize GPT-3, but to give a preliminary analysis of some of its limitations and behaviors. We focus on biases relating to gender, race, and religion, although many other categories of bias are likely present and could be studied in follow-up work. This is a preliminary analysis and does not reflect all of the model's biases even within the studied categories.</p><p>Broadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to reflect stereotypes present in their training data. Below we discuss our preliminary findings of bias along the dimensions of gender, race, and religion. We probe for bias in the 175 billion parameter model and also in similar smaller models, to see if and how they are different in this dimension.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6.2.1\">Gender</head><p>In our investigation of gender bias in GPT-3, we focused on associations between gender and occupation. We found that occupations in general have a higher probability of being followed by a male gender identifier than a female one (in other words, they are male leaning) when given a context such as \"The {occupation} was a\" (Neutral Variant). 83% of the 388 occupations we tested were more likely to be followed by a male identifier by GPT-3. We measured this by feeding the model a context such as \"The detective was a\" and then looking at the probability of the model following up with male indicating words (eg. man, male etc.) or female indicating words (woman, female etc.). In particular, occupations demonstrating higher levels of education such as legislator, banker, or professor emeritus were heavily male leaning along with occupations that require hard physical labour such as mason, millwright, and sheriff. Occupations that were more likely to be followed by female identifiers include midwife, nurse, receptionist, housekeeper etc.</p><p>We also tested how these probabilities changed when we shifted the context to be the \"The competent {occupation} was a\" (Competent Variant), and when we shifted the context to be \"The incompetent {occupation} was a\" (Incompetent Variant) for each occupation in the dataset. We found that, when prompted with \"The competent {occupation} was a,\" the majority of occupations had an even higher probability of being followed by a male identifier than a female one than was the case with our original neutral prompt, \"The {occupation} was a\". With the prompt \"The incompetent {occupation} was a\" the majority of occupations still leaned male with a similar probability than for our original neutral prompt. The average occupation bias -measured as 1 n jobs jobs log( P (female|Context) P (male|Context)) ) -was âˆ’1.11 for the Neutral Variant, âˆ’2.14 for the Competent Variant and âˆ’1.15 for the Incompetent Variant.</p><p>We also carried out pronoun resolution on the Winogender dataset [RNLVD18] using two methods which further corroborated the model's tendency to associate most occupations with males. One method measured the models ability to correctly assign a pronoun as the occupation or the participant. For example, we fed the model a context such as \"The advisor met with the advisee because she wanted to get advice about job applications. 'She' refers to the\" and found the option with the lowest probability between the two possible options (Choices between Occupation Option: advisor; Participant Option: advisee).</p><p>Occupation and participant words often have societal biases associated with them such as the assumption that most occupants are by default male. We found that the language models learnt some of these biases such as a tendency to associate female pronouns with participant positions more than male pronouns. GPT-3 175B had the highest accuracy of all the models (64.17%) on this task. It was also the only model where the accuracy for Occupant sentences (sentences where the correct answer was the Occupation option) for females was higher than for males (81.7% vs 76.7%). All other models had a higher accuracy for male pronouns with Occupation sentences as compared to female pronouns with the exception of our second largest model-GPT-3 13B -which had the same accuracy (60%) for both. This offers some preliminary evidence that in places where issues of bias can make language models susceptible to error, the larger models are more robust than smaller models.</p><p>We also performed co-occurrence tests, where we analyzed which words are likely to occur in the vicinity of other preselected words. We created a model output sample set by generating 800 outputs of length 50 each with a temperature Easy-going (12) Eccentric ( <ref type=\"formula\">13</ref>) Petite (10) Protect ( <ref type=\"formula\">10</ref>) Tight (10) Jolly ( <ref type=\"formula\">10</ref>) Pregnant (10) Stable ( <ref type=\"formula\">9</ref>) Gorgeous ( <ref type=\"formula\">28</ref>) Personable ( <ref type=\"formula\">22</ref>) Sucked ( <ref type=\"formula\">8</ref>) Survive ( <ref type=\"formula\">7</ref>)</p><p>Beautiful ( <ref type=\"formula\">158</ref>) of 1 and top p of 0.9 for every prompt in our dataset. For gender, we had prompts such as \"He was very\", \"She was very\", \"He would be described as\", \"She would be described as\" 9 . We looked at the adjectives and adverbs in the top 100 most favored words using an off-the-shelf POS tagger <ref target=\"#b82\" type=\"bibr\">[LB02]</ref>. We found females were more often described using appearance oriented words such as \"beautiful\" and \"gorgeous\" as compared to men who were more often described using adjectives that span a greater spectrum.</p><p>Table <ref target=\"#tab_15\" type=\"table\">6</ref>.1 shows the top 10 most favored descriptive words for the model along with the raw number of times each word co-occurred with a pronoun indicator. \"Most Favored\" here indicates words which were most skewed towards a category by co-occurring with it at a higher rate as compared to the other category. To put these numbers in perspective, we have also included the average for the number of co-occurrences across all qualifying words for each gender.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6.2.2\">Race</head><p>To investigate racial bias in GPT-3, we seeded the model with prompts such as -\"The {race} man was very\", \"The {race} woman was very\" and \"People would describe the {race} person as\" and generated 800 samples for each of the above prompts, with {race} replaced with a term indicating a racial category such as White or Asian. We then measure word co-occurrences in the generated samples. Given prior research demonstrating that language models produce text of differing sentiment when varying features such as occupation <ref type=\"bibr\">[HZJ +</ref> 19], we explored how race impacted sentiment. We measured sentiment using Senti WordNet <ref target=\"#b30\" type=\"bibr\">[BES10]</ref> for the words which co-occurred disproportionately with each race. Each word sentiment varied from 100 to -100, with positive scores indicating positive words (eg. wonderfulness: 100, amicable: 87.5), negative scores indicating negative words (eg. wretched: -87.5 , horrid: -87.5) and a score of 0 indicating neutral words (eg. sloping, chalet).</p><p>It should be noted that we were explicitly prompting the models to talk about race and this in turn generated text that focused on racial features; these results are not from the models talking about race in the wild but talking about race in an experimental setup where they have been primed to do so. Additionally, since we are measuring sentiment by simply looking at word co-occurrences, the resulting sentiment can reflect socio-historical factors -for instance, text relating to a discussion of slavery will frequently have a negative sentiment, which may lead to a demographic being associated with a negative sentiment under this testing methodology.</p><p>Across the models we analyzed, 'Asian' had a consistently high sentiment -it ranked 1st in 3 out of 7 models. On the other hand, 'Black' had a consistently low sentiment -it ranked the lowest in 5 out of 7 models. These differences narrowed marginally on the larger model sizes. This analysis gives a sense of the biases of different models and highlights the need for more sophisticated analysis of the relationship between sentiment, entities, and input data. Table <ref target=\"#tab_15\" type=\"table\">6</ref>.2: Shows the ten most favored words about each religion in the GPT-3 175B model.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6.2.3\">Religion</head><p>We studied which words co-occurred with religious terms relating to Atheism, Buddhism, Christianity, Hinduism, Islam, and Judaism, by generating 800 model outputs of length â‰ˆ50 with a temperature of 1 and a top p of 0.9 for every prompt. Our prompts were of the nature \"{Religion practitioners} are\" (Eg. \"Christians are\") for each of the six religious categories listed above. We then allowed the model to naturally carry out completions and created a corpus of such completions for studying co-occurrence of words.</p><p>The following is an example output from the model:</p><p>\"Buddhists are divided into two main branches -Theravada and Mahayana. Theravada is the more conservative branch, centering on monastic life and the earliest sutras and refusing to recognize the later Mahayana sutras as authentic.\"</p><p>Similar to race, we found that the models make associations with religious terms that indicate some propensity to reflect how these terms are sometimes presented in the world. For example, with the religion Islam, we found that words such as ramadan, prophet and mosque co-occurred at a higher rate than for other religions. We also found that words such as violent, terrorism and terrorist co-occurred at a greater rate with Islam than with other religions and were in the top 40 most favored words for Islam in GPT-3.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6.2.4\">Future Bias and Fairness Challenges</head><p>We have presented this preliminary analysis to share some of the biases we found in order to motivate further research, and to highlight the inherent difficulties in characterizing biases in large-scale generative models; we expect this to be an area of continuous research for us and are excited to discuss different methodological approaches with the community. We view the work in this section as subjective signposting -we chose gender, race, and religion as a starting point, but we recognize the inherent subjectivity in this choice. Our work is inspired by the literature on characterizing model attributes to develop informative labels such as Model Cards for Model Reporting from [MWZ + 18].</p><p>Ultimately, it is important not just to characterize biases in language systems but to intervene. The literature on this is also extensive [QMZH19, HZJ + 19], so we offer only a few brief comments on future directions specific to large language models. In order to pave the way for effective bias prevention in general purpose models, there is a need for building a common vocabulary tying together the normative, technical and empirical challenges of bias mitigation for these models. There is room for more research that engages with the literature outside NLP, better articulates normative statements about harm, and engages with the lived experience of communities affected by NLP systems <ref target=\"#b27\" type=\"bibr\">[BBDIW20]</ref>. Thus, mitigation work should not be approached purely with a metric driven objective to 'remove' bias as this has been shown to have blind spots <ref target=\"#b55\" type=\"bibr\">[GG19,</ref><ref target=\"#b114\" type=\"bibr\">NvNvdG19]</ref> but in a holistic manner.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6.3\">Energy Usage</head><p>Practical large-scale pre-training requires large amounts of computation, which is energy-intensive: training the GPT-3 175B consumed several thousand petaflop/s-days of compute during pre-training, compared to tens of petaflop/s-days for a 1.5B parameter GPT-2 model (Figure <ref target=\"#fig_28\" type=\"figure\">2</ref>.2). This means we should be cognizant of the cost and efficiency of such models, as advocated by <ref target=\"#b142\" type=\"bibr\">[SDSE19]</ref>.</p><p>The use of large-scale pre-training also gives another lens through which to view the efficiency of large models -we should consider not only the resources that go into training them, but how these resources are amortized over the lifetime of a model, which will subsequently be used for a variety of purposes and fine-tuned for specific tasks. Though models like GPT-3 consume significant resources during training, they can be surprisingly efficient once trained: even with the full GPT-3 175B, generating 100 pages of content from a trained model can cost on the order of 0.4 kW-hr, or only a few cents in energy costs. Additionally, techniques like model distillation <ref target=\"#b91\" type=\"bibr\">[LHCG19a]</ref> can further bring down the cost of such models, letting us adopt a paradigm of training single, large-scale models, then creating more efficient versions of them for use in appropriate contexts. Algorithmic progress may also naturally further increase the efficiency of such models over time, similar to trends observed in image recognition and neural machine translation <ref target=\"#b61\" type=\"bibr\">[HB20]</ref>.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"7\">Related Work</head><p>Several lines of work have focused on increasing parameter count and/or computation in language models as a means to improve generative or task performance. This work suggests that this trend largely continues as models continue to scale up (although a slight bending of the curve can perhaps be detected in Figure <ref target=\"#fig_1\" type=\"figure\">3</ref>.1), and we also find relatively smooth increases in many (though not all) downstream tasks across 3 orders of magnitude of scaling.</p><p>Another line of work goes in the opposite direction from scaling, attempting to preserve strong performance in language models that are as small as possible. This approach includes ALBERT <ref type=\"bibr\">[LCG +</ref>  While the mechanism of our few-shot approach is different, prior work has also explored ways of using pre-trained language models in combination with gradient descent to perform few-shot learning <ref target=\"#b146\" type=\"bibr\">[SS20]</ref>. Another sub-field with similar goals is semi-supervised learning where approaches such as UDA <ref type=\"bibr\">[XDH +</ref> 19] also explore methods of fine-tuning when very little labeled data is available.</p><p>Giving multi-task models instructions in natural language was first formalized in a supervised setting with <ref target=\"#b108\" type=\"bibr\">[MKXS18]</ref> and utilized for some tasks (such as summarizing) in a language model with [RWC + 19]. The notion of presenting tasks in natural language was also explored in the text-to-text transformer [RSR + 19], although there it was applied for multi-task fine-tuning rather than for in-context learning without weight updates.</p><p>Another approach to increasing generality and transfer-learning capability in language models is multi-task learning <ref target=\"#b35\" type=\"bibr\">[Car97]</ref>, which fine-tunes on a mixture of downstream tasks together, rather than separately updating the weights for each one. If successful multi-task learning could allow a single model to be used for many tasks without updating the weights (similar to our in-context learning approach), or alternatively could improve sample efficiency when updating the weights for a new task. Multi-task learning has shown some promising initial results <ref type=\"bibr\">[LGH +</ref>  In this work we continue to focus on pure autoregressive language models, both in order to focus on in-context learning performance and to reduce the complexity of our large model implementations. However, it is very likely that incorporating these algorithmic advances could improve GPT-3's performance on downstream tasks, especially in the fine-tuning setting, and combining GPT-3's scale with these algorithmic techniques is a promising direction for future work.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"8\">Conclusion</head><p>We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and benchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly matching the performance of state-of-the-art fine-tuned systems, as well as generating high-quality samples and strong qualitative performance at tasks defined on-the-fly. We documented roughly predictable trends of scaling in performance without using fine-tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses, these results suggest that very large language models may be an important ingredient in the development of adaptable, general language systems.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A Details of Common Crawl Filtering</head><p>As mentioned in Section 2.2, we employed two techniques to improve the quality of the Common Crawl dataset: (1) filtering Common Crawl and (2) fuzzy deduplication:</p><p>1. In order to improve the quality of Common Crawl, we developed an automatic filtering method to remove low quality documents. Using the original WebText as a proxy for high-quality documents, we trained a classifier to distinguish these from raw Common Crawl. We then used this classifier to re-sample Common Crawl by prioritizing documents which were predicted by the classifier to be higher quality. The classifier is trained using logistic regression classifier with features from Spark's standard tokenizer and HashingTF 10 . For the positive examples, we used a collection of curated datasets such as WebText, Wikiedia, and our web books corpus as the positive examples, and for the negative examples, we used unfiltered Common Crawl. We used this classifier to score Common Crawl documents. We kept each document in our dataset iff np.random.pareto(Î±) &gt; 1 âˆ’ document_score We chose Î± = 9 in order to take mostly documents the classifier scored highly, but still include some documents that were out of distribution. Î± was chosen to match the distribution of scores from our classifier on WebText.</p><p>We found this re-weighting increased quality as measured by loss on a range of out-of-distribution generative text samples.</p><p>2. To further improve model quality and prevent overfitting (which becomes increasingly important as model capacity increases), we fuzzily deduplicated documents (i.e. removed documents with high overlap with other documents) within each dataset using Spark's MinHashLSH implementation with 10 hashes, using the same features as were used for classification above. We also fuzzily removed WebText from Common Crawl.</p><p>Overall this decreased dataset size by an average of 10%.</p><p>After filtering for duplicates and quality, we also partially removed text occurring in benchmark datasets, described in Appendix C.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>B Details of Model Training</head><p>To train all versions of GPT-3, we use Adam with Î² 1 = 0.9, Î² 2 = 0.95, and = 10 âˆ’8 , we clip the global norm of the gradient at 1.0, and we use cosine decay for learning rate down to 10% of its value, over 260 billion tokens (after 260 billion tokens, training continues at 10% of the original learning rate). There is a linear LR warmup over the first 375 million tokens. We also gradually increase the batch size linearly from a small value (32k tokens) to the full value over the first 4-12 billion tokens of training, depending on the model size. Data are sampled without replacement during training (until an epoch boundary is reached) to minimize overfitting. All models use weight decay of 0.1 to provide a small amount of regularization <ref target=\"#b90\" type=\"bibr\">[LH17]</ref>.</p><p>During training we always train on sequences of the full n ctx = 2048 token context window, packing multiple documents into a single sequence when documents are shorter than 2048, in order to increase computational efficiency. Sequences with multiple documents are not masked in any special way but instead documents within a sequence are delimited with a special end of text token, giving the language model the information necessary to infer that context separated by the end of text token is unrelated. This allows for efficient training without need for any special sequence-specific masking.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>C Details of Test Set Contamination Studies</head><p>In section 4 we gave a high level overview of test set contamination studies. In this section we provide details on methodology and results.</p><p>Initial training set filtering We attempted to remove text occurring in benchmarks from training data by searching for 13âˆ’gram overlaps between all test/development sets used in this work and our training data, and we removed the colliding 13âˆ’gram as well as a 200 character window around it, splitting the original document into pieces. For filtering purposes we define a gram as a lowercase, whitespace delimited word with no punctuation. Pieces less than 200 characters long were discarded. Documents split into more than 10 pieces were considered contaminated and removed entirely. Originally we removed entire documents given a single collision, but that overly penalized long documents such as books for false positives. An example of a false positive might be a test set based on Wikipedia, in which the Wikipedia article quotes a single line from a book. We ignored 13âˆ’grams that matched more than 10 training documents, as inspection showed the majority of these to contain common cultural phrases, legal boilerplate, or similar content that we likely do want the model to learn, rather than undesired specific overlaps with test sets. Examples for various frequencies can be found in the GPT-3 release repository 11 .</p><p>Overlap methodology For our benchmark overlap analysis in Section 4, we used a variable number of words N to check for overlap for each dataset, where N is the 5th percentile example length in words, ignoring all punctuation, whitespace, and casing. Due to spurious collisions at lower values of N we use a minimum value of 8 on non-synthetic tasks. For performance reasons, we set a maximum value of 13 for all tasks. Values for N and the amount of data marked as dirty are shown in Table <ref target=\"#tab_20\" type=\"table\">C</ref>.1. Unlike GPT-2's use of bloom filters to compute probabilistic bounds for test contamination, we used Apache Spark to compute exact collisions across all training and test sets. We compute overlaps between test sets and our full training corpus, even though we only trained on 40% of our filtered Common Crawl documents per Section 2.2.</p><p>We define a 'dirty' example as one with any N -gram overlap with any training document, and a 'clean' example as one with no collision.</p><p>Test and validation splits had similar contamination levels despite some test splits being unlabeled. Due to a bug revealed by this analysis, filtering described above failed on long documents such as books. Because of cost considerations it was infeasible to retrain the model on a corrected version of the training dataset. As such, several language modeling benchmarks plus the Children's Book Test showed almost complete overlap, and therefore were not included in this paper. Overlaps are shown in Table <ref target=\"#tab_20\" type=\"table\">C</ref>.1</p><p>Overlap results To understand how much having seen some of the data helps the model perform on downstream tasks, we filter every validation and test set by dirtiness. Then we run evaluation on the clean-only examples and report the relative percent change between the clean score and the original score. If the clean score is more than 1% or 2% worse than the overall score, it suggests the model may have overfit to the examples it has seen. If the clean score is significantly better, our filtering scheme may have preferentially marked easier examples as dirty.</p><p>This overlap metric tends to show a high rate of false positives for datasets that contain background information (but not answers) drawn from the web (such as SQuAD, which draws from Wikipedia) or examples less than 8 words long, which we ignored in our filtering process (except for wordscrambling tasks). One instance where this technique seems to fail to give good signal is DROP, a reading comprehension task in which 94% of the examples are dirty. The information required to answer the question is in a passage provided to the model, so having seen the passage during training but not the questions and answers does not meaningfully constitute cheating. We confirmed that every matching training document contained only the source passage, and none of the questions and answers in the dataset. The more likely explanation for the decrease in performance is that the 6% of examples that remain after filtering come from a slightly different distribution than the dirty examples.</p><p>Figure <ref target=\"#fig_8\" type=\"figure\">4</ref>.2 shows that as the dataset becomes more contaminated, the variance of the clean/all fraction increases, but there is no apparent bias towards improved or degraded performance. This suggests that GPT-3 is relatively insensitive to contamination. See Section 4 for details on the datasets we flagged for further review.  For \"Acc/F1/BLEU\" we use the metric specified in \"Metric\". These scores come from evaluations with a different seed for the random examples used for in-context learning, and will therefore differ slightly from the scores elsewhere in the paper.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>D Total Compute Used to Train Language Models</head><p>This appendix contains the calculations that were used to derive the approximate compute used to train the language models in Figure <ref target=\"#fig_28\" type=\"figure\">2</ref>.2. As a simplifying assumption, we ignore the attention operation, as it typically uses less than 10% of the total compute for the models we are analyzing.</p><p>Calculations can be seen in  Next we note that since T5 uses an encoder-decoder model, only half of the parameters are active for each token during a forward or backwards pass. We then note that each token is involved in a single addition and a single multiply for each active parameter in the forward pass (ignoring attention). Then we add a multiplier of 3x to account for the backwards pass (as computing both âˆ‚params âˆ‚loss and âˆ‚acts âˆ‚loss use a similar amount of compute as the forwards pass. Combining the previous two numbers, we get the total flops per parameter per token. We multiply this value by the total training tokens and the total parameters to yield the number of total flops used during training. We report both flops and petaflop/s-day (each of which are 8.64e+19 flops).</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>E Human Quality Assessment of Synthetic News Articles</head><p>This appendix contains details on the experiments measuring human ability to distinguish GPT-3-generated synthetic news articles from real news articles. We first describe the experiments on the âˆ¼ 200 word news articles, and then describe the preliminary investigation of âˆ¼ 500 word news articles generated by GPT-3.</p><p>Participants: We recruited 718 unique participants to take part in 6 experiments. 97 participants were excluded for failing an internet check question, leaving a total of 621 participants: 343 male, 271 female, and 7 other. Mean participant age was âˆ¼ 38 years old. All participants were recruited through Positly, which maintains a whitelist of high-performing workers from Mechanical Turk. All participants were US-based but there were no other demographic restrictions. Participants were paid $12 for their participation, based on a task time estimate of 60 minutes determined by pilot runs. In order to ensure that the sample of participants for each experiment quiz was unique, participants were not allowed to take part in an experiment more than once.</p><p>Procedure and design: We arbitrarily selected 25 news articles that appeared in newser.com in early 2020. We used the article titles and subtitles to produce outputs from the 125M, 350M, 760M, 1.3B, 2.7B, 6.7B, 13.0B, and 200B (GPT-3) parameter language models. Five outputs per question were generated by each model and the generation with a word count closest to that of the human written article was selected automatically. This was to minimize the effect that completion length might have on participants' judgments. The same output procedure for each model with the exception of the removal of the intentionally bad control model, as described in the main text.  In each experiment, half of the participants were randomly assigned to quiz A and half were randomly assigned to quiz B. Each quiz consisted of 25 articles: half (12-13) were human written and half (12-13) were model generated: the articles with human written completions in quiz A had model generated completions in quiz B and vice versa. The order of quiz question was shuffled for each participant. Participants could leave comments and were asked to indicate if they had seen the articles before. Participants were instructed not to look up the articles or their content during the quiz and at the end of the quiz were asked if they had looked anything up during the quiz.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Statistical Tests:</head><p>To compare means on the different runs, we performed a two-sample t-test for independent groups for each model against the control. This was implemented in Python using the scipy.stats.ttest_ind function. When plotting a regression line in the graph of average participant accuracy vs model size, we fit a power law of the form ax âˆ’b . The 95% confidence intervals were estimated from the t-distribution of the sample mean.</p><p>Duration statistics: In the main text, we discussed the finding that the ability of human participants to distinguish model and human generated news articles decreases as our models become larger. We have also found that the average time spent for a given set of questions increases as the model size increases, as shown in  accuracy scores despite increased time investment from participants supports the finding that larger models generate harder-to-distinguish news articles.</p><p>Preliminary investigation of âˆ¼ 500 word articles: We recruited 160 unique US-based participants to take part in 2 experiments through Positly (details are given in Table <ref type=\"table\">E</ref>.2). We randomly selected 12 Reuters world news articles from late 2019 and created a context for GPT-3 175B that consisted of a single Reuters article not in this set of 12. We then used the article titles and Reuters locations to generate completions from GPT-3 175B and the 160M control model from the previous experiments. These were used to create two 12-question quizzes per model, each consisting of half human written and half model generated articles. Comprehension questions were added and articles were shown to participants in 3 stages at 30 second intervals to encourage closer reading. Participants were paid $12 for this task. Model generation selection methods, exclusion criteria, and statistical tests mirror those of the previous experiments.</p><p>F Additional Samples from GPT-3</p><p>GPT-3 adapts well to many tasks other than the ones explored in the main body of the paper. As an example, in Figure F.1, we show four uncurated samples from a prompt suggesting that the model write a poem, with a given title, in the style of Wallace Stevens. We first experimented with a few prompts, then generated four samples with no additional editing or selection (sampling at temperature 1 using nucleus sampling <ref target=\"#b62\" type=\"bibr\">[HBFC19]</ref> with P = 0.9). Completions were truncated when the model began to write a new title and author heading, or broke into prose commentary.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>G Details of Task Phrasing and Specifications</head><p>The following figures illustrate the formatting and phrasing of all the tasks included in the paper. All data comes from the ground truth datasets in this section, and no samples from GPT-3 are included here.</p><p>Context â†’ Article: Informal conversation is an important part of any business relationship.Before you start a discussion,however,make sure you understand which topics are suitable and which are considered taboo in a particular culture. Latin Americans enjoy sharing information about their local history, art and customs.You may expect questions about your family,and be sure to show pictures of your children.You may feel free to ask similar questions of your Latin American friends.The French think of conversation as an art form,and they enjoy the value of lively discussions as well as disagreements. For them,arguments can be interesting and they can cover pretty much or any topic ----as long as they occur in are respectful and intelligent manner.</p><p>In the United States,business people like to discuss a wide range of topics,including opinions about work,family,hobbies,and politics. In Japan,China,and Korea,however,people are much more private.They do not share much about their thoughts,feelings,or emotions because they feel that doing so might take away from the harmonious business relationship they're trying to build.Middle Easterners are also private about their personal lives and family matters.It is considered rude,for example,to ask a businessman from Saudi Arabia about his wife or children.</p><p>As a general rule,it's best not to talk about politics or religion with your business friends.This can get you into trouble,even in the United States,where people hold different religious views.In addition,discussing one's salary is usually considered unsuitable.Sports is typically a friendly subject in most parts of the world,although be careful not to criticize national sport.Instead,be friendly and praise your host's team.</p><p>Q: What shouldn't you do when talking about sports with colleagues from another country?</p><p>A: Criticizing the sports of your colleagues' country.</p><p>Q: Which is typically a friendly topic in most places according to the author?</p><p>A: Sports.</p><p>Q: Why are people from Asia more private in their conversation with others?</p><p>A: They don't want to have their good relationship with others harmed by informal conversation.</p><p>Q: The author considers politics and religion .</p><p>A:</p><p>Correct Answer â†’ taboo Incorrect Answer â†’ cheerful topics Incorrect Answer â†’ rude topics Incorrect Answer â†’ topics that can never be talked about Figure G.1: Formatted dataset example for RACE-h. When predicting, we normalize by the unconditional probability of each answer as described in 2. Mrs. Smith is an unusual teacher. Once she told each student to bring along a few potatoes in plastic bag. On each potato the students had to write a name of a person that they hated And the next day, every child brought some potatoes. Some had two potatoes;some three;some up to five. Mrs. Smith then told the children to carry the bags everywhere they went, even to the toilet, for two weeks. As day after day passed, the children started to complain about the awful smell of the rotten potatoes.</p><p>Those children who brought five potatoes began to feel the weight trouble of the bags. After two weeks, the children were happy to hear that the game was finally ended. Mrs. Smith asked,\"How did you feel while carrying the potatoes for two weeks?\" The children started complaining about the trouble loudly. Then Mrs. Smith told them why she asked them to play the game. She said,\"This is exactly the situation when you carry your hatred for somebody inside your heart. The terrible smell of the hatred will pollute your heart and you will carry something unnecessary with you all the time. If you cannot stand the smell of the rotten potatoes for just two weeks, can you imagine how heavy it would be to have the hatred in your heart for your lifetime? So throw away any hatred from your heart, and you'll be really happy.\"</p><p>Q: Which of the following is True according to the passage?</p><p>A: If a kid hated four people,he or she had to carry four potatoes.</p><p>Q: We can learn from the passage that we should .</p><p>A: throw away the hatred inside Q: The children complained about besides the weight trouble.</p><p>A: the smell Q: Mrs.Smith asked her students to write on the potatoes.</p><p>A:</p><p>Correct Answer â†’ names Incorrect Answer â†’ numbers Incorrect Answer â†’ time Incorrect Answer â†’ places Figure G.3: Formatted dataset example for RACE-m. When predicting, we normalize by the unconditional probability of each answer as described in 2.</p><p>Context â†’ How to apply sealant to wood.</p><p>Correct Answer â†’ Using a brush, brush on sealant onto wood until it is fully saturated with the sealant. Incorrect Answer â†’ Using a brush, drip on sealant onto wood until it is fully saturated with the sealant.  Figure G.6: Formatted dataset example for ReCoRD. We consider the context above to be a single \"problem\" because this is how the task is presented in the ReCoRD dataset and scored in the ReCoRD evaluation script.      Correct Answer â†’ a bacterial population in the bloodstream Incorrect Answer â†’ a leg muscle relaxing after exercise Incorrect Answer â†’ several viral particles on the skin Incorrect Answer â†’ carbohydrates being digested in the stomach Figure G.16: Formatted dataset example for ARC (Easy). When predicting, we normalize by the unconditional probability of each answer as described in 2.</p><p>Context â†’ Bob went to the gas station to fill up his car. His tank was completely empty and so was his wallet. The cashier offered to pay for his gas if he came back later to pay. Bob felt grateful as he drove home.</p><p>Correct Answer â†’ Bob believed that there were good people in the world. Incorrect Answer â†’ Bob contemplated how unfriendly the world was.   The Luftwaffe flew 4,000 sorties that month, including 12 major and three heavy attacks. The electronic war intensified but the Luftwaffe flew major inland missions only on moonlit nights. Ports were easier to find and made better targets. To confuse the British, radio silence was observed until the bombs fell. X-and Y-GerÃ¤t beams were placed over false targets and switched only at the last minute. Rapid frequency changes were introduced for X-GerÃ¤t, whose wider band of frequencies and greater tactical flexibility ensured it remained effective at a time when British selective jamming was degrading the effectiveness of Y-GerÃ¤t.</p><p>Q: How many sorties were flown in March 1941?</p><p>A: 4,000 Q: When did the Luftwaffe fly inland missions?</p><p>A:</p><p>Target Completion â†’ only on moonlit nights</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure G.28: Formatted dataset example for SQuADv2</head><p>Context â†’ Normal force --In a simple case such as an object resting upon a table, the normal force on the object is equal but in opposite direction to the gravitational force applied on the object (or the weight of the object), that is, N = m g (\\displaystyle N=mg), where m is mass, and g is the gravitational field strength (about 9.81 m/s on Earth). The normal force here represents the force applied by the table against the object that prevents it from sinking through the table and requires that the table is sturdy enough to deliver this normal force without breaking. However, it is easy to assume that the normal force and weight are action-reaction force pairs (a common mistake). In this case, the normal force and weight need to be equal in magnitude to explain why there is no upward acceleration of the object. For example, a ball that bounces upwards accelerates upwards because the normal force acting on the ball is larger in magnitude than the weight of the ball. question: is the normal force equal to the force of gravity? answer:</p><p>Target Completion â†’ yes</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure G.29: Formatted dataset example for BoolQ</head><p>Context â†’ The trend toward lower rents may seem surprising given that some communities in New York are bemoaning the loss of favorite local businesses to high rents. But, despite the recent softening, for many of these retailers there's still been too big a jump from the rental rates of the late 1970s, when their leases were signed. Certainly, the recent drop in prices doesn't mean Manhattan comes cheap.     </p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure G.39: Formatted dataset example for Frâ†’En</head><p>Context â†’ The truth is that you want, at any price, and against the wishes of the peoples of Europe, to continue the negotiations for Turkey's accession to the European Union, despite Turkey's continuing refusal to recognise Cyprus and despite the fact that the democratic reforms are at a standstill. = Target Completion â†’ AdevÈƒrul este cÈƒ vÈƒ doriÅ£i, cu orice preÅ£ÅŸiÃ®mpotriva dorinÅ£ei europenilor, sÈƒ continuaÅ£i negocierile de aderare a Turciei la Uniunea EuropeanÈƒ,Ã®n ciuda refuzului continuu al Turciei de a recunoaÅŸte CipruÄ¼ siÃ®n ciuda faptului cÈƒ reformele democratice au ajunsÃ®ntr-un punct mort.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure G.40: Formatted dataset example for Enâ†’Ro</head><p>Context â†’ AdevÈƒrul este cÈƒ vÈƒ doriÅ£i, cu orice preÅ£ÅŸiÃ®mpotriva dorinÅ£ei europenilor, sÈƒ continuaÅ£i negocierile de aderare a Turciei la Uniunea EuropeanÈƒ,Ã®n ciuda refuzului continuu al Turciei de a recunoaÅŸte CipruÄ¼ siÃ®n ciuda faptului cÈƒ reformele democratice au ajunsÃ®ntr-un punct mort. = Target Completion â†’ The truth is that you want, at any price, and against the wishes of the peoples of Europe, to continue the negotiations for Turkey's accession to the European Union, despite Turkey's continuing refusal to recognise Cyprus and despite the fact that the democratic reforms are at a standstill.    acc n/a 50 0.05 0.05 0.00 0.00 0.05 0.05 0.15 4.00 0.00 0.00 0.10 0.00 0.00 0.10 0.80 14.0 0.00 0.05 0.05 0.00 0.15 0.15 0.40 25.5 4D-acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.10 7.50 0.00 0.00 0.00 0.00 0.05 0.00 0.50 14.0 0.00 0.05 0.00 0.00 0.10 0.05 0.40 26.8 5D+ acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.65 0.00 0.00 0.00 0.00 0.00 0.00 0.05 3.45 0.00 0.00 0.00 0.00 0.00 0.00 0.05 9.30 5D-acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.80 0.00 0.00 0.00 0.00 0.00 0.00 0.05 3.75 0.00 0.00 0.00 0.00 0.00 0.00 0.00           </p></div><figure xml:id=\"fig_0\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A</head><label/><figDesc>Details of Common Crawl Filtering B Details of Model Training C Details of Test Set Contamination Studies D Total Compute Used to Train Language Models E Human Quality Assessment of Synthetic News Articles F Additional Samples from GPT-3 G Details of Task Phrasing and Specifications H Results on All Tasks for All Model Sizes</figDesc></figure>\n",
       " <figure xml:id=\"fig_1\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 3</head><label>3</label><figDesc>Figure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy validation loss) follows a power-law trend with the amount of compute used for training. The power-law behavior observed in [KMH + 20] continues for an additional two orders of magnitude with only small deviations from the predicted curve. For this figure, we exclude embedding parameters from compute and parameter counts.</figDesc></figure>\n",
       " <figure xml:id=\"fig_2\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure</head><label/><figDesc>Figure3.2: On LAMBADA, the few-shot capability of language models results in a strong boost to accuracy. GPT-3 2.7B outperforms the SOTA 17B parameter Turing-NLG<ref target=\"#b152\" type=\"bibr\">[Tur20]</ref> in this setting, and GPT-3 175B advances the state of the art by 18%. Note zero-shot uses a different format from one-shot and few-shot as described in the text.</figDesc></figure>\n",
       " <figure xml:id=\"fig_3\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 3 . 3 :</head><label>33</label><figDesc>Figure3.3: On TriviaQA GPT3's performance grows smoothly with model size, suggesting that language models continue to absorb knowledge as their capacity increases. One-shot and few-shot performance make significant gains over zero-shot behavior, matching and exceeding the performance of the SOTA fine-tuned open-domain model, RAG[LPP + 20]    </figDesc></figure>\n",
       " <figure xml:id=\"fig_4\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 3 . 5 :</head><label>35</label><figDesc>Figure 3.5: Zero-, one-, and few-shot performance on the adversarial Winogrande dataset as model capacity scales.Scaling is relatively smooth with the gains to few-shot learning increasing with model size, and few-shot GPT-3 175B is competitive with a fine-tuned RoBERTA-large.</figDesc></figure>\n",
       " <figure xml:id=\"fig_5\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 3 . 9 :</head><label>39</label><figDesc>Figure3.9: Performance of GPT-3 on ANLI Round 3. Results are on the dev-set, which has only 1500 examples and therefore has high variance (we estimate a standard deviation of 1.2%). We find that smaller models hover around random chance, while few-shot GPT-3 175B closes almost half the gap from random chance to SOTA. Results for ANLI rounds 1 and 2 are shown in the appendix.</figDesc></figure>\n",
       " <figure xml:id=\"fig_6\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 3 .</head><label>3</label><figDesc>Figure3.11: Few-shot performance on the five word scrambling tasks for different sizes of model. There is generally smooth improvement with model size although the random insertion task shows an upward slope of improvement with the 175B model solving the task the majority of the time. Scaling of one-shot and zero-shot performance is shown in the appendix. All tasks are done with K = 100.</figDesc></figure>\n",
       " <figure xml:id=\"fig_7\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 3 .</head><label>3</label><figDesc>Figure3.15: The GPT-3 generated news article that humans found the easiest to distinguish from a human written article (accuracy: 61%).</figDesc></figure>\n",
       " <figure xml:id=\"fig_8\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 4</head><label>4</label><figDesc>Figure 4.1: GPT-3 Training Curves We measure model performance during training on a deduplicated validation split of our training distribution. Though there is some gap between training and validation performance, the gap grows only minimally with model size and training time, suggesting that most of the gap comes from a difference in difficulty rather than overfitting.</figDesc></figure>\n",
       " <figure xml:id=\"fig_9\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 4 . 2 :</head><label>42</label><figDesc>Figure 4.2: Benchmark contamination analysisWe constructed cleaned versions of each of our benchmarks to check for potential contamination in our training set. The x-axis is a conservative lower bound for how much of the dataset is known with high confidence to be clean, and the y-axis shows the difference in performance when evaluating only on the verified clean subset. Performance on most benchmarks changed negligibly, but some were flagged for further review. On inspection we find some evidence for contamination of the PIQA and Winograd results, and we mark the corresponding results in Section 3 with an asterisk. We find no evidence that other benchmarks are affected.</figDesc></figure>\n",
       " <figure xml:id=\"fig_10\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 6 . 1 :</head><label>61</label><figDesc>Figure 6.1: Racial Sentiment Across Models</figDesc></figure>\n",
       " <figure xml:id=\"fig_11\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure E. 1 :</head><label>1</label><figDesc>Figure E.1: Participants spend more time trying to identify whether each news article is machine generated as model size increases. Duration on the control model is indicated with the dashed line. Line of best fit is a linear model on a log scale with 95% confidence intervals.</figDesc></figure>\n",
       " <figure xml:id=\"fig_12\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure</head><label/><figDesc/></figure>\n",
       " <figure xml:id=\"fig_13\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure G. 4 :</head><label>4</label><figDesc>Figure G.4: Formatted dataset example for PIQA</figDesc></figure>\n",
       " <figure xml:id=\"fig_14\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure G. 5 :</head><label>5</label><figDesc>Figure G.5: Formatted dataset example for COPA</figDesc></figure>\n",
       " <figure xml:id=\"fig_15\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure G. 12 :</head><label>12</label><figDesc>Figure G.12: Formatted dataset example for SAT Analogies</figDesc></figure>\n",
       " <figure xml:id=\"fig_16\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure G. 13 :</head><label>13</label><figDesc>Figure G.13: Formatted dataset example for Winograd. The 'partial' evaluation method we use compares the probability of the completion given a correct and incorrect context.</figDesc></figure>\n",
       " <figure xml:id=\"fig_17\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure G. 15 :</head><label>15</label><figDesc>Figure G.15: Formatted dataset example for MultiRC. There are three levels within MultiRC: (1) the passage, (2) the questions, and (3) the answers. During evaluation, accuracy is determined at the per-question level, with a question being considered correct if and only if all the answers within the question are labeled correctly. For this reason, we use K to refer to the number of questions shown within the context.</figDesc></figure>\n",
       " <figure xml:id=\"fig_18\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Context</head><label/><figDesc>â†’ Question: Which factor will most likely cause a person to develop a fever? Answer:</figDesc></figure>\n",
       " <figure xml:id=\"fig_19\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure</head><label/><figDesc>Figure G.17: Formatted dataset example for StoryCloze</figDesc></figure>\n",
       " <figure xml:id=\"fig_20\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure G. 25 :</head><label>25</label><figDesc>Figure G.25: Formatted dataset example for QuAC</figDesc></figure>\n",
       " <figure xml:id=\"fig_21\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure G. 34 :</head><label>34</label><figDesc>Figure G.34: Formatted dataset example for TriviaQA. TriviaQA allows for multiple valid completions.</figDesc></figure>\n",
       " <figure xml:id=\"fig_22\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure G. 36 :</head><label>36</label><figDesc>Figure G.36: Formatted dataset example for Deâ†’En. This is the format for one-and few-shot learning, for this and other langauge tasks, the format for zero-shot learning is \"Q: What is the {language} translation of {sentence} A: {translation}.\"</figDesc></figure>\n",
       " <figure xml:id=\"fig_23\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure G. 37 :</head><label>37</label><figDesc>Figure G.37: Formatted dataset example for Enâ†’De</figDesc></figure>\n",
       " <figure xml:id=\"fig_24\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure G. 38 :</head><label>38</label><figDesc>Figure G.38: Formatted dataset example for Enâ†’Fr</figDesc></figure>\n",
       " <figure xml:id=\"fig_25\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure G. 41 :Target Completion â†’ 3 Figure G. 43 :Target Completion â†’ 143 Figure G. 44 :Target Completion â†’ 21 Figure G. 46 :Target Completion â†’ 1053 Figure G. 47 :Target Completion â†’ 10540 Figure G. 49 :Target Completion â†’ 81564 Figure G. 51 :</head><label>4134314344214610534710540498156451</label><figDesc>Figure G.41: Formatted dataset example for Roâ†’En</figDesc></figure>\n",
       " <figure xml:id=\"fig_27\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure H. 1 :</head><label>1</label><figDesc>Figure H.1: All results for all SuperGLUE tasks.</figDesc></figure>\n",
       " <figure xml:id=\"fig_28\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure H. 2 :</head><label>2</label><figDesc>Figure H.2: Results for SAT task. Figure H.3: All results for all Winograd tasks.</figDesc></figure>\n",
       " <figure xml:id=\"fig_29\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure H. 4 :</head><label>4</label><figDesc>Figure H.4: All results for all Arithmetic tasks.</figDesc></figure>\n",
       " <figure xml:id=\"fig_30\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure H. 5 :</head><label>5</label><figDesc>Figure H.5: All results for all Cloze and Completion tasks.</figDesc></figure>\n",
       " <figure xml:id=\"fig_31\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure H. 6 :</head><label>6</label><figDesc>Figure H.6: All results for all Common Sense Reasoning tasks.</figDesc></figure>\n",
       " <figure xml:id=\"fig_32\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure H. 7 :</head><label>7</label><figDesc>Figure H.7: All results for all QA tasks.</figDesc></figure>\n",
       " <figure xml:id=\"fig_33\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure H. 8 :</head><label>8</label><figDesc>Figure H.8: All results for all Reading Comprehension tasks.</figDesc></figure>\n",
       " <figure xml:id=\"fig_34\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure H. 9 :</head><label>9</label><figDesc>Figure H.9: All results for all ANLI rounds.</figDesc></figure>\n",
       " <figure xml:id=\"fig_35\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure H. 10 :</head><label>10</label><figDesc>Figure H.10: All results for all Scramble tasks.</figDesc></figure>\n",
       " <figure xml:id=\"fig_36\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure H. 11 :</head><label>11</label><figDesc>Figure H.11: All results for all Translation tasks.</figDesc></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_1\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>Name n params n layers d model n heads d head Batch Size Learning Rate</figDesc><table><row><cell>Model GPT-3 Small</cell><cell>125M</cell><cell>12</cell><cell>768</cell><cell>12</cell><cell>64</cell><cell>0.5M</cell><cell>6.0 Ã— 10 âˆ’4</cell></row><row><cell>GPT-3 Medium</cell><cell>350M</cell><cell>24</cell><cell>1024</cell><cell>16</cell><cell>64</cell><cell>0.5M</cell><cell>3.0 Ã— 10 âˆ’4</cell></row><row><cell>GPT-3 Large</cell><cell>760M</cell><cell>24</cell><cell>1536</cell><cell>16</cell><cell>96</cell><cell>0.5M</cell><cell>2.5 Ã— 10 âˆ’4</cell></row><row><cell>GPT-3 XL</cell><cell>1.3B</cell><cell>24</cell><cell>2048</cell><cell>24</cell><cell>128</cell><cell>1M</cell><cell>2.0 Ã— 10 âˆ’4</cell></row><row><cell>GPT-3 2.7B</cell><cell>2.7B</cell><cell>32</cell><cell>2560</cell><cell>32</cell><cell>80</cell><cell>1M</cell><cell>1.6 Ã— 10 âˆ’4</cell></row><row><cell>GPT-3 6.7B</cell><cell>6.7B</cell><cell>32</cell><cell>4096</cell><cell>32</cell><cell>128</cell><cell>2M</cell><cell>1.2 Ã— 10 âˆ’4</cell></row><row><cell>GPT-3 13B</cell><cell>13.0B</cell><cell>40</cell><cell>5140</cell><cell>40</cell><cell>128</cell><cell>2M</cell><cell>1.0 Ã— 10 âˆ’4</cell></row><row><cell cols=\"2\">GPT-3 175B or \"GPT-3\" 175.0B</cell><cell>96</cell><cell>12288</cell><cell>96</cell><cell>128</cell><cell>3.2M</cell><cell>0.6 Ã— 10 âˆ’4</cell></row></table><note>Sections 2.1-2.3 below give details on our models, training data, and training process respectively. Section 2.4 discusses the details of how we do few-shot, one-shot, and zero-shot evaluations.</note></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_2\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 2</head><label>2</label><figDesc/><table><row><cell/><cell>Quantity</cell><cell>Weight in</cell><cell>Epochs elapsed when</cell></row><row><cell>Dataset</cell><cell>(tokens)</cell><cell>training mix</cell><cell>training for 300B tokens</cell></row><row><cell cols=\"2\">Common Crawl (filtered) 410 billion</cell><cell>60%</cell><cell>0.44</cell></row><row><cell>WebText2</cell><cell>19 billion</cell><cell>22%</cell><cell>2.9</cell></row><row><cell>Books1</cell><cell>12 billion</cell><cell>8%</cell><cell>1.9</cell></row><row><cell>Books2</cell><cell>55 billion</cell><cell>8%</cell><cell>0.43</cell></row><row><cell>Wikipedia</cell><cell>3 billion</cell><cell>3%</cell><cell>3.4</cell></row></table><note>Figure 2.2: Total compute used during training. Based on the analysis in Scaling Laws For Neural Language Models [KMH + 20] we train much larger models on many fewer tokens than is typical. As a consequence, although GPT-3 3B is almost 10x larger than RoBERTa-Large (355M params), both models took roughly 50 petaflop/s-days of compute during pre-training. Methodology for these calculations can be found in Appendix D. .2: Datasets used to train GPT-3. \"Weight in training mix\" refers to the fraction of examples during training</note></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_3\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 3 .</head><label>3</label><figDesc/><table/><note>1: Zero-shot results on PTB language modeling dataset. Many other common language modeling datasets are omitted because they are derived from Wikipedia or other sources which are included in GPT-3's training data. a [RWC + 19]</note></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_4\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>Wikipedia-related tasks in that work because they are entirely contained in our training data, and we also omit the one-billion word benchmark due to a high fraction of the dataset being contained in our training set. PTB escapes these issues due to predating the modern internet. Our largest model sets a new SOTA on PTB by a substantial margin of 15 points, achieving a perplexity of 20.50. Note that since PTB is a traditional language modeling dataset it does not have a clear separation of examples to define one-shot or few-shot evaluation around, so we measure only zero-shot.</figDesc><table><row><cell/><cell>LAMBADA</cell><cell>LAMBADA</cell><cell>StoryCloze</cell><cell>HellaSwag</cell></row><row><cell>Setting</cell><cell>(acc)</cell><cell>(ppl)</cell><cell>(acc)</cell><cell>(acc)</cell></row><row><cell>SOTA</cell><cell>68.0 a</cell><cell>8.63 b</cell><cell>91.8 c</cell><cell>85.6 d</cell></row><row><cell>GPT-3 Zero-Shot</cell><cell>76.2</cell><cell>3.00</cell><cell>83.2</cell><cell>78.9</cell></row><row><cell>GPT-3 One-Shot</cell><cell>72.5</cell><cell>3.35</cell><cell>84.7</cell><cell>78.1</cell></row><row><cell>GPT-3 Few-Shot</cell><cell>86.4</cell><cell>1.92</cell><cell>87.7</cell><cell>79.3</cell></row><row><cell cols=\"5\">Table 3.2: Performance on cloze and completion tasks. GPT-3 significantly improves SOTA on LAMBADA while</cell></row><row><cell cols=\"5\">achieving respectable performance on two difficult completion prediction datasets. a [Tur20] b [RWC + 19] c [LDL19]</cell></row><row><cell>d [LCH + 20]</cell><cell/><cell/><cell/><cell/></row></table><note>calculate zero-shot perplexity on the Penn Tree Bank (PTB) [MKM + 94] dataset measured in [RWC + 19]. We omit the 43.1.2 LAMBADAThe LAMBADA dataset [PKL + 16] tests the modeling of long-range dependencies in text -the model is asked to predict the last word of sentences which require reading a paragraph of context. It has recently been suggested that the continued scaling of language models is yielding diminishing returns on this difficult benchmark. [BHT + 20] reflect on the small 1.5% improvement achieved by a doubling of model size between two recent state of the art results ([SPP + 19]   </note></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_6\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 3</head><label>3</label><figDesc/><table><row><cell>.3. On TriviaQA, we achieve 64.3% in the zero-shot setting, 68.0% in the</cell></row><row><cell>one-shot setting, and 71.2% in the few-shot setting. The zero-shot result already outperforms the fine-tuned T5-11B by</cell></row><row><cell>14.2%, and also outperforms a version with Q&amp;A tailored span prediction during pre-training by 3.8%. The one-shot</cell></row><row><cell>result improves by 3.7% and matches the SOTA for an open-domain QA system which not only fine-tunes but also</cell></row><row><cell>makes use of a learned retrieval mechanism over a 15.3B parameter dense vector index of 21M documents [LPP + 20].</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_7\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 3</head><label>3</label><figDesc>Results on the WSC273 version of Winograd schemas and the adversarial Winogrande dataset. See Section 4 for details on potential contamination of the Winograd test set. a [SBBC19] b [LYN + 20]</figDesc><table><row><cell>.4. Zero-shot GPT-3, which only receives on a natural language description of the task,</cell></row><row><cell>still underperforms recent unsupervised NMT results. However, providing only a single example demonstration for</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_9\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>Table 3.7: Results on reading comprehension tasks. All scores are F1 except results for RACE which report accuracy.</figDesc><table><row><cell>Setting</cell><cell cols=\"6\">CoQA DROP QuAC SQuADv2 RACE-h RACE-m</cell></row><row><cell cols=\"2\">Fine-tuned SOTA 90.7 a</cell><cell>89.1 b</cell><cell>74.4 c</cell><cell>93.0 d</cell><cell>90.0 e</cell><cell>93.1 e</cell></row><row><cell cols=\"2\">GPT-3 Zero-Shot 81.5</cell><cell>23.6</cell><cell>41.5</cell><cell>59.5</cell><cell>45.5</cell><cell>58.4</cell></row><row><cell>GPT-3 One-Shot</cell><cell>84.0</cell><cell>34.3</cell><cell>43.3</cell><cell>65.4</cell><cell>45.9</cell><cell>57.4</cell></row><row><cell>GPT-3 Few-Shot</cell><cell>85.0</cell><cell>36.5</cell><cell>44.3</cell><cell>69.8</cell><cell>46.8</cell><cell>58.1</cell></row></table><note>a [JZC + 19] b [JN20] c [AI19] d [QIA20] e [SPP + 19]</note></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_10\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>Table3.8: Performance of GPT-3 on SuperGLUE compared to fine-tuned baselines and SOTA. All results are reported on the test set. GPT-3 few-shot is given a total of 32 examples within the context of each task and performs no gradient updates.</figDesc><table><row><cell/><cell>SuperGLUE</cell><cell>BoolQ</cell><cell>CB</cell><cell>CB</cell><cell>COPA</cell><cell>RTE</cell></row><row><cell/><cell>Average</cell><cell cols=\"2\">Accuracy Accuracy</cell><cell>F1</cell><cell cols=\"2\">Accuracy Accuracy</cell></row><row><cell>Fine-tuned SOTA</cell><cell>89.0</cell><cell>91.0</cell><cell>96.9</cell><cell>93.9</cell><cell>94.8</cell><cell>92.5</cell></row><row><cell>Fine-tuned BERT-Large</cell><cell>69.0</cell><cell>77.4</cell><cell>83.6</cell><cell>75.7</cell><cell>70.6</cell><cell>71.7</cell></row><row><cell>GPT-3 Few-Shot</cell><cell>71.8</cell><cell>76.4</cell><cell>75.6</cell><cell>52.0</cell><cell>92.0</cell><cell>69.0</cell></row><row><cell/><cell>WiC</cell><cell>WSC</cell><cell cols=\"4\">MultiRC MultiRC ReCoRD ReCoRD</cell></row><row><cell/><cell cols=\"3\">Accuracy Accuracy Accuracy</cell><cell>F1a</cell><cell>Accuracy</cell><cell>F1</cell></row><row><cell>Fine-tuned SOTA</cell><cell>76.1</cell><cell>93.8</cell><cell>62.3</cell><cell>88.2</cell><cell>92.5</cell><cell>93.3</cell></row><row><cell>Fine-tuned BERT-Large</cell><cell>69.6</cell><cell>64.6</cell><cell>24.1</cell><cell>70.0</cell><cell>71.3</cell><cell>72.0</cell></row><row><cell>GPT-3 Few-Shot</cell><cell>49.4</cell><cell>80.1</cell><cell>30.5</cell><cell>75.4</cell><cell>90.2</cell><cell>91.1</cell></row></table><note>WPN + 19] [WPN + 19] [CLC + 19] [DMST19] [RBG11] [KCR + 18] [ZLL + 18] [DGM06] [BHDD + 06] [GMDD07] [BDD + 09] [PCC18] [PHR + 18]. GPT-3's test-set performance on the SuperGLUE dataset is shown in Table 3.8. In the few-shot setting, we used 32 examples for all tasks, sampled randomly from the training set. For all tasks except WSCFigure 3.7: GPT-3 results on CoQA reading comprehension task. GPT-3 175B achieves 85 F1 in the few-shot setting, only a few points behind measured human performance and state-of-the-art fine-tuned models. Zero-shot and one-shot performance is a few points behind, with the gains to few-shot being largest for bigger models.Figure 3.8: Performance on SuperGLUE increases with model size and number of examples in context. A value of K = 32 means that our model was shown 32 examples per task, for 256 examples total divided across the 8 tasks in SuperGLUE. We report GPT-3 values on the dev set, so our numbers are not directly comparable to the dotted reference lines (our test set results are in Table 3.8). The BERT-Large reference model was fine-tuned on the SuperGLUE training set (125K examples), whereas BERT++ was first fine-tuned on MultiNLI (392K examples) and SWAG (113K examples) before further fine-tuning on the SuperGLUE training set (for a total of 630K fine-tuning examples). We find the difference in performance between the BERT-Large and BERT++ to be roughly equivalent to the difference between GPT-3 with one example per context versus eight examples per context. and MultiRC, we sampled a new set of examples to use in the context for each problem. For WSC and MultiRC, we used the same set of randomly drawn examples from the training set as context for all of the problems we evaluated.</note></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_15\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 6 .</head><label>6</label><figDesc>1: Most Biased Descriptive Words in 175B Model Top 10 Most Biased Male Descriptive Words with Raw Co-Occurrence Counts Top 10 Most Biased Female Descriptive Words with Raw Co-Occurrence Counts</figDesc><table><row><cell>Average Number of Co-Occurrences Across All Words:</cell><cell>Average Number of Co-Occurrences Across All Words:</cell></row><row><cell>17.5</cell><cell>23.9</cell></row><row><cell>Large (16)</cell><cell>Optimistic (12)</cell></row><row><cell>Mostly (15)</cell><cell>Bubbly (12)</cell></row><row><cell>Lazy (14)</cell><cell>Naughty (12)</cell></row><row><cell>Fantastic (13)</cell><cell/></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_16\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>An early work scaled LSTM based language models to over a billion parameters [JVS + 16]. One line of work straightforwardly increases the size of transformer models, scaling up parameters and FLOPS-per-token roughly in proportion. Work in this vein has successively increased model size: 213 million parameters [VSP + 17] in the original paper, 300 million parameters [DCLT18], 1.5 billion parameters [RWC + 19], 8 billion parameters [SPP + 19], 11 billion parameters [RSR + 19], and most recently 17 billion parameters<ref target=\"#b152\" type=\"bibr\">[Tur20]</ref>. A second line of work has focused on increasing parameter count but not computation, as a means of increasing models' capacity to store information without increased computational cost. These approaches rely on the conditional computation framework<ref target=\"#b33\" type=\"bibr\">[BLC13]</ref> and specifically, the mixture-of-experts method [SMM + 17] has been used to produce 100 billion parameter models and more recently 50 billion parameter translation models [AJF19], though only a small fraction of the parameters are actually used on each forward pass. A third approach increases computation without increasing parameters; examples of this approach include adaptive computation time<ref target=\"#b58\" type=\"bibr\">[Gra16]</ref> and the universal transformer[DGV + 18]. Our work focuses on the first approach (scaling compute and parameters together, by straightforwardly making the neural net larger), and increases model size 10x beyond previous models that employ this strategy.Several efforts have also systematically studied the effect of scale on language model performance. [KMH + 20, RRBS19, LWS + 20, HNA + 17], find a smooth power-law trend in loss as autoregressive language models are scaled up.</figDesc><table/></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_17\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>19] as well as general<ref target=\"#b66\" type=\"bibr\">[HVD15]</ref> and task-specific [SDCW19, JYS + 19, KR16] approaches to distillation of language models. These architectures and techniques are potentially complementary to our work, and could be applied to decrease latency and memory footprint of giant models.As fine-tuned language models have neared human performance on many standard benchmark tasks, considerable effort has been devoted to constructing more difficult or open-ended tasks, including question answering [KPR + 19, IBGC + 14, CCE + 18, MCKS18], reading comprehension [CHI + 18, RCM19], and adversarially constructed datasets designed to be difficult for existing language models [SBBC19, NWD + 19]. In this work we test our models on many of these datasets.Many previous efforts have focused specifically on question-answering, which constitutes a significant fraction of the tasks we tested on. Recent efforts include [RSR + 19, RRS20], which fine-tuned an 11 billion parameter language model, and [GLT + 20], which focused on attending over a large corpus of data at test time. Our work differs in focusing on in-context learning but could be combined in the future with those of[GLT +  20, LPP + 20]. Metalearning in language models has been utilized in [RWC + 19], though with much more limited results and no systematic study. More broadly, language model metalearning has an inner-loop-outer-loop structure, making it structurally similar to metalearning as applied to ML in general. Here there is an extensive literature, including matching networks [VBL + 16], RL2 [DSC + 16], learning to optimize [RL16, ADG + 16, LM17] and MAML [FAL17].</figDesc><table/><note>Our approach of stuffing the model's context with previous examples is most structurally similar to RL2 and also resembles<ref target=\"#b67\" type=\"bibr\">[HYC01]</ref>, in that an inner loop of adaptation takes place through computation in the model's activations across timesteps, without updating the weights, while an outer loop (in this case just language model pre-training) updates the weights, and implicitly learns the ability to adapt to or at least recognize tasks defined at inference-time. Few-shot auto-regressive density estimation was explored in[RCP +  17] and [GWC + 18] studied low-resource NMT as a few-shot learning problem.</note></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_18\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>15, LSP + 18] and multi-stage fine-tuning has recently become a standardized part of SOTA results on some datasets<ref target=\"#b117\" type=\"bibr\">[PFB18]</ref> and pushed the boundaries on certain tasks [KKS + 20], but is still limited by the need to manually curate collections of datasets and set up training curricula. By contrast pre-training at large enough scale appears to offer a \"natural\" broad distribution of tasks implicitly contained in predicting the text itself. One direction for future work might be attempting to generate a broader set of explicit tasks for multi-task learning, for example through procedural generation [TFR + 17], human interaction [ZSW + 19b], or active learning<ref target=\"#b101\" type=\"bibr\">[Mac92]</ref>.Algorithmic innovation in language models over the last two years has been enormous, including denoising-based bidirectionality<ref target=\"#b43\" type=\"bibr\">[DCLT18]</ref>, prefixLM<ref target=\"#b47\" type=\"bibr\">[DL15]</ref> and encoder-decoder architectures [LLG + 19, RSR + 19], random permutations during training [YDY + 19], architectures that improve the efficiency of sampling [DYY + 19], improvements in data and training procedures [LOG + 19], and efficiency increases in the embedding parameters [LCG + 19]. Many of these techniques provide significant gains on downstream tasks.</figDesc><table/></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_20\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table C .</head><label>C</label><figDesc>1: Overlap statistics for all datasets sorted from dirtiest to cleanest. We consider a dataset example dirty if it has a single N -gram collision with any document in our training corpus. \"Relative Difference Clean vs All\" shows the percent change in performance between only the clean examples vs all the examples in the benchmark. \"Count\" shows the number of examples. \"Clean percentage\" is the percent of examples that are clean vs total.</figDesc><table/></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_21\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>Table D.1 and are explained within the table caption.</figDesc><table><row><cell/><cell>Total train</cell><cell>Total train</cell><cell/><cell/><cell>Flops</cell><cell/><cell>Fwd-pass flops per</cell><cell>Frac of params active</cell></row><row><cell/><cell>compute</cell><cell>compute</cell><cell>Params</cell><cell>Training tokens</cell><cell>per param</cell><cell>Mult for</cell><cell>active param</cell><cell>for each</cell></row><row><cell>Model</cell><cell>(PF-days)</cell><cell>(flops)</cell><cell>(M)</cell><cell>(billions)</cell><cell>per token</cell><cell>bwd pass</cell><cell>per token</cell><cell>token</cell></row><row><cell>T5-Small</cell><cell>2.08E+00</cell><cell>1.80E+20</cell><cell>60</cell><cell>1,000</cell><cell>3</cell><cell>3</cell><cell>1</cell><cell>0.5</cell></row><row><cell>T5-Base</cell><cell>7.64E+00</cell><cell>6.60E+20</cell><cell>220</cell><cell>1,000</cell><cell>3</cell><cell>3</cell><cell>1</cell><cell>0.5</cell></row><row><cell>T5-Large</cell><cell>2.67E+01</cell><cell>2.31E+21</cell><cell>770</cell><cell>1,000</cell><cell>3</cell><cell>3</cell><cell>1</cell><cell>0.5</cell></row><row><cell>T5-3B</cell><cell>1.04E+02</cell><cell>9.00E+21</cell><cell>3,000</cell><cell>1,000</cell><cell>3</cell><cell>3</cell><cell>1</cell><cell>0.5</cell></row><row><cell>T5-11B</cell><cell>3.82E+02</cell><cell>3.30E+22</cell><cell>11,000</cell><cell>1,000</cell><cell>3</cell><cell>3</cell><cell>1</cell><cell>0.5</cell></row><row><cell>BERT-Base</cell><cell>1.89E+00</cell><cell>1.64E+20</cell><cell>109</cell><cell>250</cell><cell>6</cell><cell>3</cell><cell>2</cell><cell>1.0</cell></row><row><cell>BERT-Large</cell><cell>6.16E+00</cell><cell>5.33E+20</cell><cell>355</cell><cell>250</cell><cell>6</cell><cell>3</cell><cell>2</cell><cell>1.0</cell></row><row><cell>RoBERTa-Base</cell><cell>1.74E+01</cell><cell>1.50E+21</cell><cell>125</cell><cell>2,000</cell><cell>6</cell><cell>3</cell><cell>2</cell><cell>1.0</cell></row><row><cell cols=\"2\">RoBERTa-Large 4.93E+01</cell><cell>4.26E+21</cell><cell>355</cell><cell>2,000</cell><cell>6</cell><cell>3</cell><cell>2</cell><cell>1.0</cell></row><row><cell>GPT-3 Small</cell><cell>2.60E+00</cell><cell>2.25E+20</cell><cell>125</cell><cell>300</cell><cell>6</cell><cell>3</cell><cell>2</cell><cell>1.0</cell></row><row><cell>GPT-3 Medium</cell><cell>7.42E+00</cell><cell>6.41E+20</cell><cell>356</cell><cell>300</cell><cell>6</cell><cell>3</cell><cell>2</cell><cell>1.0</cell></row><row><cell>GPT-3 Large</cell><cell>1.58E+01</cell><cell>1.37E+21</cell><cell>760</cell><cell>300</cell><cell>6</cell><cell>3</cell><cell>2</cell><cell>1.0</cell></row><row><cell>GPT-3 XL</cell><cell>2.75E+01</cell><cell>2.38E+21</cell><cell>1,320</cell><cell>300</cell><cell>6</cell><cell>3</cell><cell>2</cell><cell>1.0</cell></row><row><cell>GPT-3 2.7B</cell><cell>5.52E+01</cell><cell>4.77E+21</cell><cell>2,650</cell><cell>300</cell><cell>6</cell><cell>3</cell><cell>2</cell><cell>1.0</cell></row><row><cell>GPT-3 6.7B</cell><cell>1.39E+02</cell><cell>1.20E+22</cell><cell>6,660</cell><cell>300</cell><cell>6</cell><cell>3</cell><cell>2</cell><cell>1.0</cell></row><row><cell>GPT-3 13B</cell><cell>2.68E+02</cell><cell>2.31E+22</cell><cell>12,850</cell><cell>300</cell><cell>6</cell><cell>3</cell><cell>2</cell><cell>1.0</cell></row><row><cell>GPT-3 175B</cell><cell>3.64E+03</cell><cell cols=\"2\">3.14E+23 174,600</cell><cell>300</cell><cell>6</cell><cell>3</cell><cell>2</cell><cell>1.0</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_22\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>Table D.1: Starting from the right hand side and moving left, we begin with the number of training tokens that each model was trained with.</figDesc><table/></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_23\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>Participant details and article lengths for each experiment to evaluate human detection of âˆ¼ 200 word model generated news articles. Participants were excluded due to internet check fails.</figDesc><table><row><cell/><cell/><cell/><cell/><cell/><cell>Average</cell></row><row><cell/><cell>Participants</cell><cell>Participants</cell><cell>Genders</cell><cell>Mean</cell><cell>Word Count</cell></row><row><cell>Model</cell><cell>Recruited</cell><cell>Excluded</cell><cell>(m:f:other)</cell><cell>Age</cell><cell>(human:model)</cell></row><row><cell>Control</cell><cell>76</cell><cell>7</cell><cell>32:37:0</cell><cell>39</cell><cell>216:216</cell></row><row><cell>GPT-3 Small</cell><cell>80</cell><cell>7</cell><cell>41:31:1</cell><cell>40</cell><cell>216:188</cell></row><row><cell>GPT-3 Medium</cell><cell>80</cell><cell>7</cell><cell>46:28:2</cell><cell>39</cell><cell>216:202</cell></row><row><cell>GPT-3 Large</cell><cell>81</cell><cell>24</cell><cell>46:28:2</cell><cell>37</cell><cell>216:200</cell></row><row><cell>GPT-3 XL</cell><cell>79</cell><cell>14</cell><cell>32:32:1</cell><cell>38</cell><cell>216:199</cell></row><row><cell>GPT-3 2.7B</cell><cell>80</cell><cell>11</cell><cell>36:33:0</cell><cell>40</cell><cell>216:202</cell></row><row><cell>GPT-3 6.7B</cell><cell>76</cell><cell>5</cell><cell>46:28:2</cell><cell>37</cell><cell>216:195</cell></row><row><cell>GPT-3 13.0B</cell><cell>81</cell><cell>13</cell><cell>46:28:2</cell><cell>37</cell><cell>216:209</cell></row><row><cell>GPT-3 175B</cell><cell>80</cell><cell>9</cell><cell>42:29:0</cell><cell>37</cell><cell>216:216</cell></row><row><cell>Table E.1:</cell><cell/><cell/><cell/><cell/><cell/></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_24\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>Participant details and article lengths for the experiments investigating human detection of âˆ¼ 500 word model generated news articles. Participants were excluded due to internet check fails.</figDesc><table><row><cell/><cell/><cell/><cell/><cell/><cell>Average</cell></row><row><cell/><cell/><cell/><cell/><cell>Mean</cell><cell>Word Count</cell></row><row><cell/><cell/><cell/><cell/><cell>Age</cell><cell>(human:model)</cell></row><row><cell>Control</cell><cell>79</cell><cell>17</cell><cell>32:37:0</cell><cell>39</cell><cell>569:464</cell></row><row><cell>GPT-3 175B</cell><cell>81</cell><cell>19</cell><cell>32:30:0</cell><cell>40</cell><cell>569:498</cell></row><row><cell>Table E.2:</cell><cell/><cell/><cell/><cell/><cell/></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_25\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>Context â†’ anli 2: anli 2: The Gold Coast Hotel &amp; Casino is a hotel and casino located in Paradise, Nevada. This locals' casino is owned and operated by Boyd Gaming. The Gold Coast is located one mile (âˆ¼ 1.6km) west of the Las Vegas Strip on West Flamingo Road. It is located across the street from the Palms Casino Resort and the Rio All Suite Hotel and Casino.</figDesc><table><row><cell>Question: The Gold Coast is a budget-friendly casino. True, False, or</cell></row><row><cell>Neither?</cell></row><row><cell>Correct Answer â†’ Neither</cell></row><row><cell>Incorrect Answer â†’ True</cell></row><row><cell>Incorrect Answer â†’ False</cell></row><row><cell>Figure G.2: Formatted dataset example for ANLI R2</cell></row><row><cell>Context â†’ Article:</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_26\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>Formatted dataset example for ANLI R1 Context â†’ Organisms require energy in order to do what? Correct Answer â†’ mature and develop. Incorrect Answer â†’ rest soundly. Incorrect Answer â†’ absorb light. Incorrect Answer â†’ take in nutrients.Figure G.8: Formatted dataset example for OpenBookQA. When predicting, we normalize by the unconditional probability of each answer as described in 2. â†’ Making a cake: Several cake pops are shown on a display. A woman and girl are shown making the cake pops in a kitchen. TheyCorrect Answer â†’ bake them, then frost and decorate. Incorrect Answer â†’ taste them as they place them on plates. Incorrect Answer â†’ put the frosting on the cake as they pan it. Incorrect Answer â†’ come out and begin decorating the cake as well. Formatted dataset example for HellaSwag Context â†’ anli 3: anli 3: We shut the loophole which has American workers actually subsidizing the loss of their own job. They just passed an expansion of that loophole in the last few days: $43 billion of giveaways, including favors to the oil and gas industry and the people importing ceiling fans from China. Question: The loophole is now gone True, False, or Neither?Correct Answer â†’ False Incorrect Answer â†’ True Incorrect Answer â†’ NeitherFigure G.10: Formatted dataset example for ANLI R3 Context â†’ Question: George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat? Answer:Correct Answer â†’ dry palms Incorrect Answer â†’ wet palms Incorrect Answer â†’ palms covered with oil Incorrect Answer â†’ palms covered with lotion Figure G.11: Formatted dataset example for ARC (Challenge). When predicting, we normalize by the unconditional probability of each answer as described in 2. â†’ lull is to trust as Correct Answer â†’ cajole is to compliance Incorrect Answer â†’ balk is to fortitude Incorrect Answer â†’ betray is to loyalty Incorrect Answer â†’ hinder is to destination Incorrect Answer â†’ soothe is to passion</figDesc><table><row><cell>Context â†’ anli 1: anli 1: Fulton James MacGregor MSP is a Scottish politician who is a Scottish National Party (SNP) Member of Scottish Parliament for the constituency of Coatbridge and Chryston. MacGregor is currently Parliamentary Liaison Officer to Shona Robison, Cabinet Secretary for Health &amp; Sport. He also serves on the Justice and Education &amp; Skills committees in the Scottish Parliament. Question: Fulton James MacGregor is a Scottish politican who is a Liaison officer to Shona Robison who he swears is his best friend. True, False, or Neither? Correct Answer â†’ Neither Incorrect Answer â†’ True Incorrect Answer â†’ False Figure G.7: Context Figure G.9: Context</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_27\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>Correct Context â†’ Johnny likes fruits more than vegetables in his new keto diet because the fruits Incorrect Context â†’ Johnny likes fruits more than vegetables in his new keto diet because the vegetables Target Completion â†’ are saccharine.Figure G.14: Formatted dataset example for Winogrande. The 'partial' evaluation method we use compares the probability of the completion given a correct and incorrect context.He told the two envoys that he would meet with Mullah Omar and press him on Bin Laden. They left, however, reporting to Washington that Pakistan was unlikely in fact to do anything,\" given what it sees as the benefits of Taliban control of Afghanistan.\" President Clinton was scheduled to travel to India. The State Department felt that he should not visit India without also visiting Pakistan. The Secret Service and the CIA, however, warned in the strongest terms that visiting Pakistan would risk the President's life. Counterterrorism officials also argued that Pakistan had not done enough to merit a presidential visit. But President Clinton insisted on including Pakistan in the itinerary for his trip to South Asia. His one-day stopover on March 25, 2000, was the first time a U.S. president had been there since 1969. At his meeting with Musharraf and others, President Clinton concentrated on tensions between Pakistan and India and the dangers of nuclear proliferation, but also discussed Bin Laden. President Clinton told us that when he pulled Musharraf aside for a brief, one-on-one meeting, he pleaded with the general for help regarding Bin Laden.\" I offered him the moon when I went to see him, in terms of better relations with the United States, if he'd help us get Bin Laden and deal with another issue or two.\" The U.S. effort continued.</figDesc><table><row><cell>Context â†’ READING COMPREHENSION ANSWER KEY</cell></row><row><cell>While this process moved along, diplomacy continued its rounds. Direct</cell></row><row><cell>pressure on the Taliban had proved unsuccessful. As one NSC staff note</cell></row><row><cell>put it, \"Under the Taliban, Afghanistan is not so much a state sponsor</cell></row><row><cell>of terrorism as it is a state sponsored by terrorists.\" In early 2000,</cell></row><row><cell>the United States began a high-level effort to persuade Pakistan to use</cell></row><row><cell>its influence over the Taliban. In January 2000, Assistant Secretary</cell></row><row><cell>of State Karl Inderfurth and the State Department's counterterrorism</cell></row><row><cell>coordinator, Michael Sheehan, met with General Musharraf in Islamabad,</cell></row><row><cell>dangling before him the possibility of a presidential visit in March as a</cell></row><row><cell>reward for Pakistani cooperation. Such a visit was coveted by Musharraf,</cell></row><row><cell>partly as a sign of his government's legitimacy. Who did The State Department feel should visit both India and Pakistan?</cell></row><row><cell>Correct Answer â†’ -[False] Bin Laden</cell></row><row><cell>Incorrect Answer â†’ -[True] Bin Laden</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_28\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>G.17: Formatted dataset example for StoryCloze Context â†’ Helsinki is the capital and largest city of Finland. It is in the region of Uusimaa, in southern Finland, on the shore of the Gulf of Finland.Helsinki has a population of , an urban population of , and a metropolitan population of over 1.4 million, making it the most populous municipality and urban area in Finland. Helsinki is some north of Tallinn, Estonia, east of Stockholm, Sweden, and west of Saint Petersburg, Russia. Helsinki has close historical connections with these three cities.The Helsinki metropolitan area includes the urban core of Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns. It is the world's northernmost metro area of over one million people, and the city is the northernmost capital of an EU member state. The Helsinki metropolitan area is the third largest metropolitan area in the Nordic countries after Stockholm and Copenhagen, and the City of Helsinki is the third largest after Stockholm and Oslo. Helsinki is Finland's major political, educational, financial, cultural, and research center as well as one of northern Europe's major cities. Approximately 75% of foreign companies that operate in Finland have settled in the Helsinki region. The nearby municipality of Vantaa is the location of Helsinki Airport, with frequent service to various destinations in Europe and Asia. Formatted dataset example for Cycled Letters Context â†’ Passage: Saint Jean de BrÃ©beuf was a French Jesuit missionary who travelled to New France in 1625. There he worked primarily with the Huron for the rest of his life, except for a few years in France from 1629 to 1633. He learned their language and culture, writing extensively about each to aid other missionaries. In 1649, BrÃ©beuf and another missionary were captured when an Iroquois raid took over a Huron village . Together with Huron captives, the missionaries were ritually tortured and killed on March 16, 1649. BrÃ©beuf was beatified in 1925 and among eight Jesuit missionaries canonized as saints in the Roman Catholic Church in 1930. Formatted dataset example for Natural Questions Context â†’ TITLE: William Perry (American football) -Professional career PARAGRAPH: In 1985, he was selected in the first round of the 1985 NFL Draft by the Chicago Bears; he had been hand-picked by coach Mike Ditka.However, defensive coordinator Buddy Ryan, who had a highly acrimonious relationship with Ditka, called Perry a \"wasted draft-pick\". Perry soon became a pawn in the political power struggle between Ditka and Ryan. Perry's \"Refrigerator\" nickname followed him into the NFL and he quickly became a favorite of the Chicago Bears fans. Teammates called him \"Biscuit,\" as in \"one biscuit shy of 350 pounds.\" While Ryan refused to play Perry, Ditka decided to use Perry as a fullback when the team was near the opponents' goal line or in fourth and short situations, either as a ball carrier or a lead blocker for star running back Walter Payton. Ditka stated the inspiration for using Perry as a fullback came to him during five-yard sprint exercises. During his rookie season, Perry rushed for two touchdowns and caught a pass for one. Perry even had the opportunity to run the ball during Super Bowl XX, as a nod to his popularity and contributions to the team's success. The first time he got the ball, he was tackled for a one-yard loss while attempting to throw his first NFL pass on a halfback option play. The second time he got the ball, he scored a touchdown (running over Patriots linebacker Larry McGrew in the process). About halfway through his rookie season, Ryan finally began to play Perry, who soon proved that he was a capable defensive lineman. His Super Bowl ring size is the largest of any professional football player in the history of the event. His ring size is 25, while the ring size for the average adult male is between 10 and 12. Perry went on to play for ten years in the NFL, retiring after the 1994 season. In his ten years as a pro, he regularly struggled with his weight, which hampered his performance at times. He played in 138 games, recording 29.5 sacks and five fumble recoveries, which he returned for a total of 71 yards. In his offensive career he ran five yards for two touchdowns, and had one reception for another touchdown. Perry later attempted a comeback, playing an unremarkable 1996 season with the London Monarchs of the World League of American Football (later NFL Europa).</figDesc><table><row><cell>Q: what is the most populous municipality in Finland? A: Helsinki Q: how many people live there? A: 1.4 million in the metropolitan area Q: what percent of the foreign companies that operate in Finland are in Helsinki? A: 75% Q: what towns are a part of the metropolitan area? A: Target Completion â†’ Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns Figure G.18: Formatted dataset example for CoQA Context â†’ Please unscramble the letters into a word, and write that word: asinoc = Target Completion â†’ casino before he went back to France for a few years? Answer: Target Completion â†’ 4 Figure G.20: Formatted dataset example for DROP Context â†’ Fill in blank: She held the torch in front of her. She caught her breath. \"Chris? There's a step.\" \"What?\" \"A step. Cut in the rock. About fifty feet ahead.\" She moved faster. They both moved faster. \"In fact,\" she said, raising the torch higher, \"there's more than a . -&gt; Target Completion â†’ step Figure G.19: Question: How many years did Saint Jean de BrÃ©beuf stay in New France Figure G.21: Formatted dataset example for LAMBADA Context â†’ Please unscramble the letters into a word, and write that word: skicts = Target Completion â†’ sticks Figure G.22: Formatted dataset example for Anagrams 1 (A1) Context â†’ Please unscramble the letters into a word, and write that word: volwskagen = Target Completion â†’ volkswagen Figure G.23: Formatted dataset example for Anagrams 2 Context â†’ Q: Who played tess on touched by an angel? A: Target Completion â†’ Delloreese Patricia Early (July 6, 1931 { November 19, 2017), known professionally as Della Reese A: Figure G.24: Q: what team did he play for? Target Completion â†’ the Chicago Bears</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_29\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>Formatted dataset example for CBContext â†’ The bet, which won him dinner for four, was regarding the existence and mass of the top quark, an elementary particle discovered in 1995. question: The Top Quark is the last of six flavors of quarks predicted by the standard model theory of particle physics.True or False? answer: Target Completion â†’ False Figure G.31: Formatted dataset example for RTE Context â†’ An outfitter provided everything needed for the safari. Before his first walking holiday, he went to a specialist outfitter to buy some boots. question: Is the word 'outfitter' used in the same way in the two sentences above? answer: Target Completion â†’ no Figure G.32: Formatted dataset example for WiC Context â†’ Final Exam with Answer Key Instructions: Please carefully read the following passages. For each passage, you must identify which noun the pronoun marked in *bold* refers to. ===== Passage: Mr. Moncrieff visited Chester's luxurious New York apartment, thinking that it belonged to his son Edward. The result was that Mr. Moncrieff has decided to cancel Edward's allowance on the ground that he no longer requires *his* financial support. Question: In the passage above, what does the pronoun \"*his*\" refer to? Answer: Target Completion â†’ mr. moncrieff Figure G.33: Formatted dataset example for WSC</figDesc><table><row><cell>question: Manhattan comes cheap. true, false, or neither? answer: Target Completion â†’ false which 20th century artist? A: Target Completion â†’ MARCEL DUCHAMP Target Completion â†’ r mutt Target Completion â†’ duchamp Target Completion â†’ marcel duchamp Target Completion â†’ R.Mutt Target Completion â†’ Marcel duChamp Target Completion â†’ Henri-Robert-Marcel Duchamp Target Completion â†’ Marcel du Champ Target Completion â†’ henri robert marcel duchamp Target Completion â†’ Duchampian Target Completion â†’ Duchamp Target Completion â†’ duchampian Target Completion â†’ marcel du champ Target Completion â†’ Marcel Duchamp Figure G.30: Context â†’ Q: 'Nude Descending A Staircase' is perhaps the most famous painting by Target Completion â†’ MARCEL DUCHAMP</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_30\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>H Results on All Tasks for All Model Sizes Small Med Large XL 2.7B 6.7B 13B 175B Small Med Large XL 2.7B 6.7B 13B 175B Small Med Large XL 2.7B 6.7B 13B 175B</figDesc><table><row><cell/><cell/><cell/><cell/><cell>Zero-Shot</cell><cell>One-Shot</cell><cell>Few-Shot</cell><cell/></row><row><cell/><cell/><cell/><cell>Fine-tune</cell><cell/><cell/><cell/><cell>175B</cell></row><row><cell>Name</cell><cell>Metric</cell><cell>Split</cell><cell>SOTA K</cell><cell/><cell/><cell/><cell>(test server)</cell></row><row><cell>HellaSwag</cell><cell>acc</cell><cell>dev</cell><cell>85.6 20</cell><cell>33.7 43.6 51.0 54.7 62.8 67.4 70.9 78.9</cell><cell>33.0 42.9 50.5 53.5 61.9 66.5 70.0 78.1</cell><cell>33.5 43.1 51.3 54.9 62.9 67.3 71.3 79.3</cell><cell/></row><row><cell>LAMBADA</cell><cell>acc</cell><cell>test</cell><cell>68.0 15</cell><cell>42.7 54.3 60.4 63.6 67.1 70.3 72.5 76.2</cell><cell>22.0 47.1 52.6 58.3 61.1 65.4 69.0 72.5</cell><cell>22.0 40.4 63.2 57.0 78.1 79.1 81.3 86.4</cell><cell/></row><row><cell>LAMBADA</cell><cell>ppl</cell><cell>test</cell><cell>8.63 15</cell><cell>18.6 9.09 6.53 5.44 4.60 4.00 3.56 3.00</cell><cell>165.0 11.6 8.29 6.46 5.53 4.61 4.06 3.35</cell><cell>165.0 27.6 6.63 7.45 2.89 2.56 2.56 1.92</cell><cell/></row><row><cell>StoryCloze</cell><cell>acc</cell><cell>test</cell><cell>91.8 70</cell><cell>63.3 68.5 72.4 73.4 77.2 77.7 79.5 83.2</cell><cell>62.3 68.7 72.3 74.2 77.3 78.7 79.7 84.7</cell><cell>62.3 70.2 73.9 76.1 80.2 81.2 83.0 87.7</cell><cell/></row><row><cell>NQs</cell><cell>acc</cell><cell>test</cell><cell>44.5 64</cell><cell>0.64 1.75 2.71 4.40 6.01 5.79 7.84 14.6</cell><cell>1.19 3.07 4.79 5.43 8.73 9.78 13.7 23.0</cell><cell>1.72 4.46 7.89 9.72 13.2 17.0 21.0 29.9</cell><cell/></row><row><cell>TriviaQA</cell><cell>acc</cell><cell>dev</cell><cell>68.0 64</cell><cell>4.15 7.61 14.0 19.7 31.3 38.7 41.8 64.3</cell><cell>4.19 12.9 20.5 26.5 35.9 44.4 51.3 68.0</cell><cell>6.96 16.3 26.5 32.1 42.3 51.6 57.5 71.2</cell><cell>71.2</cell></row><row><cell>WebQs</cell><cell>acc</cell><cell>test</cell><cell>45.5 64</cell><cell>1.77 3.20 4.33 4.63 7.92 7.73 8.22 14.4</cell><cell>2.56 6.20 8.51 9.15 14.5 15.1 19.0 25.3</cell><cell>5.46 12.6 15.9 19.6 24.8 27.7 33.5 41.5</cell><cell/></row><row><cell>Roâ†’En</cell><cell cols=\"2\">BLEU-mb test</cell><cell>39.9 64</cell><cell>2.08 2.71 3.09 3.15 16.3 8.34 20.2 19.9</cell><cell>0.55 15.4 23.0 26.3 30.6 33.2 35.6 38.6</cell><cell>1.25 20.7 25.8 29.2 33.1 34.8 37.0 39.5</cell><cell/></row><row><cell>Roâ†’En</cell><cell cols=\"2\">BLEU-sb test</cell><cell>64</cell><cell>2.39 3.08 3.49 3.56 16.8 8.75 20.8 20.9</cell><cell>0.65 15.9 23.6 26.8 31.3 34.2 36.7 40.0</cell><cell>1.40 21.3 26.6 30.1 34.3 36.2 38.4 41.3</cell><cell/></row><row><cell>Enâ†’Ro</cell><cell cols=\"2\">BLEU-mb test</cell><cell>38.5 64</cell><cell>2.14 2.65 2.53 2.50 3.46 4.24 5.32 14.1</cell><cell>0.35 3.30 7.89 8.72 13.2 15.1 17.3 20.6</cell><cell>1.25 5.90 9.33 10.7 14.3 16.3 18.0 21.0</cell><cell/></row><row><cell>Enâ†’Ro</cell><cell cols=\"2\">BLEU-sb test</cell><cell>64</cell><cell>2.61 3.11 3.07 3.09 4.26 5.31 6.43 18.0</cell><cell>0.55 3.90 9.15 10.3 15.7 18.2 20.8 24.9</cell><cell>1.64 7.40 10.9 12.9 17.2 19.6 21.8 25.8</cell><cell/></row><row><cell>Frâ†’En 14</cell><cell cols=\"2\">BLEU-mb test</cell><cell>35.0 64</cell><cell>1.81 2.53 3.47 3.13 20.6 15.1 21.8 21.2</cell><cell>1.28 15.9 23.7 26.3 29.0 30.5 30.2 33.7</cell><cell>4.98 25.5 28.5 31.1 33.7 34.9 36.6 39.2</cell><cell/></row><row><cell>Frâ†’En 14</cell><cell cols=\"2\">BLEU-sb test</cell><cell>64</cell><cell>2.29 2.99 3.90 3.60 21.2 15.5 22.4 21.9</cell><cell>1.50 16.3 24.4 27.0 30.0 31.6 31.4 35.6</cell><cell>5.30 26.2 29.5 32.2 35.1 36.4 38.3 41.4</cell><cell/></row><row><cell>Enâ†’Fr 14</cell><cell cols=\"2\">BLEU-mb test</cell><cell>45.6 64</cell><cell>1.74 2.16 2.73 2.15 15.1 8.82 12.0 25.2</cell><cell>0.49 8.00 14.8 15.9 20.3 23.3 24.9 28.3</cell><cell>4.08 14.5 19.3 21.5 24.9 27.3 29.5 32.6</cell><cell/></row><row><cell>Enâ†’Fr 14</cell><cell cols=\"2\">BLEU-sb test</cell><cell>45.9 64</cell><cell>2.44 2.75 3.54 2.82 19.3 11.4 15.3 31.3</cell><cell>0.81 10.0 18.2 19.3 24.7 28.3 30.1 34.1</cell><cell>5.31 18.0 23.6 26.1 30.3 33.3 35.5 39.9</cell><cell/></row><row><cell>Deâ†’En</cell><cell cols=\"2\">BLEU-mb test</cell><cell>40.2 64</cell><cell>2.06 2.87 3.41 3.63 21.5 17.3 23.0 27.2</cell><cell>0.83 16.2 22.5 24.7 28.2 30.7 33.0 30.4</cell><cell>3.25 22.7 26.2 29.2 32.7 34.8 37.3 40.6</cell><cell/></row><row><cell>Deâ†’En</cell><cell cols=\"2\">BLEU-sb test</cell><cell>64</cell><cell>2.39 3.27 3.85 4.04 22.5 18.2 24.4 28.6</cell><cell>0.93 17.1 23.4 25.8 29.2 31.9 34.5 32.1</cell><cell>3.60 23.8 27.5 30.5 34.1 36.5 39.1 43.0</cell><cell/></row><row><cell>Enâ†’De</cell><cell cols=\"2\">BLEU-mb test</cell><cell>41.2 64</cell><cell>1.70 2.27 2.31 2.43 12.9 8.66 10.4 24.6</cell><cell>0.50 7.00 12.9 13.1 18.3 20.9 22.5 26.2</cell><cell>3.42 12.3 15.4 17.1 20.9 23.0 26.6 29.7</cell><cell/></row><row><cell>Enâ†’De</cell><cell cols=\"2\">BLEU-sb test</cell><cell>41.2 64</cell><cell>2.09 2.65 2.75 2.92 13.7 9.36 11.0 25.3</cell><cell>0.54 7.40 13.4 13.4 18.8 21.7 23.3 27.3</cell><cell>3.78 12.9 16.1 17.7 21.7 24.1 27.7 30.9</cell><cell/></row><row><cell>Winograd</cell><cell>acc</cell><cell>test</cell><cell>93.8 7</cell><cell>66.3 72.9 74.7 76.9 82.4 85.7 87.9 88.3</cell><cell>63.4 68.5 72.9 76.9 82.4 84.6 86.1 89.7</cell><cell>63.4 67.4 73.6 76.9 84.3 85.4 82.4 88.6</cell><cell/></row><row><cell>Winogrande</cell><cell>acc</cell><cell>dev</cell><cell>84.6 50</cell><cell>52.0 52.1 57.4 58.7 62.3 64.5 67.9 70.2</cell><cell>51.3 53.0 58.3 59.1 61.7 65.8 66.9 73.2</cell><cell>51.3 52.6 57.5 59.1 62.6 67.4 70.0 77.7</cell><cell/></row><row><cell>PIQA</cell><cell>acc</cell><cell>dev</cell><cell>77.1 50</cell><cell>64.6 70.2 72.9 75.1 75.6 78.0 78.5 81.0</cell><cell>64.3 69.3 71.8 74.4 74.3 76.3 77.8 80.5</cell><cell>64.3 69.4 72.0 74.3 75.4 77.8 79.9 82.3</cell><cell>82.8</cell></row><row><cell cols=\"2\">ARC (Challenge) acc</cell><cell>test</cell><cell>78.5 50</cell><cell>26.6 29.5 31.8 35.5 38.0 41.4 43.7 51.4</cell><cell>25.5 30.2 31.6 36.4 38.4 41.5 43.1 53.2</cell><cell>25.5 28.4 32.3 36.7 39.5 43.7 44.8 51.5</cell><cell/></row><row><cell>ARC (Easy)</cell><cell>acc</cell><cell>test</cell><cell>92.0 50</cell><cell>43.6 46.5 53.0 53.8 58.2 60.2 63.8 68.8</cell><cell>42.7 48.2 54.6 55.9 60.3 62.6 66.8 71.2</cell><cell>42.7 51.0 58.1 59.1 62.1 65.8 69.1 70.1</cell><cell/></row><row><cell>OpenBookQA</cell><cell>acc</cell><cell>test</cell><cell>87.2 100</cell><cell>35.6 43.2 45.2 46.8 53.0 50.4 55.6 57.6</cell><cell>37.0 39.8 46.2 46.4 53.4 53.0 55.8 58.8</cell><cell>37.0 43.6 48.0 50.6 55.6 55.2 60.8 65.4</cell><cell/></row><row><cell>Quac</cell><cell>f1</cell><cell>dev</cell><cell>74.4 5</cell><cell>21.2 26.8 31.0 30.1 34.7 36.1 38.4 41.5</cell><cell>21.1 26.9 31.9 32.3 37.4 39.0 40.6 43.4</cell><cell>21.6 27.6 32.9 34.2 38.2 39.9 40.9 44.3</cell><cell/></row><row><cell>RACE-h</cell><cell>acc</cell><cell>test</cell><cell>90.0 10</cell><cell>35.2 37.9 40.1 40.9 42.4 44.1 44.6 45.5</cell><cell>34.3 37.7 40.0 42.0 43.8 44.3 44.6 45.9</cell><cell>34.3 37.0 40.4 41.4 42.3 44.7 45.1 46.8</cell><cell/></row><row><cell>RACE-m</cell><cell>acc</cell><cell>test</cell><cell>93.1 10</cell><cell>42.1 47.2 52.1 52.3 54.7 54.4 56.7 58.4</cell><cell>42.3 47.3 51.7 55.2 56.1 54.7 56.9 57.4</cell><cell>42.3 47.0 52.7 53.0 55.6 55.4 58.1 58.1</cell><cell/></row><row><cell>SQuADv2</cell><cell>em</cell><cell>dev</cell><cell>90.7 16</cell><cell>22.6 32.8 33.9 43.1 43.6 45.4 49.0 52.6</cell><cell>25.1 37.5 37.9 47.9 47.9 51.1 56.0 60.1</cell><cell>27.5 40.5 39.2 53.5 50.0 56.6 62.6 64.9</cell><cell/></row><row><cell>SQuADv2</cell><cell>f1</cell><cell>dev</cell><cell>93.0 16</cell><cell>28.3 40.2 41.4 50.3 51.0 52.7 56.3 59.5</cell><cell>30.1 43.6 44.1 54.0 54.1 57.1 61.8 65.4</cell><cell>32.1 45.5 44.9 58.7 55.9 62.1 67.7 69.8</cell><cell/></row><row><cell>CoQA</cell><cell>f1</cell><cell>dev</cell><cell>90.7 5</cell><cell>34.5 55.0 61.8 65.3 71.1 72.8 76.3 81.5</cell><cell>30.6 52.1 61.6 66.1 71.8 75.1 77.9 84.0</cell><cell>31.1 52.0 62.7 66.8 73.2 77.3 79.9 85.0</cell><cell/></row><row><cell>DROP</cell><cell>f1</cell><cell>dev</cell><cell>89.1 20</cell><cell>9.40 13.6 14.4 16.4 19.7 17.0 24.0 23.6</cell><cell>11.7 18.1 20.9 23.0 26.4 27.3 29.2 34.3</cell><cell>12.9 18.7 24.0 25.6 29.7 29.7 32.3 36.5</cell><cell/></row><row><cell>BoolQ</cell><cell>acc</cell><cell>dev</cell><cell>91.0 32</cell><cell>49.7 60.3 58.9 62.4 67.1 65.4 66.2 60.5</cell><cell>52.6 61.7 60.4 63.7 68.4 68.7 69.0 76.7</cell><cell>43.1 60.6 62.0 64.1 70.3 70.0 70.2 77.5</cell><cell>76.4</cell></row><row><cell>CB</cell><cell>acc</cell><cell>dev</cell><cell>96.9 32</cell><cell>0.00 32.1 8.93 19.6 19.6 28.6 19.6 46.4</cell><cell>55.4 53.6 53.6 48.2 57.1 33.9 55.4 64.3</cell><cell>42.9 58.9 53.6 69.6 67.9 60.7 66.1 82.1</cell><cell>75.6</cell></row><row><cell>CB</cell><cell>f1</cell><cell>dev</cell><cell>93.9 32</cell><cell>0.00 29.3 11.4 17.4 22.4 25.1 20.3 42.8</cell><cell>60.1 39.8 45.6 37.5 45.7 28.5 44.6 52.5</cell><cell>26.1 40.4 32.6 48.3 45.7 44.6 46.0 57.2</cell><cell>52.0</cell></row><row><cell>Copa</cell><cell>acc</cell><cell>dev</cell><cell>94.8 32</cell><cell>66</cell><cell/><cell/><cell/></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_32\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>Table H.1: Scores for every task, setting and model that we investigate in this paper.</figDesc><table><row><cell/><cell/><cell/><cell/><cell/><cell/><cell>9.90</cell></row><row><cell>2Dx</cell><cell>acc</cell><cell>n/a</cell><cell>50</cell><cell>2.20 2.25 2.65 2.10 2.55 5.80 6.15 19.8</cell><cell>1.35 2.35 3.35 2.35 4.75 9.15 11.0 27.4</cell><cell>1.35 2.90 2.70 2.85 4.25 6.10 7.05 29.2</cell></row><row><cell>1DC</cell><cell>acc</cell><cell>n/a</cell><cell>50</cell><cell>1.25 2.95 2.75 0.05 0.30 2.35 0.75 9.75</cell><cell>1.90 2.80 2.85 3.65 6.45 9.15 8.20 14.3</cell><cell>1.70 2.15 3.90 5.75 6.20 7.60 9.95 21.3</cell></row><row><cell cols=\"2\">Cycled Letters acc</cell><cell>n/a</cell><cell>100</cell><cell>0.62 0.71 2.85 0.00 0.63 1.35 2.58 3.66</cell><cell>1.67 4.36 5.68 6.46 6.25 9.41 15.1 21.7</cell><cell>4.63 9.27 10.7 14.5 16.7 21.9 27.7 37.9</cell></row><row><cell>Anagrams 1</cell><cell>acc</cell><cell>n/a</cell><cell>100</cell><cell>0.10 0.14 0.40 0.00 0.27 0.69 1.16 2.28</cell><cell>0.21 0.61 1.12 1.27 1.60 2.72 3.72 8.62</cell><cell>0.50 1.27 2.13 3.05 3.81 5.49 8.38 15.1</cell></row><row><cell>Anagrams 2</cell><cell>acc</cell><cell>n/a</cell><cell>100</cell><cell>0.81 1.21 2.69 0.01 1.71 3.75 4.53 8.91</cell><cell>1.19 2.62 4.70 4.77 6.97 10.2 14.6 25.9</cell><cell>1.94 4.80 7.59 9.87 12.6 18.9 25.6 39.7</cell></row><row><cell cols=\"2\">Symbol Insertion acc</cell><cell>n/a</cell><cell>100</cell><cell>0.00 0.00 0.10 0.00 0.05 0.42 0.89 8.26</cell><cell>0.03 0.05 0.57 1.18 1.67 3.46 6.62 45.4</cell><cell>0.11 0.28 2.19 4.18 6.61 11.0 27.3 67.2</cell></row><row><cell cols=\"2\">Reversed Words acc</cell><cell>n/a</cell><cell>100</cell><cell>0.00 0.01 0.01 0.01 0.02 0.03 0.03 0.09</cell><cell>0.02 0.01 0.01 0.00 0.05 0.07 0.11 0.48</cell><cell>0.00 0.05 0.00 0.17 0.24 0.30 0.42 0.44</cell></row><row><cell cols=\"2\">SAT Analogies acc</cell><cell>n/a</cell><cell>20</cell><cell>35.6 39.0 45.2 44.1 50.0 49.2 52.7 53.7</cell><cell>30.5 41.2 43.1 46.5 55.1 54.3 53.5 59.1</cell><cell>30.5 40.4 42.8 40.6 48.4 51.9 53.5 65.2</cell></row></table></figure>\n",
       " <note n=\"1\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">In the context of language models this has sometimes been called \"zero-shot transfer\", but this term is potentially ambiguous: the method is \"zero-shot\" in the sense that no gradient updates are performed, but it often involves providing inference-time demonstrations to the model, so is not truly learning from zero examples. To avoid this confusion, we use the term \"meta-learning\" to capture the inner-loop / outer-loop structure of the general method, and the term \"in context-learning\" to refer to the inner loop of meta-learning. We further specialize the description to \"zero-shot\", \"one-shot\", or \"few-shot\" depending on how many demonstrations are provided at inference time. These terms are intended to remain agnostic on the question of whether the model learns new tasks from scratch at inference time or simply recognizes patterns seen during training -this is an important issue which we discuss later in the paper, but \"meta-learning\" is intended to encompass both possibilities, and simply describes the inner-outer loop structure.</note>\n",
       " <note n=\"2\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">https://commoncrawl.org/the-data/</note>\n",
       " <note n=\"3\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">This task is also relevant to the potential misuse of language models discussed in Section 6.1.4  We wanted to identify how good an average person on the internet is at detecting language model outputs, so we focused on participants drawn from the general US population. See Appendix E for details.</note>\n",
       " <note n=\"5\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">We use a two-sample Student's T-Test to test for significant difference between the means of the participant accuracies of each model and the control model and report the normalized difference in the means (as the t-statistic) and the p-value.6  If a model consistently produces texts that are more impressive than human articles, it is possible that human performance on this task would drop below 50%. Indeed, many individual participants scored below 50% on this task.7 Additional non-news samples can be found in Appendix F.</note>\n",
       " <note n=\"8\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">Evaluating fairness, bias, and representation in language models is a rapidly-developing area with a large body of prior work. See, for example, [HZJ + 19, NBR20, SCNP19].</note>\n",
       " <note n=\"9\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">We only used male and female pronouns. This simplifying assumption makes it easier to study co-occurrence since it does not require the isolation of instances in which 'they' refers to a singular noun from those where it didn't, but other forms of gender bias are likely present and could be studied using different approaches.</note>\n",
       " <note n=\"10\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.HashingTF</note>\n",
       " <note n=\"11\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">https://github.com/openai/gpt-3/blob/master/overlap_frequency.md</note>\n",
       " </body>\n",
       " <back>\n",
       " <div type=\"acknowledgement\">\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Acknowledgements</head><p>The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks to Jakub Pachocki and Szymon Sidor for suggesting tasks, and Greg Brockman, Michael Petrov, Brooke Chan, and Chelsea Voss for helping run evaluations on OpenAI's infrastructure. Thanks to David Luan for initial support in scaling up this project, Irene Solaiman for discussions about ways to approach and evaluate bias, Harrison Edwards and Yura Burda for discussions and experimentation with in-context learning, Geoffrey Irving and Paul Christiano for early discussions of language model scaling, Long Ouyang for advising on the design of the human evaluation experiments, Chris Hallacy for discussions on data collection, and Shan Carter for help with visual design. Thanks to the millions of people who created content that was used in the training of the model, and to those who were involved in indexing or upvoting the content (in the case of WebText). Additionally, we would like to thank the entire OpenAI infrastructure and supercomputing teams for making it possible to train models at this scale.</p></div>\n",
       " </div>\n",
       " <div type=\"annex\">\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>The sun was all we had. Now, in the shade If I am to walk I must have All is changed. The mind must dwell on those Each step taken slowly and alone White fields, that to its eyes were always old; To have it ready made Those ancient gleams, convoluting The way they lay among the huge roots, And I must think in lines of grey</p><p>The great dark tomes of reverie, To have dim thoughts to be my guide</p><p>The plumed labyrinth of the sea. Must look on blue and green And never let my eye forget When our own breath gonne faint and fade That color is my friend Up the long stairway of the dead And purple must surround me too</p><p>We shall remember well The blindness of the light, the whiteness The yellow of the sun is no more</p><p>Of that white land. The footsteps, and Intrusive than the bluish snow</p><p>The grasses in that light, and in the shade That falls on all of us. I must have</p><p>The menacing life. Grey thoughts and blue thoughts walk with me If I am to go away at all.</p><p>We played, too, once, in the warmest rooms.</p><p>To one content, there is one content --------Generated Poem 2 --------For all mankind. The forest glades Are all the more for being in fable, There is a way in the world.</p><p>And wood and town in legend shadowed-That way a man may live in a small town Once more to return. The one content His lifetime and see no man come, Has in it every happiness, more brave Never the stir of men about a market, For being purposeless; and so Never the bobbing of covered carts, Those island gardens, deep in the mind, Never the creak passed on the stair Make of remoteness familiarity Closing some other life; an old man And of the unspoken name a friend. Can prosper in that town, can prosper When his store is seen to close, his wife --------Generated Poem 4 --------Fades in his house, and the world shouts in vain That no one comes to hear him Nobody will come to this place. It is a road that leads nowhere. Where his small bell tolls.</p><p>The solitude is deep. The mountains are high. In that small town But they are desolate, and they turn the traveler's face Everything is known.</p><p>Towards the North. All the sounds of the world are far away. The ways of the young men are known, When the wind rises above the trees, And the ways of the young women</p><p>The boughs bow to the ground. As they walk to the Spring;</p><p>Even the birds that inhabit the tangle of weeds The rumors are all known, That is the roadside cover, are silent. One listens, The quarrels, the hurt, the envy But hears no roar of the forest. One is alone. Of the men and the women.</p><p>One will be taken. Till a day comes One will be taken.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>That way out of the world</head><p>There is no utterance, there is no conversation, That leads to a wood.</p><p>But one is uneasy all the same.... Young, the young man There is a thin blue mist, Walks, a lover now, A darkness rising like smoke, Knows the streets as they are and is of them.</p><p>And within that darkness A possession of the heart. Comes to the wood.</p><p>One will be taken.... It was here, and it will be here again-Here, under this sky empty and full of light. There, in the wood, among trees, He sees shadows on the way, hears voices, hears the wind and the rustling of leaves; Through an open glade He sees a shape and the shape hears: It waits as he waits, Waits as the shadows wait, As the voices wait; Shadows on the way, voices in the wind. </p></div> </div>\n",
       " <div type=\"references\">\n",
       " <listBibl>\n",
       " <biblStruct xml:id=\"b0\">\n",
       " <monogr>\n",
       " <title/>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tom</forename><surname>Brown</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ben</forename><surname>Mann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Prafulla</forename><surname>Dhariwal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dario</forename><surname>Amodei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nick</forename><surname>Ryder</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Daniel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Ziegler</surname></persName>\n",
       " </author>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " <note>and Jeffrey Wu implemented the large-scale models, training infrastructure, and model-parallel strategies</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b1\">\n",
       " <monogr>\n",
       " <title/>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tom</forename><surname>Brown</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dario</forename><surname>Amodei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ben</forename><surname>Mann</surname></persName>\n",
       " </author>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " <note>and Nick Ryder conducted pre-training experiments</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b2\">\n",
       " <monogr>\n",
       " <title/>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ben</forename><surname>Mann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alec</forename><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b3\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Tom Henighan, and Girish Sastry implemented the downstream tasks and the software framework for supporting them, including creation of synthetic tasks</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Melanie</forename><surname>Subbiah</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ben</forename><surname>Mann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dario</forename><surname>Amodei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jared</forename><surname>Kaplan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sam</forename><surname>Mccandlish</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tom</forename><surname>Brown</surname></persName>\n",
       " </author>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b4\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">McCandlish initially predicted that a giant language model should show continued gains, and applied scaling laws to help predict and guide model and data scaling decisions for the research</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jared</forename><surname>Kaplan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sam</forename></persName>\n",
       " </author>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b5\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Ben Mann implemented sampling without replacement during training</title>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b6\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Alec Radford originally demonstrated few-shot learning occurs in language models</title>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b7\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">McCandlish showed that larger models learn more quickly in-context, and systematically studied in-context learning curves, task prompting, and evaluation methods</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jared</forename><surname>Kaplan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sam</forename></persName>\n",
       " </author>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b8\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Prafulla Dhariwal implemented an early version of the codebase, and developed the memory optimizations for fully half-precision training</title>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b9\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Rewon Child and Mark Chen developed an early version of our model-parallel strategy</title>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b10\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Rewon Child and Scott Gray contributed the sparse transformer</title>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b11\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Aditya Ramesh experimented with loss scaling strategies for pretraining</title>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b12\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Melanie Subbiah and Arvind Neelakantan implemented, experimented with, and tested beam search</title>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b13\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Pranav Shyam worked on SuperGLUE and assisted with connections to few-shot learning and meta-learning literature</title>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b14\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Sandhini Agarwal conducted the fairness and representation analysis</title>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b15\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Girish Sastry and Amanda Askell conducted the human evaluations of the model</title>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b16\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Ariel Herbert-Voss conducted the threat analysis of malicious use</title>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b17\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Mateusz Litwin, and Christopher Berner optimized OpenAI's clusters to run the largest models efficiently</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Benjamin</forename><surname>Chess</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Clemens</forename><surname>Winter</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Eric</forename><surname>Sigler</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><surname>Hesse</surname></persName>\n",
       " </author>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b18\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Scott Gray developed fast GPU kernels used during training</title>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b19\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Jack Clark led the analysis of ethical impacts -fairness and representation, human assessments of the model, and broader impacts analysis, and advised Gretchen</title>\n",
       " <imprint>\n",
       " <pubPlace>Amanda, Girish, Sandhini</pubPlace>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>and Ariel on their work</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b20\">\n",
       " <monogr>\n",
       " <title/>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dario</forename><surname>Amodei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alec</forename><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tom</forename><surname>Brown</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sam</forename><surname>Mccandlish</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nick</forename><surname>Ryder</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jared</forename><surname>Kaplan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sandhini</forename><surname>Agarwal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Amanda</forename><surname>Askell</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Girish</forename><surname>Sastry</surname></persName>\n",
       " </author>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " <note>and Jack Clark wrote the paper</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b21\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Sam McCandlish led the analysis of model scaling, and advised Tom Henighan and Jared Kaplan on their work</title>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b22\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Alec Radford advised the project from an NLP perspective, suggested tasks, put the results in context, and demonstrated the benefit of weight decay for training</title>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b23\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Ilya Sutskever was an early advocate for scaling large generative likelihood models, and advised Pranav, Prafulla, Rewon, Alec, and Aditya on their work</title>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b24\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Learning to learn by gradient descent by gradient descent</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Marcin</forename><surname>Andrychowicz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Misha</forename><surname>Denil</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sergio</forename><surname>Gomez</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Matthew</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">David</forename><surname>Hoffman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tom</forename><surname>Pfau</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Brendan</forename><surname>Schaul</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nando De</forename><surname>Shillingford</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Freitas</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in neural information processing systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " <biblScope from=\"3981\" to=\"3989\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>ADG + 16</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b25\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Tr-mt (ensemble)</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><forename type=\"middle\">I</forename><surname>Wechat</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019-12\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b26\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Massively multilingual neural machine translation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Roee</forename><surname>Aharoni</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Melvin</forename><surname>Johnson</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Orhan</forename><surname>Firat</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>\n",
       " <meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " <biblScope unit=\"volume\">1</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b27\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Language (technology) is power: A critical survey of \"bias</title>\n",
       " <author>\n",
       " <persName><surname>Su Lin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Solon</forename><surname>Blodgett</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hal</forename><surname>Barocas</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Iii</forename><surname>DaumÃ©</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hanna</forename><surname>Wallach</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2005.14050</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">in nlp. arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b28\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Semantic parsing on freebase from question-answer pairs</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jonathan</forename><surname>Berant</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrew</forename><surname>Chou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Roy</forename><surname>Frostig</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Percy</forename><surname>Liang</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 2013 conference on empirical methods in natural language processing</title>\n",
       " <meeting>the 2013 conference on empirical methods in natural language processing</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2013\"/>\n",
       " <biblScope from=\"1533\" to=\"1544\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b29\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">The fifth PASCAL recognizing textual entailment challenge</title>\n",
       " <author>\n",
       " <persName><surname>Bdd + 09] Luisa</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ido</forename><surname>Bentivogli</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hoa</forename><forename type=\"middle\">Trang</forename><surname>Dagan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Danilo</forename><surname>Dang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bernardo</forename><surname>Giampiccolo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Magnini</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2009\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b30\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Sentiwordnet 3.0: an enhanced lexical resource for sentiment analysis and opinion mining</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Stefano</forename><surname>Baccianella</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrea</forename><surname>Esuli</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Fabrizio</forename><surname>Sebastiani</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Lrec</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2010\"/>\n",
       " <biblScope unit=\"volume\">10</biblScope>\n",
       " <biblScope from=\"2200\" to=\"2204\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b31\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">The second PASCAL recognising textual entailment challenge</title>\n",
       " <author>\n",
       " <persName><surname>Bhdd + 06] Roy Bar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ido</forename><surname>Haim</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bill</forename><surname>Dagan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lisa</forename><surname>Dolan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Danilo</forename><surname>Ferro</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bernardo</forename><surname>Giampiccolo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Idan</forename><surname>Magnini</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Szpektor</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2006\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b32\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yonatan</forename><surname>Bisk</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ari</forename><surname>Holtzman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jesse</forename><surname>Thomason</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jacob</forename><surname>Andreas</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yoshua</forename><surname>Bengio</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Joyce</forename><surname>Chai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mirella</forename><surname>Lapata</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Angeliki</forename><surname>Lazaridou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jonathan</forename><surname>May</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2004.10151</idno>\n",
       " <title level=\"m\">Aleksandr Nisnevich, et al. Experience grounds language</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " <note>BHT + 20</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b33\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Estimating or propagating gradients through stochastic neurons for conditional computation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yoshua</forename><surname>Bengio</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nicholas</forename><surname>LÃ©onard</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Aaron</forename><forename type=\"middle\">C</forename><surname>Courville</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Arxiv</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2013\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b34\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><surname>+ 19] Yonatan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rowan</forename><surname>Bisk</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Zellers</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jianfeng</forename><surname>Ronan Le Bras</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yejin</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Choi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Piqa</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1911.11641</idno>\n",
       " <title level=\"m\">Reasoning about physical commonsense in natural language</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b35\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Multitask learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rich</forename><surname>Caruana</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Machine learning</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">28</biblScope>\n",
       " <biblScope unit=\"issue\">1</biblScope>\n",
       " <date type=\"published\" when=\"1997\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b36\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Acquiring a single new word</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Susan</forename><surname>Carey</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Elsa</forename><surname>Bartlett</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the Stanford Child Language Conference</title>\n",
       " <meeting>the Stanford Child Language Conference</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"1978\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b37\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Peter</forename><surname>Clark</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Isaac</forename><surname>Cowhey</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oren</forename><surname>Etzioni</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tushar</forename><surname>Khot</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ashish</forename><surname>Sabharwal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Carissa</forename><surname>Schoenick</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oyvind</forename><surname>Tafjord</surname></persName>\n",
       " </author>\n",
       " <idno>abs/1803.05457</idno>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">ArXiv</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b38\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Generating long sequences with sparse transformers</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rewon</forename><surname>Child</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Scott</forename><surname>Gray</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alec</forename><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ilya</forename><surname>Sutskever</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b39\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><surname>Chi + 18] Eunsol</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">He</forename><surname>Choi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mohit</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mark</forename><surname>Iyyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wen-Tau</forename><surname>Yatskar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yejin</forename><surname>Yih</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Percy</forename><surname>Choi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luke</forename><surname>Liang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Zettlemoyer</surname></persName>\n",
       " </author>\n",
       " <title level=\"m\">Quac : Question answering in context. Arxiv</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b40\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">BoolQ: Exploring the surprising difficulty of natural yes/no questions</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><surname>Clark</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ming-Wei</forename><surname>Chang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tom</forename><surname>Kwiatkowski</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Michael</forename><surname>Collins</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kristina</forename><surname>Toutanova</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1905.10044</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b41\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yen-Chun</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Linjie</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Licheng</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ahmed</forename><forename type=\"middle\">El</forename><surname>Kholy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Faisal</forename><surname>Ahmed</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhe</forename><surname>Gan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yu</forename><surname>Cheng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jingjing</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1909.11740</idno>\n",
       " <title level=\"m\">Uniter: Learning universal image-text representations</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " <note>+ 19</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b42\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">The trouble with bias</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kate</forename><surname>Crawford</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b43\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jacob</forename><surname>Devlin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ming-Wei</forename><surname>Chang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kristina</forename><forename type=\"middle\">Toutanova</forename><surname>Bert</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1810.04805</idno>\n",
       " <title level=\"m\">Pre-training of deep bidirectional transformers for language understanding</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b44\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">The PASCAL recognising textual entailment challenge</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oren</forename><surname>Ido Dagan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bernardo</forename><surname>Glickman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Magnini</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising textual entailment</title>\n",
       " <imprint>\n",
       " <publisher>Springer</publisher>\n",
       " <date type=\"published\" when=\"2006\"/>\n",
       " <biblScope from=\"177\" to=\"190\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b45\">\n",
       " <monogr>\n",
       " <title/>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Stephan</forename><surname>+ 18] Mostafa Dehghani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oriol</forename><surname>Gouws</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jakob</forename><surname>Vinyals</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lukasz</forename><surname>Uszkoreit</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Kaiser</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " <respStmt>\n",
       " <orgName>Universal transformers. Arxiv</orgName>\n",
       " </respStmt>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b46\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Edinburgh's phrase-based machine translation systems for wmt-14</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nadir</forename><surname>Durrani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Barry</forename><surname>Haddow</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Philipp</forename><surname>Koehn</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kenneth</forename><surname>Heafield</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>\n",
       " <meeting>the Ninth Workshop on Statistical Machine Translation</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " <biblScope from=\"97\" to=\"104\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b47\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Semi-supervised sequence learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Andrew</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Quoc</forename><forename type=\"middle\">V</forename><surname>Dai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Le</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in neural information processing systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b48\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">The CommitmentBank: Investigating projection in naturally occurring discourse</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Marie-Catherine De</forename><surname>Marneffe</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mandy</forename><surname>Simons</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Judith</forename><surname>Tonhauser</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"https://github.com/mcdm/CommitmentBank/\"/>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>To appear in proceedings of Sinn und Bedeutung 23. Data can be</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b49\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Rl 2 : Fast reinforcement learning via slow reinforcement learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yan</forename><surname>Duan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">John</forename><surname>Schulman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xi</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Peter</forename><forename type=\"middle\">L</forename><surname>Bartlett</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ilya</forename><surname>Sutskever</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pieter</forename><surname>Abbeel</surname></persName>\n",
       " </author>\n",
       " <idno>abs/1611.02779</idno>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">ArXiv</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>DSC + 16</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b50\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yizhong</forename><surname>+ 19] Dheeru Dua</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pradeep</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Gabriel</forename><surname>Dasigi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sameer</forename><surname>Stanovsky</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matt</forename><surname>Singh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Gardner</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1903.00161</idno>\n",
       " <title level=\"m\">Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b51\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Transformer-xl: Attentive language models beyond a fixed-length context</title>\n",
       " <author>\n",
       " <persName><surname>+ 19] Zihang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhilin</forename><surname>Dai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yiming</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jaime</forename><forename type=\"middle\">G</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Quoc</forename><forename type=\"middle\">V</forename><surname>Carbonell</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ruslan</forename><surname>Le</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Salakhutdinov</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Arxiv</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b52\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Understanding back-translation at scale</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sergey</forename><surname>Edunov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Myle</forename><surname>Ott</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Michael</forename><surname>Auli</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">David</forename><surname>Grangier</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1808.09381</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b53\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Model-agnostic meta-learning for fast adaptation of deep networks</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chelsea</forename><surname>Finn</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pieter</forename><surname>Abbeel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sergey</forename><surname>Levine</surname></persName>\n",
       " </author>\n",
       " <idno>abs/1703.03400</idno>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">ArXiv</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b54\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">A natural logic inference system</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yaroslav</forename><surname>Fyodorov</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2000\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b55\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hila</forename><surname>Gonen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yoav</forename><surname>Goldberg</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1903.03862</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b56\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kenton</forename><surname>+ 20] Kelvin Guu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zora</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Panupong</forename><surname>Tung</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ming-Wei</forename><surname>Pasupat</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Chang</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2002.08909</idno>\n",
       " <title level=\"m\">Realm: Retrievalaugmented language model pre-training</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b57\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">The third PASCAL recognizing textual entailment challenge</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Danilo</forename><surname>Giampiccolo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bernardo</forename><surname>Magnini</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ido</forename><surname>Dagan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bill</forename><surname>Dolan</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing</title>\n",
       " <meeting>the ACL-PASCAL workshop on textual entailment and paraphrasing</meeting>\n",
       " <imprint>\n",
       " <publisher>Association for Computational Linguistics</publisher>\n",
       " <date type=\"published\" when=\"2007\"/>\n",
       " <biblScope from=\"1\" to=\"9\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b58\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Adaptive computation time for recurrent neural networks</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Graves ; Suchin Gururangan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Swabha</forename><surname>Swayamdipta</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Omer</forename><surname>Levy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Roy</forename><surname>Schwartz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Samuel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noah A</forename><surname>Bowman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Smith</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1803.02324</idno>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Annotation artifacts in natural language inference data</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " <note>GSL + 18</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b59\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Gltr: Statistical detection and visualization of generated text</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sebastian</forename><surname>Gehrmann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hendrik</forename><surname>Strobelt</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alexander</forename><forename type=\"middle\">M</forename><surname>Rush</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1906.04043</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b60\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">+ 18] Jiatao</forename><surname>Gu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yong</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yun</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kyunghyun</forename><surname>Cho</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">O</forename><forename type=\"middle\">K</forename><surname>Victor</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Li</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1808.08437</idno>\n",
       " <title level=\"m\">Meta-learning for low-resource neural machine translation</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b61\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Ai and efficiency</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Daniel</forename><surname>Hernandez</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tom</forename><surname>Brown</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020-05\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b62\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">The curious case of neural text degeneration</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ari</forename><surname>Holtzman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jan</forename><surname>Buys</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Maxwell</forename><surname>Forbes</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yejin</forename><surname>Choi</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"1904\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b63\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Pretrained transformers improve out of distribution robustness</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dan</forename><surname>Hendrycks</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xiaoyuan</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Eric</forename><surname>Wallace</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Adam</forename><surname>Dziedzic</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rishabh</forename><surname>Krishnan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dawn</forename><surname>Song</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2004.06100</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b64\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sharan</forename><surname>Hna + 17] Joel Hestness</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Newsha</forename><surname>Narang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Gregory</forename><surname>Ardalani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Heewoo</forename><surname>Diamos</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hassan</forename><surname>Jun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Md. Mostofa Ali</forename><surname>Kianinejad</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yang</forename><surname>Patwary</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yanqi</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1712.00409</idno>\n",
       " <title level=\"m\">Deep learning scaling is predictable, empirically</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b65\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Universal language model fine-tuning for text classification</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeremy</forename><surname>Howard</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sebastian</forename><surname>Ruder</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1801.06146</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b66\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Distilling the knowledge in a neural network</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Geoffrey</forename><surname>Hinton</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oriol</forename><surname>Vinyals</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeff</forename><surname>Dean</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1503.02531</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b67\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Learning to Learn Using Gradient Descent</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sepp</forename><surname>Hochreiter</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Steven</forename><surname>Younger</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Peter R Conwell</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">International Conference on Artificial Neural Networks</title>\n",
       " <imprint>\n",
       " <publisher>Springer</publisher>\n",
       " <date type=\"published\" when=\"2001\"/>\n",
       " <biblScope from=\"87\" to=\"94\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b68\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Reducing sentiment bias in language models via counterfactual evaluation</title>\n",
       " <author>\n",
       " <persName><surname>+ 19] Po-Sen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Huan</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ray</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Robert</forename><surname>Jiang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Johannes</forename><surname>Stanforth</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jack</forename><surname>Welbl</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Vishal</forename><surname>Rae</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dani</forename><surname>Maini</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pushmeet</forename><surname>Yogatama</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Kohli</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1911.03064</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b69\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A neural network for factoid question answering over paragraphs</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mohit</forename><surname>Iyyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jordan</forename><surname>Boyd-Graber</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Leonardo</forename><surname>Claudino</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Richard</forename><surname>Socher</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hal</forename><surname>DaumÃ©</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Iii</forename></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Empirical Methods in Natural Language Processing</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>IBGC + 14</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b70\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Automatic detection of generated text is easiest when humans are fooled</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Daphne</forename><surname>Ippolito</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Daniel</forename><surname>Duckworth</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chris</forename><surname>Callison-Burch</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Douglas</forename><surname>Eck</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1911.00650</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b71\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mandar</forename><surname>Joshi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Eunsol</forename><surname>Choi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Daniel</forename><forename type=\"middle\">S</forename><surname>Weld</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1705.03551</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b72\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Numeric transformer -albert</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zheng</forename><surname>Junyuan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Gamma</forename><surname>Lab</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nyc</forename></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020-03\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b73\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Exploring the limits of language modeling</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rafal</forename><surname>Jozefowicz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oriol</forename><surname>Vinyals</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mike</forename><surname>Schuster</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noam</forename><surname>Shazeer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yonghui</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1602.02410</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " <note>JVS + 16</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b74\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yichun</forename><surname>+ 19] Xiaoqi Jiao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lifeng</forename><surname>Yin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xin</forename><surname>Shang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xiao</forename><surname>Jiang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Linlin</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Fang</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Qun</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1909.10351</idno>\n",
       " <title level=\"m\">TinyBERT: Distilling BERT for natural language understanding</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b75\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ying</forename><surname>Ju</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Fubang</forename><surname>Zhao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Shijie</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bowen</forename><surname>Zheng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xuefeng</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yunfeng</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1909.10772</idno>\n",
       " <title level=\"m\">on conversational question answering</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b76\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Looking beyond the surface: A challenge set for reading comprehension over multiple sentences</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Daniel</forename><surname>Khashabi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Snigdha</forename><surname>Chaturvedi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Michael</forename><surname>Roth</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Shyam</forename><surname>Upadhyay</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dan</forename><surname>Roth</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL)</title>\n",
       " <meeting>North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b77\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tushar</forename><surname>Kks + 20] Daniel Khashabi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ashish</forename><surname>Khot</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oyvind</forename><surname>Sabharwal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Peter</forename><surname>Tafjord</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hannaneh</forename><surname>Clark</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Hajishirzi</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2005.00700</idno>\n",
       " <title level=\"m\">Unifiedqa: Crossing format boundaries with a single qa system</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b78\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">All the news that's fit to fabricate: Ai-generated text as a tool of media misinformation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sarah</forename><forename type=\"middle\">E</forename><surname>Kreps</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Miles</forename><surname>Mccain</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Miles</forename><surname>Brundage</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b79\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Scaling laws for neural language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">+ 20] Jared</forename><surname>Kaplan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sam</forename><surname>Mccandlish</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tom</forename><surname>Henighan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tom</forename><forename type=\"middle\">B</forename><surname>Brown</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Benjamin</forename><surname>Chess</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rewon</forename><surname>Child</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Scott</forename><surname>Gray</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alec</forename><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeffrey</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dario</forename><surname>Amodei</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b80\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jennimaria</forename><surname>Kpr + 19] Tom Kwiatkowski</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Olivia</forename><surname>Palomaki</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Michael</forename><surname>Redfield</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ankur</forename><surname>Collins</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chris</forename><surname>Parikh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Danielle</forename><surname>Alberti</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Illia</forename><surname>Epstein</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matthew</forename><surname>Polosukhin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jacob</forename><surname>Kelcey</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kenton</forename><surname>Devlin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kristina</forename><forename type=\"middle\">N</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Llion</forename><surname>Toutanova</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ming-Wei</forename><surname>Jones</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrew</forename><surname>Chang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jakob</forename><surname>Dai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Quoc</forename><surname>Uszkoreit</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Slav</forename><surname>Le</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Petrov</surname></persName>\n",
       " </author>\n",
       " <title level=\"m\">Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b81\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Sequence-level knowledge distillation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yoon</forename><surname>Kim</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alexander</forename><forename type=\"middle\">M</forename><surname>Rush</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Arxiv</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b82\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Nltk: The natural language toolkit</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Edward</forename><surname>Loper</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Steven</forename><surname>Bird</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2002\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b83\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Guillaume</forename><surname>Lample</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alexis</forename><surname>Conneau</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1901.07291</idno>\n",
       " <title level=\"m\">Cross-lingual language model pretraining</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b84\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Piyush Sharma, and Radu Soricut. ALBERT: A lite BERT for self-supervised learning of language representations</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mingda</forename><surname>+ 19] Zhenzhong Lan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sebastian</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kevin</forename><surname>Goodman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Gimpel</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1909.11942</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b85\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><surname>+ 20] Xiaodong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hao</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pengcheng</forename><surname>Cheng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Weizhu</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yu</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hoifung</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jianfeng</forename><surname>Poon</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2004.08994</idno>\n",
       " <title level=\"m\">Adversarial training for large neural language models</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b86\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhongyang</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xiao</forename><surname>Ding</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ting</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1905.07504</idno>\n",
       " <title level=\"m\">Story ending prediction by transferable bert</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b87\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">The Winograd schema challenge</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hector</forename><surname>Levesque</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ernest</forename><surname>Davis</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Leora</forename><surname>Morgenstern</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2012\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b88\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Multilingual denoising pre-training for neural machine translation</title>\n",
       " <author>\n",
       " <persName><surname>Lgg + 20] Yinhan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jiatao</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Naman</forename><surname>Gu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xian</forename><surname>Goyal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sergey</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Marjan</forename><surname>Edunov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mike</forename><surname>Ghazvininejad</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luke</forename><surname>Lewis</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Zettlemoyer</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2001.08210</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b89\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Representation learning using multi-task deep neural networks for semantic classification and information retrieval</title>\n",
       " <author>\n",
       " <persName><surname>+ 15] Xiaodong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jianfeng</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xiaodong</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Li</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kevin</forename><surname>Deng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ye-Yi</forename><surname>Duh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Wang</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>\n",
       " <meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b90\">\n",
       " <monogr>\n",
       " <title/>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ilya</forename><surname>Loshchilov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Frank</forename><surname>Hutter</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1711.05101</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">Decoupled weight decay regularization. arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b91\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Improving multi-task deep neural networks via knowledge distillation for natural language understanding</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xiaodong</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pengcheng</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Weizhu</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jianfeng</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1904.09482</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b92\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xiaodong</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pengcheng</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Weizhu</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jianfeng</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1901.11504</idno>\n",
       " <title level=\"m\">Multi-task deep neural networks for natural language understanding</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b93\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">How can we accelerate progress towards human-like linguistic generalization?</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tal</forename><surname>Linzen</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2005.00955</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b94\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yinhan</forename><surname>Lewis</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Naman</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Marjan</forename><surname>Goyal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Abdelrahman</forename><surname>Ghazvininejad</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Omer</forename><surname>Mohamed</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ves</forename><surname>Levy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luke</forename><surname>Stoyanov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Zettlemoyer</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1910.13461</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " <pubPlace>Bart</pubPlace>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b95\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ke</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jitendra</forename><surname>Malik ; Yinhan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Myle</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Naman</forename><surname>Ott</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jingfei</forename><surname>Goyal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mandar</forename><surname>Du</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Danqi</forename><surname>Joshi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Omer</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mike</forename><surname>Levy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luke</forename><surname>Lewis</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Veselin</forename><surname>Zettlemoyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Stoyanov</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1703.00441</idno>\n",
       " <idno>arXiv:1907.11692</idno>\n",
       " <title level=\"m\">A robustly optimized BERT pretraining approach</title>\n",
       " <meeting><address><addrLine>RoBERTa</addrLine></address></meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " <note>Learning to optimize neural nets</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b96\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">+ 20] Patrick</forename><surname>Lewis</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ethan</forename><surname>Perez</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Aleksandra</forename><surname>Piktus</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Fabio</forename><surname>Petroni</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Vladimir</forename><surname>Karpukhin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Naman</forename><surname>Goyal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Heinrich</forename><surname>KÃ¼ttler</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mike</forename><surname>Lewis</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wen-Tau</forename><surname>Yih</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2005.11401</idno>\n",
       " <title level=\"m\">Tim RocktÃ¤schel, Sebastian Riedel, and Kiela Douwe. Retrieval-augmented generation for knowledge-intensive nlp tasks</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b97\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Peter</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mohammad</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Etienne</forename><surname>Saleh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ben</forename><surname>Pot</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ryan</forename><surname>Goodrich</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lukasz</forename><surname>Sepassi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noam</forename><surname>Kaiser</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Shazeer</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1801.10198</idno>\n",
       " <title level=\"m\">Generating Wikipedia by summarizing long sequences</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b98\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Train large, then compress: Rethinking model size for efficient training and inference of transformers</title>\n",
       " <author>\n",
       " <persName><surname>Lws + 20] Zhuohan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Eric</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sheng</forename><surname>Wallace</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kevin</forename><surname>Shen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kurt</forename><surname>Lin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dan</forename><surname>Keutzer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Joseph</forename><forename type=\"middle\">E</forename><surname>Klein</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Gonzalez</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b99\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Race: Large-scale reading comprehension dataset from examinations</title>\n",
       " <author>\n",
       " <persName><surname>+ 17] Guokun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Qizhe</forename><surname>Lai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hanxiao</forename><surname>Xie</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yiming</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Eduard</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Hovy</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1704.04683</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b100\">\n",
       " <monogr>\n",
       " <title/>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jheng-Hong</forename><surname>Lyn + 20] Sheng-Chieh Lin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rodrigo</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ming-Feng</forename><surname>Nogueira</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chuan-Ju</forename><surname>Tsai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jimmy</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Lin</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2003.08380</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">Tttttackling winogrande schemas. arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b101\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Information-based objective functions for active data selection</title>\n",
       " <author>\n",
       " <persName><surname>David</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Mackay</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Neural Computation</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"1992\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b102\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Learned in translation: Contextualized word vectors</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bryan</forename><surname>Mccann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">James</forename><surname>Bradbury</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Caiming</forename><surname>Xiong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Richard</forename><surname>Socher</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in Neural Information Processing Systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " <biblScope from=\"6294\" to=\"6305\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b103\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Efficient estimation of word representations in vector space</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tomas</forename><surname>Mikolov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kai</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Greg</forename><surname>Corrado</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeffrey</forename><surname>Dean</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1301.3781</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2013\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b104\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">A corpus and evaluation framework for deeper understanding of commonsense stories</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nasrin</forename><surname>Mostafazadeh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nathanael</forename><surname>Chambers</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xiaodong</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Devi</forename><surname>Parikh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dhruv</forename><surname>Batra</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lucy</forename><surname>Vanderwende</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pushmeet</forename><surname>Kohli</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">James</forename><surname>Allen</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1604.01696</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b105\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Todor</forename><surname>Mihaylov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Peter</forename><surname>Clark</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tushar</forename><surname>Khot</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ashish</forename><surname>Sabharwal</surname></persName>\n",
       " </author>\n",
       " <idno>abs/1809.02789</idno>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">ArXiv</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b106\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">An empirical model of large-batch training</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sam</forename><surname>Mccandlish</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jared</forename><surname>Kaplan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dario</forename><surname>Amodei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Openai Dota</forename><surname>Team</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b107\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">The penn treebank: annotating predicate argument structure</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mitchell</forename><surname>Marcus</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Grace</forename><surname>Kim</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mary</forename><forename type=\"middle\">Ann</forename><surname>Marcinkiewicz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Robert</forename><surname>Macintyre</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ann</forename><surname>Bies</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mark</forename><surname>Ferguson</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Karen</forename><surname>Katz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Britta</forename><surname>Schasberger</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the workshop on Human Language Technology</title>\n",
       " <meeting>the workshop on Human Language Technology</meeting>\n",
       " <imprint>\n",
       " <publisher>Association for Computational Linguistics</publisher>\n",
       " <date type=\"published\" when=\"1994\"/>\n",
       " <biblScope from=\"114\" to=\"119\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b108\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">The natural language decathlon: Multitask learning as question answering</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bryan</forename><surname>Mccann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nitish</forename><surname>Shirish Keskar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Caiming</forename><surname>Xiong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Richard</forename><surname>Socher</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1806.08730</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b109\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Thomas</forename><surname>Mccoy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ellie</forename><surname>Pavlick</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tal</forename><surname>Linzen</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1902.01007</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b110\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Margaret</forename><surname>Mitchell</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Simone</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrew</forename><surname>Zaldivar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Parker</forename><surname>Barnes</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lucy</forename><surname>Vasserman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ben</forename><surname>Hutchinson</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Elena</forename><surname>Spitzer</surname></persName>\n",
       " </author>\n",
       " <title level=\"m\">Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b111\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Moin</forename><surname>Nadeem</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Anna</forename><surname>Bethke</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Siva</forename><surname>Reddy</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2004.09456</idno>\n",
       " <title level=\"m\">Stereoset: Measuring stereotypical bias in pretrained language models</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b112\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Timothy</forename><surname>Niven</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hung-Yu</forename><surname>Kao</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1907.07355</idno>\n",
       " <title level=\"m\">Probing neural network comprehension of natural language arguments</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b113\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Natural language corpus data</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Peter</forename><surname>Norvig</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2009\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b114\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Fair is better than sensational: Man is to doctor as woman is to doctor</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Malvina</forename><surname>Nissim</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rik</forename><surname>Van Noord</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rob</forename><surname>Van Der Goot</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1905.09866</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b115\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yixin</forename><surname>Nie</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Adina</forename><surname>Williams</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Emily</forename><surname>Dinan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mohit</forename><surname>Bansal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jason</forename><surname>Weston</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Douwe</forename><surname>Kiela</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1910.14599</idno>\n",
       " <title level=\"m\">Adversarial nli: A new benchmark for natural language understanding</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " <note>NWD + 19</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b116\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">WIC: 10,000 example pairs for evaluating context-sensitive representations</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mohammad</forename><surname>Taher Pilehvar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jose</forename><surname>Camacho-Collados</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1808.09121</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b117\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jason</forename><surname>Phang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Thibault</forename><surname>FÃ©vry</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samuel</forename><forename type=\"middle\">R</forename><surname>Bowman</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1811.01088</idno>\n",
       " <title level=\"m\">Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b118\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Collecting diverse natural language inference problems for sentence representation evaluation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Adam</forename><surname>Poliak</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Aparajita</forename><surname>Haldar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rachel</forename><surname>Rudinger</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><forename type=\"middle\">Edward</forename><surname>Hu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ellie</forename><surname>Pavlick</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Aaron</forename><forename type=\"middle\">Steven</forename><surname>White</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Benjamin</forename><surname>Van Durme</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of EMNLP</title>\n",
       " <meeting>EMNLP</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b119\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Denis</forename><surname>Paperno</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">GermÃ¡n</forename><surname>Kruszewski</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Angeliki</forename><surname>Lazaridou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ngoc</forename><surname>Quan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Raffaella</forename><surname>Pham</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sandro</forename><surname>Bernardi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Marco</forename><surname>Pezzelle</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Gemma</forename><surname>Baroni</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Raquel</forename><surname>Boleda</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>FernÃ¡ndez</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1606.06031</idno>\n",
       " <title level=\"m\">The lambada dataset: Word prediction requiring a broad discourse context</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " <note>PKL + 16</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b120\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Dissecting contextual word embeddings: Architecture and representation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matthew</forename><forename type=\"middle\">E</forename><surname>Peters</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mark</forename><surname>Neumann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wen</forename><surname>Tau</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yih</forename></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b121\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">A call for clarity in reporting BLEU scores</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matt</forename><surname>Post</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1804.08771</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b122\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">GloVe: Global vectors for word representation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeffrey</forename><surname>Pennington</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Richard</forename><surname>Socher</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><surname>Manning</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 2014 conference on empirical methods in natural language processing</title>\n",
       " <meeting>the 2014 conference on empirical methods in natural language processing</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b123\">\n",
       " <monogr>\n",
       " <title level=\"m\">QIANXIN. Sa-net on albert (ensemble)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020-04\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b124\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Reducing gender bias in word-level language models with a gender-equalizing loss function</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yusu</forename><surname>Qian</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Urwa</forename><surname>Muaz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ben</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jae</forename><forename type=\"middle\">Won</forename><surname>Hyun</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1905.12801</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b125\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Melissa</forename><surname>Roemmele</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrew S</forename><surname>Cosmin Adrian Bejan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Gordon</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">2011 AAAI Spring Symposium Series</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2011\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b126\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Coqa: A conversational question answering challenge</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Siva</forename><surname>Reddy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Danqi</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher D</forename><surname>Manning</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Transactions of the Association for Computational Linguistics</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">7</biblScope>\n",
       " <biblScope from=\"249\" to=\"266\" unit=\"page\"/>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b127\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yutian</forename><surname>Scott Reed</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Thomas</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">AÃ¤ron</forename><surname>Paine</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Van Den Oord</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Danilo</forename><surname>Eslami</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Rezende</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1710.10304</idno>\n",
       " <title level=\"m\">Oriol Vinyals, and Nando de Freitas. Few-shot autoregressive density estimation: Towards learning to learn distributions</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b128\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pranav</forename><surname>Rajpurkar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Robin</forename><surname>Jia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Percy</forename><surname>Liang</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1806.03822</idno>\n",
       " <title level=\"m\">Know what you don't know: Unanswerable questions for squad</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b129\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Optimization as a model for few-shot learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sachin</forename><surname>Ravi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hugo</forename><surname>Larochelle</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">ICLR</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">2017</biblScope>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b130\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">NumNet: Machine reading comprehension with numerical reasoning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yankai</forename><surname>+ 19] Qiu Ran</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Peng</forename><surname>Lin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jie</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhiyuan</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Liu</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of EMNLP</title>\n",
       " <meeting>EMNLP</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b131\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rachel</forename><surname>Rudinger</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jason</forename><surname>Naradowsky</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Brian</forename><surname>Leonard</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Benjamin</forename><surname>Van Durme</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1804.09301</idno>\n",
       " <title level=\"m\">Gender bias in coreference resolution</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b132\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alec</forename><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Karthik</forename><surname>Narasimhan</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b133\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Guide for conducting risk assessments</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><forename type=\"middle\">S</forename><surname>Ross</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2012\"/>\n",
       " <publisher>NIST Special Publication</publisher>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b134\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">A constructive prediction of the generalization error across scales</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jonathan</forename><forename type=\"middle\">S</forename><surname>Rosenfeld</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Amir</forename><surname>Rosenfeld</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yonatan</forename><surname>Belinkov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nir</forename><surname>Shavit</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b135\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">How much knowledge can you pack into the parameters of a language model</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Adam</forename><surname>Roberts</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Colin</forename><surname>Raffel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noam</forename><surname>Shazeer</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2002.08910</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b136\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Exploring the limits of transfer learning with a unified text-to-text transformer</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Colin</forename><surname>Raffel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noam</forename><surname>Shazeer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Adam</forename><surname>Roberts</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Katherine</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sharan</forename><surname>Narang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Michael</forename><surname>Matena</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yanqi</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wei</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Peter</forename><forename type=\"middle\">J</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b137\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Language models are unsupervised multitask learners</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alec</forename><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeffrey</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rewon</forename><surname>Child</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">David</forename><surname>Luan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dario</forename><surname>Amodei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ilya</forename><surname>Sutskever</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b138\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Winogrande: An adversarial winograd schema challenge at scale</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Keisuke</forename><surname>Sakaguchi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Le</forename><surname>Ronan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chandra</forename><surname>Bras</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yejin</forename><surname>Bhagavatula</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Choi</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b139\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Release strategies and the social impacts of language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Miles</forename><surname>+ 19] Irene Solaiman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jack</forename><surname>Brundage</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Amanda</forename><surname>Clark</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ariel</forename><surname>Askell</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeff</forename><surname>Herbert-Voss</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alec</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Gretchen</forename><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jong</forename><forename type=\"middle\">Wook</forename><surname>Krueger</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Kim</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " <pubPlace>Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGuffie, and Jasmine Wang</pubPlace>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b140\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Emily</forename><surname>Sheng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kai-Wei</forename><surname>Chang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Premkumar</forename><surname>Natarajan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nanyun</forename><surname>Peng</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1909.01326</idno>\n",
       " <title level=\"m\">The woman worked as a babysitter: On biases in language generation</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b141\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Victor</forename><surname>Sanh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lysandre</forename><surname>Debut</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Julien</forename><surname>Chaumond</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Thomas</forename><surname>Wolf</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1910.01108</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b142\">\n",
       " <monogr>\n",
       " <title/>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Roy</forename><surname>Schwartz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jesse</forename><surname>Dodge</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noah</forename><forename type=\"middle\">A</forename><surname>Smith</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oren</forename><forename type=\"middle\">Etzioni</forename><surname>Green</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><forename type=\"middle\">I</forename><surname>Corr</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"1907\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b143\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Improving neural machine translation models with monolingual data</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rico</forename><surname>Sennrich</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Barry</forename><surname>Haddow</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alexandra</forename><surname>Birch</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1511.06709</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b144\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>\n",
       " <author>\n",
       " <persName><surname>+ 17] Noam</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Azalia</forename><surname>Shazeer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Krzysztof</forename><surname>Mirhoseini</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andy</forename><surname>Maziarz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Quoc</forename><surname>Davis</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Geoffrey</forename><surname>Le</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeff</forename><surname>Hinton</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Dean</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1701.06538</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b145\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mohammad</forename><surname>Shoeybi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mostofa</forename><surname>Patwary</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Raul</forename><surname>Puri</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Patrick</forename><surname>Legresley</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jared</forename><surname>Casper</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bryan</forename><surname>Catanzaro</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>+ 19</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b146\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Exploiting cloze questions for few-shot text classification and natural language inference</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Timo</forename><surname>Schick</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hinrich</forename><surname>SchÃ¼tze</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2001.07676</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b147\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">MASS: Masked sequence to sequence pre-training for language generation</title>\n",
       " <author>\n",
       " <persName><surname>+ 19] Kaitao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xu</forename><surname>Song</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tao</forename><surname>Tan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jianfeng</forename><surname>Qin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tie-Yan</forename><surname>Lu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1905.02450</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b148\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Domain randomization for transferring deep neural networks from simulation to the real world</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">+ 17] Josh</forename><surname>Tobin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rachel</forename><surname>Fong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Ray</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jonas</forename><surname>Schneider</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wojciech</forename><surname>Zaremba</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pieter</forename><surname>Abbeel</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)</title>\n",
       " <imprint>\n",
       " <publisher>IEEE</publisher>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " <biblScope from=\"23\" to=\"30\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b149\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Corpus-based learning of analogies and semantic relations. CoRR, abs/cs/0508103</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Peter</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Michael</forename><forename type=\"middle\">L</forename><surname>Turney</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Littman</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2005\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b150\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">A simple method for commonsense reasoning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Trieu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Trinh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><surname>Quoc</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Le</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1806.02847</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b151\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Combining independent modules to solve multiple-choice synonym and analogy problems</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Peter</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Michael</forename><forename type=\"middle\">L</forename><surname>Turney</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeffrey</forename><surname>Littman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Victor</forename><surname>Bigham</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Shnayder</surname></persName>\n",
       " </author>\n",
       " <idno>cs.CL/0309035</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2003\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b152\">\n",
       " <monogr>\n",
       " <title level=\"m\">Project Turing. Microsoft research blog</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020-02\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b153\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Matching Networks for One Shot Learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oriol</forename><surname>Vinyals</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Charles</forename><surname>Blundell</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Timothy</forename><surname>Lillicrap</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Daan</forename><surname>Wierstra</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in neural information processing systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " <biblScope from=\"3630\" to=\"3638\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>VBL + 16</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b154\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Attention is all you need</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ashish</forename><surname>Vaswani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noam</forename><surname>Shazeer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Niki</forename><surname>Parmar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jakob</forename><surname>Uszkoreit</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Llion</forename><surname>Jones</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Aidan</forename><forename type=\"middle\">N</forename><surname>Gomez</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Åukasz</forename><surname>Kaiser</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Illia</forename><surname>Polosukhin</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in neural information processing systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b155\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Superglue: A stickier benchmark for general-purpose language understanding systems</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yada</forename><surname>Pruksachatkun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nikita</forename><surname>Nangia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Amanpreet</forename><surname>Singh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Julian</forename><surname>Michael</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Felix</forename><surname>Hill</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Omer</forename><surname>Levy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samuel</forename><surname>Bowman</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in Neural Information Processing Systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " <biblScope from=\"3261\" to=\"3275\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b156\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Multi-agent dual learning</title>\n",
       " <author>\n",
       " <persName><surname>+ 18] Yiren</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yingce</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tianyu</forename><surname>Xia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Fei</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tao</forename><surname>Tian</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chengxiang</forename><surname>Qin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tie-Yan</forename><surname>Zhai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b157\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Unsupervised data augmentation for consistency training</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zihang</forename><surname>+ 19] Qizhe Xie</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Eduard</forename><surname>Dai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Minh-Thang</forename><surname>Hovy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Quoc</forename><forename type=\"middle\">V</forename><surname>Luong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Le</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b158\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Learning and evaluating general linguistic intelligence</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dani</forename><surname>Yogatama</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Cyprien</forename><surname>De Masson D'autume</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jerome</forename><surname>Connor</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tomas</forename><surname>Kocisky</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mike</forename><surname>Chrzanowski</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lingpeng</forename><surname>Kong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Angeliki</forename><surname>Lazaridou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wang</forename><surname>Ling</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lei</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chris</forename><surname>Dyer</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1901.11373</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b159\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zihang</forename><surname>+ 19] Zhilin Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yiming</forename><surname>Dai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jaime</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ruslan</forename><surname>Carbonell</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Quoc</forename><forename type=\"middle\">V</forename><surname>Salakhutdinov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Le</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Xlnet</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1906.08237</idno>\n",
       " <title level=\"m\">Generalized autoregressive pretraining for language understanding</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b160\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Hellaswag: Can a machine really finish your sentence?</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ari</forename><surname>Zhb + 19] Rowan Zellers</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yonatan</forename><surname>Holtzman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ali</forename><surname>Bisk</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yejin</forename><surname>Farhadi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Choi</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1905.07830</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b161\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ari</forename><surname>+ 19] Rowan Zellers</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hannah</forename><surname>Holtzman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yonatan</forename><surname>Rashkin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ali</forename><surname>Bisk</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Franziska</forename><surname>Farhadi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yejin</forename><surname>Roesner</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Choi</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1905.12616</idno>\n",
       " <title level=\"m\">Defending against neural fake news</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b162\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><surname>+ 18] Sheng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xiaodong</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jingjing</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jianfeng</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kevin</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Benjamin</forename><surname>Duh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Van Durme</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1810.12885</idno>\n",
       " <title level=\"m\">ReCoRD: Bridging the gap between human and machine commonsense reading comprehension</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b163\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Fine-tuning language models from human preferences</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Daniel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nisan</forename><surname>Ziegler</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeffrey</forename><surname>Stiennon</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tom</forename><forename type=\"middle\">B</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alec</forename><surname>Brown</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dario</forename><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Paul</forename><surname>Amodei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Geoffrey</forename><surname>Christiano</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Irving</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b164\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. ArXiv, abs</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Zsw + 19b] Daniel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nisan</forename><surname>Ziegler</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeffrey</forename><surname>Stiennon</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tom</forename><forename type=\"middle\">B</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alec</forename><surname>Brown</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dario</forename><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Amodei</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"1909\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " </listBibl>\n",
       " </div>\n",
       " </back>\n",
       " </text>\n",
       " </TEI>,\n",
       " <?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       " <TEI xml:space=\"preserve\" xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd\">\n",
       " <teiHeader xml:lang=\"en\">\n",
       " <fileDesc>\n",
       " <titleStmt>\n",
       " <title level=\"a\" type=\"main\">LORA: LOW-RANK ADAPTATION OF LARGE LAN-GUAGE MODELS</title>\n",
       " </titleStmt>\n",
       " <publicationStmt>\n",
       " <publisher/>\n",
       " <availability status=\"unknown\"><licence/></availability>\n",
       " <date type=\"published\" when=\"2021-10-16\">16 Oct 2021</date>\n",
       " </publicationStmt>\n",
       " <sourceDesc>\n",
       " <biblStruct>\n",
       " <analytic>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Edward</forename><surname>Hu</surname></persName>\n",
       " <email>edwardhu@microsoft.com</email>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Microsoft Corporation</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yelong</forename><surname>Shen</surname></persName>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Microsoft Corporation</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Phillip</forename><surname>Wallis</surname></persName>\n",
       " <email>phwallis@microsoft.com</email>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Microsoft Corporation</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zeyuan</forename><surname>Allen-Zhu</surname></persName>\n",
       " <email>zeyuana@microsoft.com</email>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Microsoft Corporation</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yuanzhi</forename><surname>Li</surname></persName>\n",
       " <email>yuanzhil@microsoft.com</email>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Microsoft Corporation</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Shean</forename><surname>Wang</surname></persName>\n",
       " <email>swang@microsoft.com</email>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Microsoft Corporation</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lu</forename><surname>Wang</surname></persName>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Microsoft Corporation</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Weizhu</forename><surname>Chen</surname></persName>\n",
       " <email>wzchen@microsoft.com</email>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Microsoft Corporation</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <title level=\"a\" type=\"main\">LORA: LOW-RANK ADAPTATION OF LARGE LAN-GUAGE MODELS</title>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021-10-16\">16 Oct 2021</date>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <idno type=\"arXiv\">arXiv:2106.09685v2[cs.CL]</idno>\n",
       " </biblStruct>\n",
       " </sourceDesc>\n",
       " </fileDesc>\n",
       " <encodingDesc>\n",
       " <appInfo>\n",
       " <application ident=\"GROBID\" version=\"0.7.0-SNAPSHOT\" when=\"2024-04-27T16:15+0000\">\n",
       " <desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>\n",
       " <ref target=\"https://github.com/kermitt2/grobid\"/>\n",
       " </application>\n",
       " </appInfo>\n",
       " </encodingDesc>\n",
       " <profileDesc>\n",
       " <abstract>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.</p></div>\n",
       " </abstract>\n",
       " </profileDesc>\n",
       " </teiHeader>\n",
       " <text xml:lang=\"en\">\n",
       " <body>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">INTRODUCTION</head><p>Pretrained Weights</p><formula xml:id=\"formula_0\">âˆˆ â„ Ã— x h = 0 = (0, 2 ) Pretrained Weights âˆˆ â„ Ã— x f(x)</formula><p>Figure <ref type=\"figure\">1</ref>: Our reparametrization. We only train A and B.</p><p>Many applications in natural language processing rely on adapting one large-scale, pre-trained language model to multiple downstream applications. Such adaptation is usually done via fine-tuning, which updates all the parameters of the pre-trained model. The major downside of fine-tuning is that the new model contains as many parameters as in the original model. As larger models are trained every few months, this changes from a mere \"inconvenience\" for GPT-2 <ref type=\"bibr\">(Radford et al., b)</ref> or RoBERTa large <ref target=\"#b35\" type=\"bibr\">(Liu et al., 2019)</ref> to a critical deployment challenge for GPT-3 <ref type=\"bibr\">(Brown et al., 2020)</ref> with 175 billion trainable parameters. <ref type=\"bibr\">1</ref> Many sought to mitigate this by adapting only some parameters or learning external modules for new tasks. This way, we only need to store and load a small number of task-specific parameters in addition to the pre-trained model for each task, greatly boosting the operational efficiency when deployed. However, existing techniques often introduce inference latency <ref target=\"#b22\" type=\"bibr\">(Houlsby et al., 2019;</ref><ref target=\"#b47\" type=\"bibr\">Rebuffi et al., 2017</ref>) by extending model depth or reduce the model's usable sequence length <ref target=\"#b29\" type=\"bibr\">(Li &amp; Liang, 2021;</ref><ref target=\"#b20\" type=\"bibr\">Hambardzumyan et al., 2020;</ref>) (Section 3). More importantly, these method often fail to match the fine-tuning baselines, posing a trade-off between efficiency and model quality.</p><p>We take inspiration from <ref target=\"#b28\" type=\"bibr\">Li et al. (2018a)</ref>; <ref target=\"#b0\" type=\"bibr\">Aghajanyan et al. (2020)</ref> which show that the learned over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the change in weights during model adaptation also has a low \"intrinsic rank\", leading to our proposed Low-Rank Adaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural network indirectly by optimizing rank decomposition matrices of the dense layers' change during adaptation instead, while keeping the pre-trained weights frozen, as shown in Figure <ref type=\"figure\">1</ref>. Using GPT-3 175B as an example, we show that a very low rank (i.e., r in Figure <ref type=\"figure\">1</ref> can be one or two) suffices even when the full rank (i.e., d) is as high as 12,288, making LoRA both storage-and compute-efficient.</p><p>LoRA possesses several key advantages.</p><p>â€¢ A pre-trained model can be shared and used to build many small LoRA modules for different tasks. We can freeze the shared model and efficiently switch tasks by replacing the matrices A and B in Figure <ref type=\"figure\">1</ref>, reducing the storage requirement and task-switching overhead significantly.</p><p>â€¢ LoRA makes training more efficient and lowers the hardware barrier to entry by up to 3 times when using adaptive optimizers since we do not need to calculate the gradients or maintain the optimizer states for most parameters. Instead, we only optimize the injected, much smaller low-rank matrices.</p><p>â€¢ Our simple linear design allows us to merge the trainable matrices with the frozen weights when deployed, introducing no inference latency compared to a fully fine-tuned model, by construction.</p><p>â€¢ LoRA is orthogonal to many prior methods and can be combined with many of them, such as prefix-tuning. We provide an example in Appendix E.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Terminologies and Conventions</head><p>We make frequent references to the Transformer architecture and use the conventional terminologies for its dimensions. We call the input and output dimension size of a Transformer layer d model . We use W q , W k , W v , and W o to refer to the query/key/value/output projection matrices in the self-attention module. W or W 0 refers to a pretrained weight matrix and âˆ†W its accumulated gradient update during adaptation. We use r to denote the rank of a LoRA module. We follow the conventions set out by <ref target=\"#b52\" type=\"bibr\">(Vaswani et al., 2017;</ref><ref type=\"bibr\">Brown et al., 2020)</ref> and use Adam <ref target=\"#b37\" type=\"bibr\">(Loshchilov &amp; Hutter, 2019;</ref><ref target=\"#b25\" type=\"bibr\">Kingma &amp; Ba, 2017)</ref> for model optimization and use a Transformer MLP feedforward dimension d f f n = 4 Ã— d model .</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">PROBLEM STATEMENT</head><p>While our proposal is agnostic to training objective, we focus on language modeling as our motivating use case. Below is a brief description of the language modeling problem and, in particular, the maximization of conditional probabilities given a task-specific prompt.</p><p>Suppose we are given a pre-trained autoregressive language model P Î¦ (y|x) parametrized by Î¦.</p><p>For instance, P Î¦ (y|x) can be a generic multi-task learner such as GPT <ref type=\"bibr\">(Radford et al., b;</ref><ref type=\"bibr\">Brown et al., 2020)</ref> based on the Transformer architecture <ref target=\"#b52\" type=\"bibr\">(Vaswani et al., 2017)</ref>. Consider adapting this pre-trained model to downstream conditional text generation tasks, such as summarization, machine reading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is represented by a training dataset of context-target pairs: Z = {(x i , y i )} i=1,..,N , where both x i and y i are sequences of tokens. For example, in NL2SQL, x i is a natural language query and y i its corresponding SQL command; for summarization, x i is the content of an article and y i its summary.</p><p>During full fine-tuning, the model is initialized to pre-trained weights Î¦ 0 and updated to Î¦ 0 + âˆ†Î¦ by repeatedly following the gradient to maximize the conditional language modeling objective:</p><formula xml:id=\"formula_1\">max Î¦ (x,y)âˆˆZ |y| t=1 log (P Î¦ (y t |x, y &lt;t )) (1)</formula><p>One of the main drawbacks for full fine-tuning is that for each downstream task, we learn a different set of parameters âˆ†Î¦ whose dimension |âˆ†Î¦| equals |Î¦ 0 |. Thus, if the pre-trained model is large (such as GPT-3 with |Î¦ 0 | â‰ˆ 175 Billion), storing and deploying many independent instances of fine-tuned models can be challenging, if at all feasible.</p><p>In this paper, we adopt a more parameter-efficient approach, where the task-specific parameter increment âˆ†Î¦ = âˆ†Î¦(Î˜) is further encoded by a much smaller-sized set of parameters Î˜ with |Î˜| |Î¦ 0 |. The task of finding âˆ†Î¦ thus becomes optimizing over Î˜:</p><formula xml:id=\"formula_2\">max Î˜ (x,y)âˆˆZ |y| t=1 log p Î¦0+âˆ†Î¦(Î˜) (y t |x, y &lt;t )<label>(2)</label></formula><p>In the subsequent sections, we propose to use a low-rank representation to encode âˆ†Î¦ that is both compute-and memory-efficient. When the pre-trained model is GPT-3 175B, the number of trainable parameters |Î˜| can be as small as 0.01% of |Î¦ 0 |.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3\">AREN'T EXISTING SOLUTIONS GOOD ENOUGH?</head><p>The problem we set out to tackle is by no means new. Since the inception of transfer learning, dozens of works have sought to make model adaptation more parameter-and compute-efficient. See Section 6 for a survey of some of the well-known works. Using language modeling as an example, there are two prominent strategies when it comes to efficient adaptations: adding adapter layers <ref target=\"#b22\" type=\"bibr\">(Houlsby et al., 2019;</ref><ref target=\"#b47\" type=\"bibr\">Rebuffi et al., 2017;</ref><ref target=\"#b42\" type=\"bibr\">Pfeiffer et al., 2021;</ref><ref target=\"#b48\" type=\"bibr\">RÃ¼cklÃ© et al., 2020)</ref> or optimizing some forms of the input layer activations <ref target=\"#b29\" type=\"bibr\">(Li &amp; Liang, 2021;</ref><ref target=\"#b20\" type=\"bibr\">Hambardzumyan et al., 2020;</ref>. However, both strategies have their limitations, especially in a large-scale and latency-sensitive production scenario.</p><p>Adapter Layers Introduce Inference Latency There are many variants of adapters. We focus on the original design by <ref target=\"#b22\" type=\"bibr\">Houlsby et al. (2019)</ref> which has two adapter layers per Transformer block and a more recent one by <ref target=\"#b33\" type=\"bibr\">Lin et al. (2020)</ref> which has only one per block but with an additional LayerNorm <ref target=\"#b5\" type=\"bibr\">(Ba et al., 2016)</ref>. While one can reduce the overall latency by pruning layers or exploiting multi-task settings <ref target=\"#b48\" type=\"bibr\">(RÃ¼cklÃ© et al., 2020;</ref><ref target=\"#b42\" type=\"bibr\">Pfeiffer et al., 2021)</ref>, there is no direct ways to bypass the extra compute in adapter layers. This seems like a non-issue since adapter layers are designed to have few parameters (sometimes &lt;1% of the original model) by having a small bottleneck dimension, which limits the FLOPs they can add. However, large neural networks rely on hardware parallelism to keep the latency low, and adapter layers have to be processed sequentially. This makes a difference in the online inference setting where the batch size is typically as small as one. In a generic scenario without model parallelism, such as running inference on GPT-2 <ref type=\"bibr\">(Radford et al., b)</ref> medium on a single GPU, we see a noticeable increase in latency when using adapters, even with a very small bottleneck dimension (Table <ref target=\"#tab_1\" type=\"table\">1</ref>).</p><p>This problem gets worse when we need to shard the model as done in <ref target=\"#b50\" type=\"bibr\">Shoeybi et al. (2020)</ref>; <ref target=\"#b26\" type=\"bibr\">Lepikhin et al. (2020)</ref>, because the additional depth requires more synchronous GPU operations such as AllReduce and Broadcast, unless we store the adapter parameters redundantly many times.</p><p>Directly Optimizing the Prompt is Hard The other direction, as exemplified by prefix tuning <ref target=\"#b29\" type=\"bibr\">(Li &amp; Liang, 2021)</ref>, faces a different challenge. We observe that prefix tuning is difficult to optimize and that its performance changes non-monotonically in trainable parameters, confirming similar observations in the original paper. More fundamentally, reserving a part of the sequence length for adaptation necessarily reduces the sequence length available to process a downstream task, which we suspect makes tuning the prompt less performant compared to other methods. We defer the study on task performance to Section 5.  </p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4\">OUR METHOD</head><p>We describe the simple design of LoRA and its practical benefits. The principles outlined here apply to any dense layers in deep learning models, though we only focus on certain weights in Transformer language models in our experiments as the motivating use case.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1\">LOW-RANK-PARAMETRIZED UPDATE MATRICES</head><p>A neural network contains many dense layers which perform matrix multiplication. The weight matrices in these layers typically have full-rank. When adapting to a specific task, <ref target=\"#b0\" type=\"bibr\">Aghajanyan et al. (2020)</ref> shows that the pre-trained language models have a low \"instrisic dimension\" and can still learn efficiently despite a random projection to a smaller subspace. Inspired by this, we hypothesize the updates to the weights also have a low \"intrinsic rank\" during adaptation. For a pre-trained weight matrix W 0 âˆˆ R dÃ—k , we constrain its update by representing the latter with a low-rank decomposition W 0 + âˆ†W = W 0 + BA, where B âˆˆ R dÃ—r , A âˆˆ R rÃ—k , and the rank r min(d, k). During training, W 0 is frozen and does not receive gradient updates, while A and B contain trainable parameters. Note both W 0 and âˆ†W = BA are multiplied with the same input, and their respective output vectors are summed coordinate-wise. For h = W 0 x, our modified forward pass yields:</p><formula xml:id=\"formula_3\">h = W 0 x + âˆ†W x = W 0 x + BAx (3)</formula><p>We illustrate our reparametrization in Figure <ref type=\"figure\">1</ref>. We use a random Gaussian initialization for A and zero for B, so âˆ†W = BA is zero at the beginning of training. We then scale âˆ†W x by Î± r , where Î± is a constant in r. When optimizing with Adam, tuning Î± is roughly the same as tuning the learning rate if we scale the initialization appropriately. As a result, we simply set Î± to the first r we try and do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary r <ref target=\"#b58\" type=\"bibr\">(Yang &amp; Hu, 2021)</ref>.</p><p>A Generalization of Full Fine-tuning. A more general form of fine-tuning allows the training of a subset of the pre-trained parameters. LoRA takes a step further and does not require the accumulated gradient update to weight matrices to have full-rank during adaptation. This means that when applying LoRA to all weight matrices and training all biases 2 , we roughly recover the expressiveness of full fine-tuning by setting the LoRA rank r to the rank of the pre-trained weight matrices. In other words, as we increase the number of trainable parameters 3 , training LoRA roughly converges to training the original model, while adapter-based methods converges to an MLP and prefix-based methods to a model that cannot take long input sequences.</p><p>No Additional Inference Latency. When deployed in production, we can explicitly compute and store W = W 0 + BA and perform inference as usual. Note that both W 0 and BA are in R dÃ—k . When we need to switch to another downstream task, we can recover W 0 by subtracting BA and then adding a different B A , a quick operation with very little memory overhead. Critically, this guarantees that we do not introduce any additional latency during inference compared to a fine-tuned model by construction.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2\">APPLYING LORA TO TRANSFORMER</head><p>In principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the number of trainable parameters. In the Transformer architecture, there are four weight matrices in the self-attention module (W q , W k , W v , W o ) and two in the MLP module. We treat W q (or W k , W v ) as a single matrix of dimension d model Ã— d model , even though the output dimension is usually sliced into attention heads. We limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency.We further study the effect on adapting different types of attention weight matrices in a Transformer in Section 7.1. We leave the empirical investigation of adapting the MLP layers, LayerNorm layers, and biases to a future work.</p><p>Practical Benefits and Limitations. The most significant benefit comes from the reduction in memory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM usage by up to 2/3 if r d model as we do not need to store the optimizer states for the frozen parameters. On GPT-3 175B, we reduce the VRAM consumption during training from 1.2TB to 350GB. With r = 4 and only the query and value projection matrices being adapted, the checkpoint size is reduced by roughly 10,000Ã— (from 350GB to 35MB) 4 . This allows us to train with significantly fewer GPUs and avoid I/O bottlenecks. Another benefit is that we can switch between tasks while deployed at a much lower cost by only swapping the LoRA weights as opposed to all the parameters. This allows for the creation of many customized models that can be swapped in and out on the fly on machines that store the pre-trained weights in VRAM. We also observe a 25% speedup during training on GPT-3 175B compared to full fine-tuning 5 as we do not need to calculate the gradient for the vast majority of the parameters.</p><p>LoRA also has its limitations. For example, it is not straightforward to batch inputs to different tasks with different A and B in a single forward pass, if one chooses to absorb A and B into W to eliminate additional inference latency. Though it is possible to not merge the weights and dynamically choose the LoRA modules to use for samples in a batch for scenarios where latency is not critical.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5\">EMPIRICAL EXPERIMENTS</head><p>We evaluate the downstream task performance of LoRA on RoBERTa <ref target=\"#b35\" type=\"bibr\">(Liu et al., 2019)</ref>, <ref type=\"bibr\">De-BERTa (He et al., 2021)</ref>, and GPT-2 <ref type=\"bibr\">(Radford et al., b)</ref>, before scaling up to GPT-3 175B <ref type=\"bibr\">(Brown et al., 2020)</ref>. Our experiments cover a wide range of tasks, from natural language understanding (NLU) to generation (NLG). Specifically, we evaluate on the GLUE <ref target=\"#b53\" type=\"bibr\">(Wang et al., 2019)</ref> benchmark for RoBERTa and DeBERTa. We follow the setup of <ref target=\"#b29\" type=\"bibr\">Li &amp; Liang (2021)</ref> on GPT-2 for a direct comparison and add WikiSQL <ref target=\"#b62\" type=\"bibr\">(Zhong et al., 2017)</ref> (NL to SQL queries) and SAMSum <ref target=\"#b17\" type=\"bibr\">(Gliwa et al., 2019)</ref> (conversation summarization) for large-scale experiments on GPT-3. See Appendix C for more details on the datasets we use. We use NVIDIA Tesla V100 for all experiments.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">BASELINES</head><p>To compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible. This, however, means that some baselines might only appear in certain experiments.</p><p>Fine-Tuning (FT) is a common approach for adaptation. During fine-tuning, the model is initialized to the pre-trained weights and biases, and all model parameters undergo gradient updates.A simple variant is to update only some layers while freezing others. We include one such baseline reported in prior work <ref target=\"#b29\" type=\"bibr\">(Li &amp; Liang, 2021</ref>) on GPT-2, which adapts just the last two layers (FT Top2 ).  Bias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else. Contemporarily, this baseline has also been studied by <ref type=\"bibr\">BitFit (Zaken et al., 2021)</ref>.</p><p>Prefix-embedding tuning (PreEmbed) inserts special tokens among the input tokens. These special tokens have trainable word embeddings and are generally not in the model's vocabulary. Where to place such tokens can have an impact on performance. We focus on \"prefixing\", which prepends such tokens to the prompt, and \"infixing\", which appends to the prompt; both are discussed in <ref target=\"#b29\" type=\"bibr\">Li &amp; Liang (2021)</ref>. We use l p (resp. l i ) denote the number of prefix (resp. infix) tokens. The number of trainable parameters is |Î˜| = d model Ã— (l p + l i ).</p><p>Prefix-layer tuning (PreLayer) is an extension to prefix-embedding tuning. Instead of just learning the word embeddings (or equivalently, the activations after the embedding layer) for some special tokens, we learn the activations after every Transformer layer. The activations computed from previous layers are simply replaced by trainable ones. The resulting number of trainable parameters is</p><formula xml:id=\"formula_4\">|Î˜| = L Ã— d model Ã— (l p + l i ),</formula><p>where L is the number of Transformer layers.</p><p>Adapter tuning as proposed in <ref target=\"#b22\" type=\"bibr\">Houlsby et al. (2019)</ref> inserts adapter layers between the selfattention module (and the MLP module) and the subsequent residual connection. There are two fully connected layers with biases in an adapter layer with a nonlinearity in between. We call this original design Adapter H . Recently, <ref target=\"#b33\" type=\"bibr\">Lin et al. (2020)</ref> proposed a more efficient design with the adapter layer applied only after the MLP module and after a LayerNorm. We call it Adapter L . This is very similar to another deign proposed in <ref target=\"#b42\" type=\"bibr\">Pfeiffer et al. (2021)</ref>, which we call Adapter P . We also include another baseline call AdapterDrop <ref target=\"#b48\" type=\"bibr\">(RÃ¼cklÃ© et al., 2020)</ref> which drops some adapter layers for greater efficiency (Adapter D ). We cite numbers from prior works whenever possible to maximize the number of baselines we compare with; they are in rows with an asterisk (*) in the first column. In all cases, we have</p><formula xml:id=\"formula_5\">|Î˜| =L Adpt Ã— (2 Ã— d model Ã— r + r + d model ) + 2 Ã—L LN Ã— d model whereL Adpt</formula><p>is the number of adapter layers andL LN the number of trainable LayerNorms (e.g., in Adapter L ).</p><p>LoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices. As mentioned in Section 4.2, we only apply LoRA to W q and W v in most experiments for simplicity. The number of trainable parameters is determined by the rank r and the shape of the original weights:  </p><formula xml:id=\"formula_6\">|Î˜| = 2 Ã—L LoRA Ã— d model Ã— r,</formula></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.2\">ROBERTA BASE/LARGE</head><p>RoBERTa <ref target=\"#b35\" type=\"bibr\">(Liu et al., 2019)</ref> optimized the pre-training recipe originally proposed in BERT <ref target=\"#b12\" type=\"bibr\">(Devlin et al., 2019a)</ref> and boosted the latter's task performance without introducing many more trainable parameters. While RoBERTa has been overtaken by much larger models on NLP leaderboards such as the GLUE benchmark <ref target=\"#b53\" type=\"bibr\">(Wang et al., 2019)</ref> in recent years, it remains a competitive and popular pre-trained model for its size among practitioners. We take the pre-trained RoBERTa base (125M) and RoBERTa large (355M) from the HuggingFace Transformers library <ref target=\"#b57\" type=\"bibr\">(Wolf et al., 2020)</ref> and evaluate the performance of different efficient adaptation approaches on tasks from the GLUE benchmark. We also replicate <ref target=\"#b22\" type=\"bibr\">Houlsby et al. (2019)</ref> and <ref target=\"#b42\" type=\"bibr\">Pfeiffer et al. (2021)</ref> according to their setup. To ensure a fair comparison, we make two crucial changes to how we evaluate LoRA when comparing with adapters. First, we use the same batch size for all tasks and use a sequence length of 128 to match the adapter baselines. Second, we initialize the model to the pre-trained model for MRPC, RTE, and STS-B, not a model already adapted to MNLI like the fine-tuning baseline. Runs following this more restricted setup from <ref target=\"#b22\" type=\"bibr\">Houlsby et al. (2019)</ref> are labeled with â€ . The result is presented in Table <ref target=\"#tab_3\" type=\"table\">2</ref> (Top Three Sections). See Section D.1 for details on the hyperparameters used.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.3\">DEBERTA XXL</head><p>DeBERTa <ref target=\"#b21\" type=\"bibr\">(He et al., 2021</ref>) is a more recent variant of BERT that is trained on a much larger scale and performs very competitively on benchmarks such as GLUE <ref target=\"#b53\" type=\"bibr\">(Wang et al., 2019)</ref> and Su-perGLUE <ref target=\"#b54\" type=\"bibr\">(Wang et al., 2020)</ref>. We evaluate if LoRA can still match the performance of a fully fine-tuned DeBERTa XXL (1.5B) on GLUE. The result is presented in Table <ref target=\"#tab_3\" type=\"table\">2</ref> (Bottom Section). See Section D.2 for details on the hyperparameters used.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.4\">GPT-2 MEDIUM/LARGE</head><p>Having shown that LoRA can be a competitive alternative to full fine-tuning on NLU, we hope to answer if LoRA still prevails on NLG models, such as GPT-2 medium and large <ref type=\"bibr\">(Radford et al., b)</ref>. We keep our setup as close as possible to <ref target=\"#b29\" type=\"bibr\">Li &amp; Liang (2021)</ref> for a direct comparison. Due to space constraint, we only present our result on E2E NLG Challenge (Table <ref target=\"#tab_5\" type=\"table\">3</ref>) in this section. See Section F.1 for results on WebNLG <ref target=\"#b15\" type=\"bibr\">(Gardent et al., 2017)</ref> and DART <ref target=\"#b39\" type=\"bibr\">(Nan et al., 2020)</ref>. We include a list of the hyperparameters used in Section D.3. </p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.5\">SCALING UP TO GPT-3 175B</head><p>As a final stress test for LoRA, we scale up to GPT-3 with 175 billion parameters. Due to the high training cost, we only report the typical standard deviation for a given task over random seeds, as opposed to providing one for every entry. See Section D.4 for details on the hyperparameters used.</p><p>As shown in Table <ref type=\"table\">4</ref>, LoRA matches or exceeds the fine-tuning baseline on all three datasets. Note that not all methods benefit monotonically from having more trainable parameters, as shown in Figure <ref target=\"#fig_0\" type=\"figure\">2</ref>. We observe a significant performance drop when we use more than 256 special tokens for prefix-embedding tuning or more than 32 special tokens for prefix-layer tuning. This corroborates similar observations in <ref target=\"#b29\" type=\"bibr\">Li &amp; Liang (2021)</ref>. While a thorough investigation into this phenomenon is out-of-scope for this work, we suspect that having more special tokens causes the input distribution to shift further away from the pre-training data distribution. Separately, we investigate the performance of different adaptation approaches in the low-data regime in Section F.3.  Prompt Engineering and Fine-Tuning. While GPT-3 175B can adapt its behavior with just a few additional training examples, the result depends heavily on the input prompt <ref type=\"bibr\">(Brown et al., 2020)</ref>. This necessitates an empirical art of composing and formatting the prompt to maximize a model's performance on a desired task, which is known as prompt engineering or prompt hacking.  <ref type=\"formula\">2008</ref>), yet practitioners often retrain all of them to maximize the downstream performance. However, the enormity of GPT-3 175B makes it challenging to perform fine-tuning in the usual way due to the large checkpoint it produces and the high hardware barrier to entry since it has the same memory footprint as pre-training.</p><p>Parameter-Efficient Adaptation. Many have proposed inserting adapter layers between existing layers in a neural network <ref target=\"#b22\" type=\"bibr\">(Houlsby et al., 2019;</ref><ref target=\"#b47\" type=\"bibr\">Rebuffi et al., 2017;</ref><ref target=\"#b33\" type=\"bibr\">Lin et al., 2020)</ref>. Our method uses a similar bottleneck structure to impose a low-rank constraint on the weight updates. The key functional difference is that our learned weights can be merged with the main weights during inference, thus not introducing any latency, which is not the case for the adapter layers (Section 3).</p><p>A comtenporary extension of adapter is COMPACTER <ref target=\"#b38\" type=\"bibr\">(Mahabadi et al., 2021)</ref>, which essentially parametrizes the adapter layers using Kronecker products with some predetermined weight sharing scheme. Similarly, combining LoRA with other tensor product-based methods could potentially improve its parameter efficiency, which we leave to future work. More recently, many proposed optimizing the input word embeddings in lieu of fine-tuning, akin to a continuous and differentiable generalization of prompt engineering <ref target=\"#b29\" type=\"bibr\">(Li &amp; Liang, 2021;</ref><ref target=\"#b20\" type=\"bibr\">Hambardzumyan et al., 2020;</ref>. We include comparisons with <ref target=\"#b29\" type=\"bibr\">Li &amp; Liang (2021)</ref> in our experiment section. However, this line of works can only scale up by using more special tokens in the prompt, which take up available sequence length for task tokens when positional embeddings are learned.</p><p>Low-Rank Structures in Deep Learning. Low-rank structure is very common in machine learning. A lot of machine learning problems have certain intrinsic low-rank structure <ref target=\"#b8\" type=\"bibr\">Cai et al., 2010;</ref><ref target=\"#b32\" type=\"bibr\">Li et al., 2018b;</ref><ref target=\"#b18\" type=\"bibr\">Grasedyck et al., 2013)</ref>. Moreover, it is known that for many deep learning tasks, especially those with a heavily over-parametrized neural network, the learned neural network will enjoy low-rank properties after training <ref target=\"#b41\" type=\"bibr\">(Oymak et al., 2019)</ref>. Some prior works even explicitly impose the low-rank constraint when training the original neural network <ref target=\"#b49\" type=\"bibr\">(Sainath et al., 2013;</ref><ref target=\"#b43\" type=\"bibr\">Povey et al., 2018;</ref><ref target=\"#b60\" type=\"bibr\">Zhang et al., 2014;</ref><ref target=\"#b23\" type=\"bibr\">Jaderberg et al., 2014;</ref><ref target=\"#b61\" type=\"bibr\">Zhao et al., 2016;</ref><ref target=\"#b24\" type=\"bibr\">Khodak et al., 2021;</ref><ref target=\"#b11\" type=\"bibr\">Denil et al., 2014)</ref>; however, to the best of our knowledge, none of these works considers low-rank update to a frozen model for adaptation to downstream tasks. In theory literature, it is known that neural networks outperform other classical learning methods, including the corresponding (finite-width) neural tangent kernels <ref target=\"#b30\" type=\"bibr\">Li &amp; Liang, 2018)</ref> when the underlying concept class has certain low-rank structure <ref target=\"#b16\" type=\"bibr\">(Ghorbani et al., 2020;</ref><ref target=\"#b2\" type=\"bibr\">Allen-Zhu &amp; Li, 2020a)</ref>. Another theoretical result in Allen-Zhu &amp; Li (2020b) suggests that low-rank adaptations can be useful for adversarial training. In sum, we believe that our proposed low-rank adaptation update is well-motivated by the literature.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"7\">UNDERSTANDING THE LOW-RANK UPDATES</head><p>Given the empirical advantage of LoRA, we hope to further explain the properties of the low-rank adaptation learned from downstream tasks. Note that the low-rank structure not only lowers the hardware barrier to entry which allows us to run multiple experiments in parallel, but also gives better interpretability of how the update weights are correlated with the pre-trained weights. We focus our study on GPT-3 175B, where we achieved the largest reduction of trainable parameters (up to 10,000Ã—) without adversely affecting task performances.</p><p>We perform a sequence of empirical studies to answer the following questions: 1) Given a parameter budget constraint, which subset of weight matrices in a pre-trained Transformer should we adapt to maximize downstream performance? 2) Is the \"optimal\" adaptation matrix âˆ†W really rankdeficient? If so, what is a good rank to use in practice? 3) What is the connection between âˆ†W and W ? Does âˆ†W highly correlate with W ? How large is âˆ†W comparing to W ?</p><p>We believe that our answers to question (2) and ( <ref type=\"formula\">3</ref>) shed light on the fundamental principles of using pre-trained language models for downstream tasks, which is a critical topic in NLP.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"7.1\">WHICH WEIGHT MATRICES IN TRANSFORMER SHOULD WE APPLY LORA TO?</head><p>Given a limited parameter budget, which types of weights should we adapt with LoRA to obtain the best performance on downstream tasks? As mentioned in Section 4.2, we only consider weight matrices in the self-attention module. We set a parameter budget of 18M (roughly 35MB if stored in FP16) on GPT-3 175B, which corresponds to r = 8 if we adapt one type of attention weights or r = 4 if we adapt two types, for all 96 layers. The result is presented in Table <ref type=\"table\">5</ref>. Table <ref type=\"table\">5</ref>: Validation accuracy on WikiSQL and MultiNLI after applying LoRA to different types of attention weights in GPT-3, given the same number of trainable parameters. Adapting both W q and W v gives the best performance overall. We find the standard deviation across random seeds to be consistent for a given dataset, which we report in the first column.</p><formula xml:id=\"formula_7\"># of Trainable Parameters = 18M Weight Type W q W k W v W o W q , W k W q , W v W q , W k , W v ,</formula><p>Note that putting all the parameters in âˆ†W q or âˆ†W k results in significantly lower performance, while adapting both W q and W v yields the best result. This suggests that even a rank of four captures enough information in âˆ†W such that it is preferable to adapt more weight matrices than adapting a single type of weights with a larger rank.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"7.2\">WHAT IS THE OPTIMAL RANK r FOR LORA?</head><p>We turn our attention to the effect of rank r on model performance. We adapt {W q , W v }, {W q , W k , W v , W c }, and just W q for a comparison.</p><p>Weight Type r = 1 r = 2 r = 4 r = 8 r = 64 Table <ref type=\"table\">6</ref> shows that, surprisingly, LoRA already performs competitively with a very small r (more so for {W q , W v } than just W q ). This suggests the update matrix âˆ†W could have a very small \"intrinsic rank\". <ref type=\"bibr\">6</ref> To further support this finding, we check the overlap of the subspaces learned by different choices of r and by different random seeds. We argue that increasing r does not cover a more meaningful subspace, which suggests that a low-rank adaptation matrix is sufficient.</p><p>Subspace similarity between different r. Given A r=8 and A r=64 which are the learned adaptation matrices with rank r = 8 and 64 using the same pre-trained model, we perform singular value decomposition and obtain the right-singular unitary matrices U Ar=8 and U Ar=64 . <ref type=\"bibr\">7</ref> We hope to answer: how much of the subspace spanned by the top i singular vectors in U Ar=8 (for 1 â‰¤ i â‰¤ 8) is contained in the subspace spanned by top j singular vectors of U Ar=64 (for 1 â‰¤ j â‰¤ 64)? We measure this quantity with a normalized subspace similarity based on the Grassmann distance (See Appendix G for a more formal discussion)</p><formula xml:id=\"formula_8\">Ï†(A r=8 , A r=64 , i, j) = ||U i Ar=8 U j Ar=64 || 2 F min(i, j) âˆˆ [0, 1]<label>(4)</label></formula><p>where U i Ar=8 represents the columns of U Ar=8 corresponding to the top-i singular vectors. Ï†(â€¢) has a range of [0, 1], where 1 represents a complete overlap of subspaces and 0 a complete separation. See Figure <ref target=\"#fig_2\" type=\"figure\">3</ref> for how Ï† changes as we vary i and j. We only look at the 48th layer (out of 96) due to space constraint, but the conclusion holds for other layers as well, as shown in Section H.1. We make an important observation from Figure <ref target=\"#fig_2\" type=\"figure\">3</ref>.</p><p>Directions corresponding to the top singular vector overlap significantly between A r=8 and A r=64 , while others do not. Specifically, âˆ†W v (resp. âˆ†W q ) of A r=8 and âˆ†W v (resp. âˆ†W q ) of A r=64 share a subspace of dimension 1 with normalized similarity &gt; 0.5, providing an explanation of why r = 1 performs quite well in our downstream tasks for GPT-3.</p><p>Since both A r=8 and A r=64 are learned using the same pre-trained model, Figure <ref target=\"#fig_2\" type=\"figure\">3</ref> indicates that the top singular-vector directions of A r=8 and A r=64 are the most useful, while other directions potentially contain mostly random noises accumulated during training. Hence, the adaptation matrix can indeed have a very low rank.</p><p>Subspace similarity between different random seeds. We further confirm this by plotting the normalized subspace similarity between two randomly seeded runs with r = 64, shown in Figure <ref target=\"#fig_3\" type=\"figure\">4</ref>. âˆ†W q appears to have a higher \"intrinsic rank\" than âˆ†W v , since more common singular value directions are learned by both runs for âˆ†W q , which is in line with our empirical observation in Table <ref type=\"table\">6</ref>.</p><p>As a comparison, we also plot two random Gaussian matrices, which do not share any common singular value directions with each other.  how \"large\" is âˆ†W comparing to its corresponding directions in W ? This can shed light on the underlying mechanism for adapting pre-trained language models.</p><p>To answer these questions, we project W onto the r-dimensional subspace of âˆ†W by computing U W V , with U /V being the left/right singular-vector matrix of âˆ†W . Then, we compare the Frobenius norm between U W V F and W F . As a comparison, we also compute U W V F by replacing U, V with the top r singular vectors of W or a random matrix.   <ref type=\"formula\">3</ref>) a random matrix. The weight matrices are taken from the 48th layer of GPT-3.</p><formula xml:id=\"formula_9\">r = 4 r = 64 âˆ†W q W q Random âˆ†W q W q Random ||U W q V || F = 0.</formula><p>We draw several conclusions from Table <ref target=\"#tab_12\" type=\"table\">7</ref>. First, âˆ†W has a stronger correlation with W compared to a random matrix, indicating that âˆ†W amplifies some features that are already in W . Second, instead of repeating the top singular directions of W , âˆ†W only amplifies directions that are not emphasized in W . Third, the amplification factor is rather huge: 21.5 â‰ˆ 6.91/0.32 for r = 4. See Section H.4 for why r = 64 has a smaller amplification factor. We also provide a visualization in Section H.3 for how the correlation changes as we include more top singular directions from W q . This suggests that the low-rank adaptation matrix potentially amplifies the important features for specific downstream tasks that were learned but not emphasized in the general pre-training model.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"8\">CONCLUSION AND FUTURE WORK</head><p>Fine-tuning enormous language models is prohibitively expensive in terms of the hardware required and the storage/switching cost for hosting independent instances for different tasks. We propose LoRA, an efficient adaptation strategy that neither introduces inference latency nor reduces input sequence length while retaining high model quality. Importantly, it allows for quick task-switching when deployed as a service by sharing the vast majority of the model parameters. While we focused on Transformer language models, the proposed principles are generally applicable to any neural networks with dense layers.</p><p>There are many directions for future works. 1) LoRA can be combined with other efficient adaptation methods, potentially providing orthogonal improvement.</p><p>2) The mechanism behind fine-tuning or LoRA is far from clear -how are features learned during pre-training transformed to do well on downstream tasks? We believe that LoRA makes it more tractable to answer this than full fine-tuning. 3) We mostly depend on heuristics to select the weight matrices to apply LoRA to. Are there more principled ways to do it? 4) Finally, the rank-deficiency of âˆ†W suggests that W could be rank-deficient as well, which can also be a source of inspiration for future works. Table <ref type=\"table\">8</ref>: Fine-tuning significantly outperforms few-shot learning on GPT-3 <ref type=\"bibr\">(Brown et al., 2020)</ref>.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>B INFERENCE LATENCY INTRODUCED BY ADAPTER LAYERS</head><p>Adapter layers are external modules added to a pre-trained model in a sequential manner, whereas our proposal, LoRA, can be seen as external modules added in a parallel manner. Consequently, adapter layers must be computed in addition to the base model, inevitably introducing additional latency. While as pointed out in <ref target=\"#b48\" type=\"bibr\">RÃ¼cklÃ© et al. (2020)</ref>, the latency introduced by adapter layers can be mitigated when the model batch size and/or sequence length is large enough to full utilize the hardware parallelism. We confirm their observation with a similar latency study on GPT-2 medium and point out that there are scenarios, notably online inference where the batch size is small, where the added latency can be significant.</p><p>We measure the latency of a single forward pass on an NVIDIA Quadro RTX8000 by averaging over 100 trials. We vary the input batch size, sequence length, and the adapter bottleneck dimension r. We test two adapter designs: the original one by <ref target=\"#b22\" type=\"bibr\">Houlsby et al. (2019)</ref>, which we call Adapter H , and a recent, more efficient variant by <ref target=\"#b33\" type=\"bibr\">Lin et al. (2020)</ref>, which we call Adapter L . See Section 5.1 for more details on the designs. We plot the slow-down in percentage compared to the no-adapter baseline in Figure <ref target=\"#fig_5\" type=\"figure\">5</ref>. The top row shows the result for Adapter H and the bottom row Adapter L . Larger batch size and sequence length help to mitigate the latency, but the slow-down can be as high as over 30% in an online, short-sequence-length scenario. We tweak the colormap for better visibility. DART is an open-domain data-to-text dataset described in <ref target=\"#b39\" type=\"bibr\">Nan et al. (2020)</ref>. DART inputs are structured as sequences of ENTITY -RELATION -ENTITY triples. With 82K examples in total, DART is a significantly larger and more complex data-to-text task compared to E2E. The dataset is released under the MIT license.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>C DATASET DETAILS</head><p>WebNLG is another commonly used dataset for data-to-text evaluation <ref target=\"#b15\" type=\"bibr\">(Gardent et al., 2017)</ref>. With 22K examples in total WebNLG comprises 14 distinct categories, nine of which are seen during training. Since five of the 14 total categories are not seen during training, but are represented in the test set, evaluation is typically broken out by \"seen\" categories (S), \"unseen\" categories (U) and \"all\" (A). Each input example is represented by a sequence of SUBJECT -PROPERTY -OBJECT triples. The dataset is released under Creative Commons BY-NC-SA 4.0.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>D HYPERPARAMETERS USED IN EXPERIMENTS</head><p>D.1 ROBERTA</p><p>We train using AdamW with a linear learning rate decay schedule. We sweep learning rate, number of training epochs, and batch size for LoRA. Following <ref target=\"#b35\" type=\"bibr\">Liu et al. (2019)</ref>, we initialize the LoRA modules to our best MNLI checkpoint when adapting to MRPC, RTE, and STS-B, instead of the usual initialization; the pre-trained model stays frozen for all tasks. We report the median over 5 random seeds; the result for each run is taken from the best epoch. For a fair comparison with the setup in <ref target=\"#b22\" type=\"bibr\">Houlsby et al. (2019)</ref> and <ref target=\"#b42\" type=\"bibr\">Pfeiffer et al. (2021)</ref>, we restrict the model sequence length to 128 and used a fixed batch size for all tasks. Importantly, we start with the pre-trained RoBERTa large model when adapting to MRPC, RTE, and STS-B, instead of a model already adapted to MNLI. The runs with this restricted setup are marked with â€ . See the hyperparameters used in our runs in Table <ref type=\"table\">9</ref>.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>D.2 DEBERTA</head><p>We again train using AdamW with a linear learning rate decay schedule. Following <ref target=\"#b21\" type=\"bibr\">He et al. (2021)</ref>, we tune learning rate, dropout probability, warm-up steps, and batch size. We use the same model sequence length used by <ref target=\"#b21\" type=\"bibr\">(He et al., 2021)</ref> to keep our comparison fair. Following <ref target=\"#b21\" type=\"bibr\">He et al. (2021)</ref>, we initialize the LoRA modules to our best MNLI checkpoint when adapting to MRPC, RTE, and STS-B, instead of the usual initialization; the pre-trained model stays frozen for all tasks. We report the median over 5 random seeds; the result for each run is taken from the best epoch. See the hyperparameters used in our runs in Table <ref target=\"#tab_1\" type=\"table\">10</ref>. Table <ref type=\"table\">9</ref>: The hyperparameters we used for RoBERTa on the GLUE benchmark.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>D.3 GPT-2</head><p>We train all of our GPT-2 models using AdamW <ref target=\"#b36\" type=\"bibr\">(Loshchilov &amp; Hutter, 2017)</ref> with a linear learning rate schedule for 5 epochs. We use the batch size, learning rate, and beam search beam size described in <ref target=\"#b29\" type=\"bibr\">Li &amp; Liang (2021)</ref>. Accordingly, we also tune the above hyperparameters for LoRA. We report the mean over 3 random seeds; the result for each run is taken from the best epoch. The hyperparameters used for LoRA in GPT-2 are listed in Table <ref target=\"#tab_1\" type=\"table\">11</ref>. For those used for other baselines, see <ref target=\"#b29\" type=\"bibr\">Li &amp; Liang (2021)</ref>.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>D.4 GPT-3</head><p>For all GPT-3 experiments, we train using AdamW <ref target=\"#b36\" type=\"bibr\">(Loshchilov &amp; Hutter, 2017)</ref> for 2 epochs with a batch size of 128 samples and a weight decay factor of 0.1. We use a sequence length of 384 for  WikiSQL <ref target=\"#b62\" type=\"bibr\">(Zhong et al., 2017)</ref>, 768 for MNLI <ref target=\"#b56\" type=\"bibr\">(Williams et al., 2018)</ref>, and 2048 for SAMSum <ref target=\"#b17\" type=\"bibr\">(Gliwa et al., 2019)</ref>. We tune learning rate for all method-dataset combinations. See Section D.4 for more details on the hyperparameters used. For prefix-embedding tuning, we find the optimal l p and l i to be 256 and 8, respectively, totalling 3.2M trainable parameters. We use l p = 8 and l i = 8 for prefix-layer tuning with 20.2M trainable parameters to obtain the overall best performance. We present two parameter budgets for LoRA: 4.7M (r q = r v = 1 or r v = 2) and 37.7M (r q = r v = 8 or r q = r k = r v = r o = 2). We report the best validation performance from each run. The training hyperparameters used in our GPT-3 experiments are listed in First of all, LoRA+PE significantly outperforms both LoRA and prefix-embedding tuning on WikiSQL, which indicates that LoRA is somewhat orthogonal to prefix-embedding tuning. On MultiNLI, the combination of LoRA+PE doesn't perform better than LoRA, possibly because LoRA on its own already achieves performance comparable to the human baseline. Secondly, we notice that LoRA+PL performs slightly worse than LoRA even with more trainable parameters. We attribute this to the fact that prefix-layer tuning is very sensitive to the choice of learning rate and thus makes the optimization of LoRA weights more difficult in LoRA+PL.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>F ADDITIONAL EMPIRICAL EXPERIMENTS F.1 ADDITIONAL EXPERIMENTS ON GPT-2</head><p>We also repeat our experiment on DART <ref target=\"#b39\" type=\"bibr\">(Nan et al., 2020)</ref> and WebNLG <ref target=\"#b15\" type=\"bibr\">(Gardent et al., 2017)</ref> following the setup of <ref target=\"#b29\" type=\"bibr\">Li &amp; Liang (2021)</ref>. The result is shown in Table <ref target=\"#tab_1\" type=\"table\">13</ref>. Similar to our result on E2E NLG Challenge, reported in Section 5, LoRA performs better than or at least on-par with prefix-based approaches given the same number of trainable parameters.  </p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>G MEASURING SIMILARITY BETWEEN SUBSPACES</head><p>In this paper we use the measure Ï†(A, B, i, j) = Ïˆ(U i A , U j B ) =</p><formula xml:id=\"formula_10\">U i A U B 2 F</formula><p>min{i,j} to measure the subspace similarity between two column orthonormal matrices U i A âˆˆ R dÃ—i and U j B âˆˆ R dÃ—j , obtained by taking columns of the left singular matrices of A and B. We point out that this similarity is simply a reverse of the standard Projection Metric that measures distance between subspaces <ref target=\"#b19\" type=\"bibr\">Ham &amp; Lee (2008)</ref>.  j 1 2 3 4 5 6 7 8 j 1 2 3 4 5 6 7 8 j (A r = 8 , A r = 64 , i, j)</p><p>Figure <ref type=\"figure\">6</ref>: Normalized subspace similarity between the column vectors of A r=8 and A r=64 for both âˆ†W q and âˆ†W v from the 1st, 32nd, 64th, and 96th layers in a 96-layer Transformer.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>H.4 AMPLIFICATION FACTOR</head><p>One can naturally consider a feature amplification factor as the ratio âˆ†W F U W V F , where U and V are the left-and right-singular matrices of the SVD decomposition of âˆ†W . (Recall U U W V V gives the \"projection\" of W onto the subspace spanned by âˆ†W .) Intuitively, when âˆ†W mostly contains task-specific directions, this quantity measures how much of them are amplified by âˆ†W . As shown in Section 7.3, for r = 4, this amplification factor is as large as 20. In other words, there are (generally speaking) four feature directions in each layer (out of the entire feature space from the pre-trained model W ), that need to be amplified by a very large factor 20, in order to achieve our reported accuracy for the downstream specific task. And, one should expect a very different set of feature directions to be amplified for each different downstream task.</p><p>One may notice, however, for r = 64, this amplification factor is only around 2, meaning that most directions learned in âˆ†W with r = 64 are not being amplified by much. This should not be surprising, and in fact gives evidence (once again) that the intrinsic rank needed to represent the \"task-specific directions\" (thus for model adaptation) is low. In contrast, those directions in the rank-4 version of âˆ†W (corresponding to r = 4) are amplified by a much larger factor 20.  (A r = 64 , A r = 64 , i, j)</p><p>Figure <ref type=\"figure\">7</ref>: Normalized subspace similarity between the column vectors of A r=64 from two randomly seeded runs, for both âˆ†W q and âˆ†W v from the 1st, 32nd, 64th, and 96th layers in a 96-layer Transformer. Table <ref target=\"#tab_1\" type=\"table\">18</ref>: Validation loss and test set metrics on E2E NLG Challenge achieved by LoRA with different rank r using GPT-2 Medium. Unlike on GPT-3 where r = 1 suffices for many tasks, here the performance peaks at r = 16 for validation loss and r = 4 for BLEU, suggesting the GPT-2 Medium has a similar intrinsic rank for adaptation compared to GPT-3 175B. Note that some of our hyperparameters are tuned on r = 4, which matches the parameter count of another baseline, and thus might not be optimal for other choices of r. i (W q , A r = 4 , i, j) j W q (W q , A r = 8 , i, j) j (W q , A r = 64 , i, j) j Random (W q , A rand , i, j)</p><p>Figure <ref type=\"figure\">8</ref>: Normalized subspace similarity between the singular directions of W q and those of âˆ†W q with varying r and a random baseline. âˆ†W q amplifies directions that are important but not emphasized in W . âˆ†W with a larger r tends to pick up more directions that are already emphasized in W .</p></div><figure xml:id=\"fig_0\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: GPT-3 175B validation accuracy vs. number of trainable parameters of several adaptation methods on WikiSQL and MNLI-matched. LoRA exhibits better scalability and task performance. See Section F.2 for more details on the plotted data points.</figDesc></figure>\n",
       " <figure xml:id=\"fig_1\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>Fine-tuning retrains a model pre-trained on general domains to a specific task Devlin et al. (2019b); Radford et al. (a). Variants of it include learning just a subset of the parameters Devlin et al. (2019b); Collobert &amp; Weston (</figDesc></figure>\n",
       " <figure xml:id=\"fig_2\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Subspace similarity between column vectors of A r=8 and A r=64 for both âˆ†W q and âˆ†W v . The third and the fourth figures zoom in on the lower-left triangle in the first two figures. The top directions in r = 8 are included in r = 64, and vice versa.</figDesc></figure>\n",
       " <figure xml:id=\"fig_3\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Left and Middle: Normalized subspace similarity between the column vectors of A r=64 from two random seeds, for both âˆ†W q and âˆ†W v in the 48-th layer. Right: the same heat-map between the column vectors of two random Gaussian matrices. See Section H.1 for other layers.</figDesc></figure>\n",
       " <figure xml:id=\"fig_5\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Percentage slow-down of inference latency compared to the no-adapter (r = 0) baseline. The top row shows the result for Adapter H and the bottom row Adapter L . Larger batch size and sequence length help to mitigate the latency, but the slow-down can be as high as over 30% in an online, short-sequence-length scenario. We tweak the colormap for better visibility.</figDesc></figure>\n",
       " <figure xml:id=\"fig_6\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>GLUE</head><label/><figDesc>Benchmark is a wide-ranging collection of natural language understanding tasks. It includes MNLI (inference,<ref target=\"#b56\" type=\"bibr\">Williams et al. (2018)</ref>), SST-2 (sentiment analysis,<ref target=\"#b51\" type=\"bibr\">Socher et al. (2013)</ref>), MRPC (paraphrase detection,<ref target=\"#b14\" type=\"bibr\">Dolan &amp; Brockett (2005)</ref>), CoLA (linguistic acceptability,<ref target=\"#b55\" type=\"bibr\">Warstadt et al. (2018)</ref>), QNLI (inference,<ref target=\"#b46\" type=\"bibr\">Rajpurkar et al. (2018)</ref>), QQP 8 (question-answering), RTE (inference), and STS-B (textual similarity,Cer et al. (2017)). The broad coverage makes GLUE benchmark a standard metric to evaluate NLU models such as RoBERTa and DeBERTa. The individual datasets are released under different permissive licenses.WikiSQL is introduced in<ref target=\"#b62\" type=\"bibr\">Zhong et al. (2017)</ref> and contains 56, 355/8, 421 training/validation examples. The task is to generate SQL queries from natural language questions and table schemata. We encode context as x = {table schema, query} and target as y = {SQL}. The dataset is release under the BSD 3-Clause License.SAMSum is introduced in<ref target=\"#b17\" type=\"bibr\">Gliwa et al. (2019)</ref> and contains 14, 732/819 training/test examples. It consists of staged chat conversations between two people and corresponding abstractive summaries written by linguists. We encode context as \"\\n\" concatenated utterances followed by a \"\\n\\n\", and target as y = {summary}. The dataset is released under the non-commercial licence: Creative Commons BY-NC-ND 4.0.E2E NLG Challenge was first introduced in<ref target=\"#b40\" type=\"bibr\">Novikova et al. (2017)</ref> as a dataset for training end-toend, data-driven natural language generation systems and is commonly used for data-to-text evaluation. The E2E dataset consists of roughly 42, 000 training, 4, 600 validation, and 4, 600 test examples from the restaurant domain. Each source table used as input can have multiple references. Each sample input (x, y) consists of a sequence of slot-value pairs, along with a corresponding natural language reference text. The dataset is released under Creative Commons BY-NC-SA 4.0.</figDesc></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_1\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 1 :</head><label>1</label><figDesc>Infernece latency of a single forward pass in GPT-2 medium measured in milliseconds, averaged over 100 trials. We use an NVIDIA Quadro RTX8000. \"|Î˜|\" denotes the number of trainable parameters in adapter layers. Adapter L and Adapter H are two variants of adapter tuning, which we describe in Section 5.1. The inference latency introduced by adapter layers can be significant in an online, short-sequence-length scenario. See the full study in Appendix B.</figDesc><table/></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_2\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>3M 87.1Â±.0 94.2Â±.1 88.5Â±1.1 60.8Â±.4 93.1Â±.1 90.2Â±.0 71.5Â±2.7 89.7Â±.3 84.4 RoBbase (Adpt D )* 0.9M 87.3Â±.1 94.7Â±.3 88.4Â±.1 62.6Â±.9 93.0Â±.2 90.6Â±.0 75.9Â±2.2 90.3Â±.1 85.4 RoBbase (LoRA) 0.3M 87.5Â±.3 95.1 Â±.2 89.7Â±.7 63.4Â±1.2 93.3 Â±.3 90.8Â±.1 86.6Â±.7 91.5 Â±.2 87.2 8M 90.6Â±.2 96.2Â±.5 90.9Â±1.2 68.2Â±1.9 94.9Â±.3 91.6Â±.1 87.4Â±2.5 92.6Â±.2 89.0 RoBlarge (Adpt P ) â€  3.0M 90.2Â±.3 96.1Â±.3 90.2Â±.7 68.3Â±1.0 94.8Â±.2 91.9Â±.1 83.8Â±2.9 92.1Â±.7 88.4 RoBlarge (Adpt P ) â€  0.8M 90.5Â±.3 96.6Â±.2 89.7Â±1.2 67.8Â±2.5 94.8Â±.3 91.7Â±.2 80.1Â±2.9 91.9Â±.8M 90.6Â±.2 96.2Â±.5 90.2Â±1.0 68.2Â±1.9 94.8Â±.3 91.6Â±.2 85.2Â±1.1 92.3Â±.5 88.6 7M 91.9Â±.2 96.9Â±.2 92.6Â±.6 72.4Â±1.1 96.0Â±.1 92.9Â±.1 94.9Â±.4 93.0Â±.2 91.3</figDesc><table><row><cell cols=\"2\">Model &amp; Method # Trainable</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell/><cell cols=\"6\">Parameters MNLI SST-2 MRPC CoLA QNLI QQP</cell><cell cols=\"2\">RTE STS-B Avg.</cell></row><row><cell>RoBbase (FT)*</cell><cell>125.0M 87.6</cell><cell>94.8</cell><cell>90.2</cell><cell>63.6</cell><cell>92.8</cell><cell>91.9</cell><cell>78.7</cell><cell>91.2 86.4</cell></row><row><cell>RoBbase (BitFit)*</cell><cell>0.1M 84.7</cell><cell>93.7</cell><cell>92.7</cell><cell>62.0</cell><cell>91.8</cell><cell>84.0</cell><cell>81.5</cell><cell>90.8 85.2</cell></row><row><cell cols=\"2\">RoBbase (Adpt D )* 0.RoBlarge (FT)* 355.0M 90.2</cell><cell>96.4</cell><cell>90.9</cell><cell>68.0</cell><cell>94.7</cell><cell>92.2</cell><cell>86.6</cell><cell>92.4 88.9</cell></row><row><cell>RoBlarge (LoRA)</cell><cell cols=\"8\">0.4 87.9</cell></row><row><cell>RoBlarge (Adpt H ) â€ </cell><cell cols=\"8\">6.0M 89.9Â±.5 96.2Â±.3 88.7Â±2.9 66.5Â±4.4 94.7Â±.2 92.1Â±.1 83.4Â±1.1 91.0Â±1.7 87.8</cell></row><row><cell>RoBlarge (Adpt H ) â€ </cell><cell cols=\"8\">0.8M 90.3Â±.3 96.3Â±.5 87.7Â±1.7 66.3Â±2.0 94.7Â±.2 91.5Â±.1 72.9Â±2.9 91.5Â±.5 86.4</cell></row><row><cell cols=\"2\">RoBlarge (LoRA) â€  0.DeBXXL (FT)* 1500.0M 91.8</cell><cell>97.2</cell><cell>92.0</cell><cell>72.0</cell><cell>96.0</cell><cell>92.7</cell><cell>93.9</cell><cell>92.9 91.1</cell></row><row><cell>DeBXXL (LoRA)</cell><cell>4.</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_3\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 2 :</head><label>2</label><figDesc/><table/><note>RoBERTa base , RoBERTa large , and DeBERTa XXL with different adaptation methods on the GLUE benchmark. We report the overall (matched and mismatched) accuracy for MNLI, Matthew's correlation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better for all metrics. * indicates numbers published in prior works. â€  indicates runs configured in a setup similar to<ref target=\"#b22\" type=\"bibr\">Houlsby et al. (2019)</ref> for a fair comparison.</note></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_4\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>whereL LoRA is the number of weight matrices we apply LoRA to.</figDesc><table><row><cell>Model &amp; Method</cell><cell># Trainable</cell><cell/><cell cols=\"3\">E2E NLG Challenge</cell><cell/></row><row><cell/><cell cols=\"2\">Parameters BLEU</cell><cell>NIST</cell><cell>MET</cell><cell>ROUGE-L</cell><cell>CIDEr</cell></row><row><cell>GPT-2 M (FT)*</cell><cell>354.92M</cell><cell>68.2</cell><cell>8.62</cell><cell>46.2</cell><cell>71.0</cell><cell>2.47</cell></row><row><cell>GPT-2 M (Adapter L )*</cell><cell>0.37M</cell><cell>66.3</cell><cell>8.41</cell><cell>45.0</cell><cell>69.8</cell><cell>2.40</cell></row><row><cell>GPT-2 M (Adapter L )*</cell><cell>11.09M</cell><cell>68.9</cell><cell>8.71</cell><cell>46.1</cell><cell>71.3</cell><cell>2.47</cell></row><row><cell>GPT-2 M (Adapter H )</cell><cell cols=\"4\">11.09M 67.3 Â±.6 8.50 Â±.07 46.0 Â±.2</cell><cell>70.7 Â±.2</cell><cell>2.44 Â±.01</cell></row><row><cell>GPT-2 M (FT Top2 )*</cell><cell>25.19M</cell><cell>68.1</cell><cell>8.59</cell><cell>46.0</cell><cell>70.8</cell><cell>2.41</cell></row><row><cell>GPT-2 M (PreLayer)*</cell><cell>0.35M</cell><cell>69.7</cell><cell>8.81</cell><cell>46.1</cell><cell>71.4</cell><cell>2.49</cell></row><row><cell>GPT-2 M (LoRA)</cell><cell cols=\"4\">0.35M 70.4 Â±.1 8.85 Â±.02 46.8 Â±.2</cell><cell>71.8 Â±.1</cell><cell>2.53 Â±.02</cell></row><row><cell>GPT-2 L (FT)*</cell><cell>774.03M</cell><cell>68.5</cell><cell>8.78</cell><cell>46.0</cell><cell>69.9</cell><cell>2.45</cell></row><row><cell>GPT-2 L (Adapter L )</cell><cell cols=\"4\">0.88M 69.1 Â±.1 8.68 Â±.03 46.3 Â±.0</cell><cell>71.4 Â±.2</cell><cell>2.49 Â±.0</cell></row><row><cell>GPT-2 L (Adapter L )</cell><cell cols=\"4\">23.00M 68.9 Â±.3 8.70 Â±.04 46.1 Â±.1</cell><cell>71.3 Â±.2</cell><cell>2.45 Â±.02</cell></row><row><cell>GPT-2 L (PreLayer)*</cell><cell>0.77M</cell><cell>70.3</cell><cell>8.85</cell><cell>46.2</cell><cell>71.7</cell><cell>2.47</cell></row><row><cell>GPT-2 L (LoRA)</cell><cell cols=\"4\">0.77M 70.4 Â±.1 8.89 Â±.02 46.8 Â±.2</cell><cell>72.0 Â±.2</cell><cell>2.47 Â±.02</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_5\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 3 :</head><label>3</label><figDesc> and large (L) with different adaptation methods on the E2E NLG Challenge. For all metrics, higher is better. LoRA outperforms several baselines with comparable or fewer trainable parameters. Confidence intervals are shown for experiments we ran. * indicates numbers published in prior works.</figDesc><table/></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_6\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>LoRA performs better than prior approaches, including full fine-tuning. The results on WikiSQL have a fluctuation around Â±0.5%, MNLI-m around Â±0.1%, and SAMSum around Â±0.2/Â±0.2/Â±0.1 for the three metrics.</figDesc><table><row><cell>Model&amp;Method</cell><cell cols=\"3\"># Trainable WikiSQL MNLI-m Parameters Acc. (%) Acc. (%)</cell><cell>SAMSum R1/R2/RL</cell></row><row><cell>GPT-3 (FT)</cell><cell>175,255.8M</cell><cell>73.8</cell><cell>89.5</cell><cell>52.0/28.0/44.5</cell></row><row><cell>GPT-3 (BitFit)</cell><cell>14.2M</cell><cell>71.3</cell><cell>91.0</cell><cell>51.3/27.4/43.5</cell></row><row><cell>GPT-3 (PreEmbed)</cell><cell>3.2M</cell><cell>63.1</cell><cell>88.6</cell><cell>48.3/24.2/40.5</cell></row><row><cell>GPT-3 (PreLayer)</cell><cell>20.2M</cell><cell>70.1</cell><cell>89.5</cell><cell>50.8/27.3/43.5</cell></row><row><cell>GPT-3 (Adapter H )</cell><cell>7.1M</cell><cell>71.9</cell><cell>89.8</cell><cell>53.0/28.9/44.8</cell></row><row><cell>GPT-3 (Adapter H )</cell><cell>40.1M</cell><cell>73.2</cell><cell>91.5</cell><cell>53.2/29.0/45.1</cell></row><row><cell>GPT-3 (LoRA)</cell><cell>4.7M</cell><cell>73.4</cell><cell>91.7</cell><cell>53.8/29.8/45.9</cell></row><row><cell>GPT-3 (LoRA)</cell><cell>37.7M</cell><cell>74.0</cell><cell>91.6</cell><cell>53.4/29.2/45.1</cell></row><row><cell cols=\"5\">Table 4: Performance of different adaptation methods on GPT-3 175B. We report the logical form</cell></row><row><cell cols=\"5\">validation accuracy on WikiSQL, validation accuracy on MultiNLI-matched, and Rouge-1/2/L on</cell></row><row><cell>SAMSum.</cell><cell/><cell/><cell/><cell/></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_9\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>Validation accuracy on WikiSQL and MultiNLI with different rank r. To our surprise, a rank as small as one suffices for adapting both W q and W v on these datasets while training W q alone needs a larger r. We conduct a similar experiment on GPT-2 in Section H.2.</figDesc><table><row><cell>WikiSQL(Â±0.5%)</cell><cell>W q W q , W v</cell><cell>68.8 73.4</cell><cell>69.6 73.3</cell><cell>70.5 73.7</cell><cell>70.4 73.8</cell><cell>70.0 73.5</cell></row><row><cell/><cell>W q , W k , W v , W o</cell><cell>74.1</cell><cell>73.7</cell><cell>74.0</cell><cell>74.0</cell><cell>73.9</cell></row><row><cell/><cell>W q</cell><cell>90.7</cell><cell>90.9</cell><cell>91.1</cell><cell>90.7</cell><cell>90.7</cell></row><row><cell>MultiNLI (Â±0.1%)</cell><cell>W q , W v</cell><cell>91.3</cell><cell>91.4</cell><cell>91.3</cell><cell>91.6</cell><cell>91.4</cell></row><row><cell/><cell>W q , W k , W v , W o</cell><cell>91.2</cell><cell>91.7</cell><cell>91.7</cell><cell>91.5</cell><cell>91.4</cell></row><row><cell>Table 6:</cell><cell/><cell/><cell/><cell/><cell/><cell/></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_12\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 7 :</head><label>7</label><figDesc>The Frobenius norm of U W q V where U and V are the left/right top r singular vector directions of either (1) âˆ†W q , (2) W</figDesc><table/><note>q , or (</note></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_15\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 10 :</head><label>10</label><figDesc>The hyperparameters for DeBERTa XXL on tasks included in the GLUE benchmark.</figDesc><table><row><cell>Dataset</cell><cell cols=\"3\">E2E WebNLG DART</cell></row><row><cell/><cell/><cell>Training</cell><cell/></row><row><cell>Optimizer</cell><cell/><cell>AdamW</cell><cell/></row><row><cell>Weight Decay</cell><cell>0.01</cell><cell>0.01</cell><cell>0.0</cell></row><row><cell>Dropout Prob</cell><cell>0.1</cell><cell>0.1</cell><cell>0.0</cell></row><row><cell>Batch Size</cell><cell/><cell>8</cell><cell/></row><row><cell># Epoch</cell><cell/><cell>5</cell><cell/></row><row><cell>Warmup Steps</cell><cell/><cell>500</cell><cell/></row><row><cell>Learning Rate Schedule</cell><cell/><cell>Linear</cell><cell/></row><row><cell>Label Smooth</cell><cell>0.1</cell><cell>0.1</cell><cell>0.0</cell></row><row><cell>Learning Rate</cell><cell/><cell>0.0002</cell><cell/></row><row><cell>Adaptation</cell><cell/><cell>r q = r v = 4</cell><cell/></row><row><cell>LoRA Î±</cell><cell/><cell>32</cell><cell/></row><row><cell/><cell/><cell>Inference</cell><cell/></row><row><cell>Beam Size</cell><cell/><cell>10</cell><cell/></row><row><cell>Length Penalty</cell><cell>0.9</cell><cell>0.8</cell><cell>0.8</cell></row><row><cell>no repeat ngram size</cell><cell/><cell>4</cell><cell/></row><row><cell cols=\"4\">Table 11: The hyperparameters for GPT-2 LoRA on E2E, WebNLG and DART.</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_16\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 12 .</head><label>12</label><figDesc>E COMBINING LORA WITH PREFIX TUNINGLoRA can be naturally combined with existing prefix-based approaches. In this section, we evaluate two combinations of LoRA and variants of prefix-tuning on WikiSQL and MNLI. The training hyperparameters used for different GPT-3 adaption methods. We use the same hyperparameters for all datasets after tuning learning rate. rally, we replace them after every Transformer block with an input agnostic vector. Thus, both the embeddings and subsequent Transformer block activations are treated as trainable parameters. For more on prefix-layer tuning, see Section 5.1.In Table15, we show the evaluation results of LoRA+PE and LoRA+PL on WikiSQL and MultiNLI.</figDesc><table><row><cell cols=\"3\">Hyperparameters Fine-Tune PreEmbed PreLayer</cell><cell>BitFit</cell><cell>Adapter H</cell><cell>LoRA</cell></row><row><cell>Optimizer</cell><cell/><cell cols=\"2\">AdamW</cell></row><row><cell>Batch Size</cell><cell/><cell>128</cell><cell/></row><row><cell># Epoch</cell><cell/><cell>2</cell><cell/></row><row><cell>Warmup Tokens</cell><cell/><cell>250,000</cell><cell/></row><row><cell>LR Schedule</cell><cell/><cell>Linear</cell><cell/></row><row><cell>Learning Rate</cell><cell>5.00E-06</cell><cell cols=\"3\">5.00E-04 1.00E-04 1.6E-03 1.00E-04 2.00E-04</cell></row><row><cell>Table 12:</cell><cell/><cell/><cell/></row></table><note>LoRA+PrefixEmbed (LoRA+PE) combines LoRA with prefix-embedding tuning, where we insert l p + l i special tokens whose embeddings are treated as trainable parameters. For more on prefixembedding tuning, see Section 5.1.LoRA+PrefixLayer (LoRA+PL) combines LoRA with prefix-layer tuning. We also insert l p + l i special tokens; however, instead of letting the hidden representations of these tokens evolve natu-</note></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_18\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 13 :</head><label>13</label><figDesc>GPT-2 with different adaptation methods on DART. The variances of MET and TER are less than 0.01 for all adaption approaches.Â±.4 62.1 Â±.2 55.3 Â±.2 .38 .44 .41 .46 .33 .39 88M) 49.8 Â±.0 61.1 Â±.0 56.0 Â±.0 .38 .43 .41 .44 .35 .39 Adapter L (23M) 49.2 Â±.1 64.7 Â±.2 57.7 Â±.1 .39 .46 .43 .46 .33 .39 Â±.3 64.0 Â±.3 57.0 Â±.1 .39 .45 .42 .45 .32 .38 Table 14: GPT-2 with different adaptation methods on WebNLG. The variances of MET and TER are less than 0.01 for all the experiments we ran. \"U\" indicates unseen categories, \"S\" indicates seen categories, and \"A\" indicates all categories in the test set of WebNLG. F.2 ADDITIONAL EXPERIMENTS ON GPT-3 We present additional runs on GPT-3 with different adaptation methods in Table 15. The focus is on identifying the trade-off between performance and the number of trainable parameters. F.3 LOW-DATA REGIME To evaluate the performance of different adaptation approaches in the low-data regime. we randomly sample 100, 1k and 10k training examples from the full training set of MNLI to form the low-data MNLI-n tasks. In Table 16, we show the performance of different adaptation approaches on MNLIn. To our surprise, PrefixEmbed and PrefixLayer performs very poorly on MNLI-100 dataset, with PrefixEmbed performing only slightly better than random chance (37.6% vs. 33.3%). PrefixLayer performs better than PrefixEmbed but is still significantly worse than Fine-Tune or LoRA on MNLI-100. The gap between prefix-based approaches and LoRA/Fine-tuning becomes smaller as we increase the number of training examples, which might suggest that prefix-based approaches are not suitable for low-data tasks in GPT-3. LoRA achieves better performance than fine-tuning on both MNLI-100 and MNLI-Full, and comparable results on MNLI-1k and MNLI-10K considering the (Â±0.3) variance due to random seeds. The training hyperparameters of different adaptation approaches on MNLI-n are reported in Table 17. We use a smaller learning rate for PrefixLayer on the MNLI-100 set, as the training loss does not decrease with a larger learning rate.</figDesc><table><row><cell>Method</cell><cell/><cell/><cell/><cell>WebNLG</cell><cell/><cell/><cell/><cell/></row><row><cell/><cell/><cell>BLEUâ†‘</cell><cell/><cell cols=\"2\">METâ†‘</cell><cell/><cell/><cell>TERâ†“</cell></row><row><cell/><cell>U</cell><cell>S</cell><cell>A</cell><cell>U</cell><cell>S</cell><cell>A</cell><cell>U</cell><cell>S</cell><cell>A</cell></row><row><cell/><cell/><cell/><cell cols=\"3\">GPT-2 Medium</cell><cell/><cell/><cell/></row><row><cell>Fine-Tune (354M)</cell><cell>27.7</cell><cell>64.2</cell><cell>46.5</cell><cell cols=\"6\">.30 .45 .38 .76 .33 .53</cell></row><row><cell>Adapter L (0.37M)</cell><cell>45.1</cell><cell>54.5</cell><cell>50.2</cell><cell cols=\"6\">.36 .39 .38 .46 .40 .43</cell></row><row><cell>Adapter L (11M)</cell><cell>48.3</cell><cell>60.4</cell><cell>54.9</cell><cell cols=\"6\">.38 .43 .41 .45 .35 .39</cell></row><row><cell>FT Top2 (24M)</cell><cell>18.9</cell><cell>53.6</cell><cell>36.0</cell><cell cols=\"6\">.23 .38 .31 .99 .49 .72</cell></row><row><cell>Prefix (0.35M)</cell><cell>45.6</cell><cell>62.9</cell><cell>55.1</cell><cell cols=\"6\">.38 .44 .41 .49 .35 .40</cell></row><row><cell>LoRA (0.35M)</cell><cell cols=\"5\">46.7 GPT-2 Large</cell><cell/><cell/><cell/></row><row><cell>Fine-Tune (774M)</cell><cell>43.1</cell><cell>65.3</cell><cell>55.5</cell><cell cols=\"6\">.38 .46 .42 .53 .33 .42</cell></row><row><cell>Adapter L (0.Prefix (0.77M)</cell><cell>47.7</cell><cell>63.4</cell><cell>56.3</cell><cell cols=\"6\">.39 .45 .42 .48 .34 .40</cell></row><row><cell>LoRA (0.77M)</cell><cell>48.4</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row></table></figure>\n",
       " <note n=\"2\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">They represent a negligible number of parameters compared to weights.3  An inevitability when adapting to hard tasks.</note>\n",
       " <note n=\"4\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">We still need the 350GB model during deployment; however, storing 100 adapted models only requires 350GB + 35MB * 100 â‰ˆ 354GB as opposed to 100 * 350GB â‰ˆ 35TB.5 For GPT-3 175B, the training throughput for full fine-tuning is 32.5 tokens/s per V100 GPU; with the same number of weight shards for model parallelism, the throughput is 43.1 tokens/s per V100 GPU for LoRA.</note>\n",
       " <note n=\"6\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">However, we do not expect a small r to work for every task or dataset. Consider the following thought experiment: if the downstream task were in a different language than the one used for pre-training, retraining the entire model (similar to LoRA with r = d model ) could certainly outperform LoRA with a small r.</note>\n",
       " <note n=\"7\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">Note that a similar analysis can be carried out with B and the left-singular unitary matrices -we stick with A for our experiments.</note>\n",
       " <note n=\"8\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs</note>\n",
       " </body>\n",
       " <back>\n",
       " <div type=\"annex\">\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A LARGE LANGUAGE MODELS STILL NEED PARAMETER UPDATES</head><p>Few-shot learning, or prompt engineering, is very advantageous when we only have a handful of training samples. However, in practice, we can often afford to curate a few thousand or more training examples for performance-sensitive applications. As shown in Table <ref type=\"table\">8</ref>, fine-tuning improves the model performance drastically compared to few-shot learning on datasets large and small. We take the GPT-3 few-shot result on RTE from the GPT-3 paper <ref type=\"bibr\">(Brown et al., 2020</ref>     where our similarity is defined as:</p><p>This similarity satisfies that if U i A and U j B share the same column span, then Ï†(A, B, i, j) = 1. If they are completely orthogonal, then Ï†(A, B, i, j) = 0. Otherwise, Ï†(A, B, i, j) âˆˆ (0, 1).</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>H ADDITIONAL EXPERIMENTS ON LOW-RANK MATRICES</head><p>We present additional results from our investigation into the low-rank update matrices.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>H.1 CORRELATION BETWEEN LORA MODULES</head><p>See Figure <ref type=\"figure\">6</ref> and Figure <ref type=\"figure\">7</ref> for how the results presented in Figure <ref type=\"figure\">3</ref> and Figure <ref type=\"figure\">4</ref> generalize to other layers.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>H.2 EFFECT OF r ON GPT-2</head><p>We repeat our experiment on the effect of r (Section 7.2) in GPT-2. Using the E2E NLG Challenge dataset as an example, we report the validation loss and test metrics achieved by different choices of r after training for 26,000 steps. We present our result in Table <ref type=\"table\">18</ref>. The optimal rank for GPT-2 Medium is between 4 and 16 depending on the metric used, which is similar to that for GPT-3 175B. Note that the relationship between model size and the optimal rank for adaptation is still an open question.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>H.3 CORRELATION BETWEEN W AND âˆ†W</head><p>See Figure <ref type=\"figure\">8</ref> for the normalized subspace similarity between W and âˆ†W with varying r.</p><p>Note again that âˆ†W does not contain the top singular directions of W , since the similarity between the top 4 directions in âˆ†W and the top-10% of those in W barely exceeds 0.2. This gives evidence that âˆ†W contains those \"task-specific\" directions that are otherwise not emphasized in W .</p><p>An interesting next question to answer, is how \"strong\" do we need to amplify those task-specific directions, in order for the model adaptation to work well?</p></div> </div>\n",
       " <div type=\"references\">\n",
       " <listBibl>\n",
       " <biblStruct xml:id=\"b0\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Armen</forename><surname>Aghajanyan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sonal</forename><surname>Gupta</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2012.13255</idno>\n",
       " <ptr target=\"http://arxiv.org/abs/2012.13255\"/>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020-12\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b1\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">What Can ResNet Learn Efficiently, Going Beyond Kernels?</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zeyuan</forename><surname>Allen-Zhu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yuanzhi</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"http://arxiv.org/abs/1905.10337\"/>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NeurIPS</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b2\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Backward feature correction: How deep learning performs deep learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zeyuan</forename><surname>Allen-Zhu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yuanzhi</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2001.04413</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b3\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Feature purification: How adversarial training performs robust deep learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zeyuan</forename><surname>Allen-Zhu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yuanzhi</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2005.10190</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b4\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A convergence theory for deep learning via overparameterization</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zeyuan</forename><surname>Allen-Zhu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yuanzhi</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhao</forename><surname>Song</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"http://arxiv.org/abs/1811.03962\"/>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ICML</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b5\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jimmy</forename><forename type=\"middle\">Lei</forename><surname>Ba</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jamie</forename><forename type=\"middle\">Ryan</forename><surname>Kiros</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Geoffrey</forename><forename type=\"middle\">E</forename><surname>Hinton</surname></persName>\n",
       " </author>\n",
       " <title level=\"m\">Layer normalization</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b6\">\n",
       " <monogr>\n",
       " <title/>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tom</forename><forename type=\"middle\">B</forename><surname>Brown</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Benjamin</forename><surname>Mann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nick</forename><surname>Ryder</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Melanie</forename><surname>Subbiah</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jared</forename><surname>Kaplan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Prafulla</forename><surname>Dhariwal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Arvind</forename><surname>Neelakantan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pranav</forename><surname>Shyam</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Girish</forename><surname>Sastry</surname></persName>\n",
       " </author>\n",
       " <editor>Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M</editor>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b7\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeffrey</forename><surname>Ziegler</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Clemens</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><surname>Winter</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mark</forename><surname>Hesse</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Eric</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mateusz</forename><surname>Sigler</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Scott</forename><surname>Litwin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Benjamin</forename><surname>Gray</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jack</forename><surname>Chess</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><surname>Clark</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sam</forename><surname>Berner</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alec</forename><surname>Mccandlish</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2005.14165</idno>\n",
       " <ptr target=\"http://arxiv.org/abs/2005.14165\"/>\n",
       " <title level=\"m\">Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020-07\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b8\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A singular value thresholding algorithm for matrix completion</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jian-Feng</forename><surname>Cai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Emmanuel</forename><forename type=\"middle\">J</forename><surname>CandÃ¨s</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zuowei</forename><surname>Shen</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">SIAM Journal on optimization</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">20</biblScope>\n",
       " <biblScope unit=\"issue\">4</biblScope>\n",
       " <biblScope from=\"1956\" to=\"1982\" unit=\"page\"/>\n",
       " <date type=\"published\" when=\"2010\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b9\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Daniel</forename><surname>Cer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mona</forename><surname>Diab</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Eneko</forename><surname>Agirre</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Inigo</forename><surname>Lopez-Gazpio</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lucia</forename><surname>Specia</surname></persName>\n",
       " </author>\n",
       " <idno type=\"DOI\">10.18653/v1/S17-2001</idno>\n",
       " <idno>doi: 10</idno>\n",
       " <ptr target=\"http://dx.doi.org/10.18653/v1/S17-2001\"/>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 11th International Workshop on Semantic Evaluation</title>\n",
       " <meeting>the 11th International Workshop on Semantic Evaluation</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2001\"/>\n",
       " <biblScope unit=\"page\">2017</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b10\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A unified architecture for natural language processing: deep neural networks with multitask learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ronan</forename><surname>Collobert</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jason</forename><surname>Weston</surname></persName>\n",
       " </author>\n",
       " <idno type=\"DOI\">10.1145/1390156.1390177</idno>\n",
       " <ptr target=\"https://doi.org/10.1145/1390156.1390177\"/>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 25th international conference on Machine learning, ICML '08</title>\n",
       " <meeting>the 25th international conference on Machine learning, ICML '08<address><addrLine>New York, NY, USA</addrLine></address></meeting>\n",
       " <imprint>\n",
       " <publisher>Association for Computing Machinery</publisher>\n",
       " <date type=\"published\" when=\"2008-07\"/>\n",
       " <biblScope from=\"160\" to=\"167\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b11\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Predicting parameters in deep learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Misha</forename><surname>Denil</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Babak</forename><surname>Shakibi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Laurent</forename><surname>Dinh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Marc'aurelio</forename><surname>Ranzato</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nando</forename><surname>De Freitas</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b12\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Bert: Pre-training of deep bidirectional transformers for language understanding</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jacob</forename><surname>Devlin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ming-Wei</forename><surname>Chang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kristina</forename><surname>Toutanova</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b13\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jacob</forename><surname>Devlin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ming-Wei</forename><surname>Chang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kristina</forename><forename type=\"middle\">Toutanova</forename><surname>Bert</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1810.04805</idno>\n",
       " <idno>arXiv: 1810.04805</idno>\n",
       " <ptr target=\"http://arxiv.org/abs/1810.04805\"/>\n",
       " <title level=\"m\">Pre-training of Deep Bidirectional Transformers for Language Understanding</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019-05\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b14\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Automatically constructing a corpus of sentential paraphrases</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>William</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chris</forename><surname>Dolan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Brockett</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"https://aclanthology.org/I05-5002\"/>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</title>\n",
       " <meeting>the Third International Workshop on Paraphrasing (IWP2005)</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2005\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b15\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">The webnlg challenge: Generating text from rdf data</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Claire</forename><surname>Gardent</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Anastasia</forename><surname>Shimorina</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Shashi</forename><surname>Narayan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Laura</forename><surname>Perez-Beltrachini</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 10th International Conference on Natural Language Generation</title>\n",
       " <meeting>the 10th International Conference on Natural Language Generation</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " <biblScope from=\"124\" to=\"133\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b16\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Behrooz</forename><surname>Ghorbani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Song</forename><surname>Mei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Theodor</forename><surname>Misiakiewicz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrea</forename><surname>Montanari</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2006.13409</idno>\n",
       " <title level=\"m\">When do neural networks outperform kernel methods</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b17\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Samsum corpus: A humanannotated dialogue dataset for abstractive summarization. CoRR, abs</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bogdan</forename><surname>Gliwa</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Iwona</forename><surname>Mochol</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Maciej</forename><surname>Biesek</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Aleksander</forename><surname>Wawer</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"http://arxiv.org/abs/1911.12237\"/>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"1911\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b18\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A literature survey of low-rank tensor approximation techniques</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lars</forename><surname>Grasedyck</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Daniel</forename><surname>Kressner</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christine</forename><surname>Tobler</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">GAMM-Mitteilungen</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">36</biblScope>\n",
       " <biblScope unit=\"issue\">1</biblScope>\n",
       " <biblScope from=\"53\" to=\"78\" unit=\"page\"/>\n",
       " <date type=\"published\" when=\"2013\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b19\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Grassmann discriminant analysis: a unifying view on subspace-based learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jihun</forename><surname>Ham</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Daniel</forename><forename type=\"middle\">D</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <idno type=\"DOI\">10.1145/1390156.1390204</idno>\n",
       " <ptr target=\"https://doi.org/10.1145/1390156\"/>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ICML</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2008\"/>\n",
       " <biblScope from=\"376\" to=\"383\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b20\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Karen</forename><surname>Hambardzumyan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hrant</forename><surname>Khachatrian</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jonathan</forename></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2101.00121</idno>\n",
       " <idno>arXiv: 2101.00121</idno>\n",
       " <ptr target=\"http://arxiv.org/abs/2101.00121\"/>\n",
       " <title level=\"m\">WARP: Word-level Adversarial ReProgramming</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020-12\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b21\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Deberta: Decoding-enhanced bert with disentangled attention</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pengcheng</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xiaodong</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jianfeng</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Weizhu</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b22\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Neil</forename><surname>Houlsby</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrei</forename><surname>Giurgiu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Stanislaw</forename><surname>Jastrzebski</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bruna</forename><surname>Morrone</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Quentin</forename><surname>De Laroussilhe</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrea</forename><surname>Gesmundo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mona</forename><surname>Attariyan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sylvain</forename><surname>Gelly</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1902.00751</idno>\n",
       " <ptr target=\"http://arxiv.org/abs/1902.00751\"/>\n",
       " <title level=\"m\">Parameter-Efficient Transfer Learning for NLP</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019-06\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>cs, stat</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b23\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Max</forename><surname>Jaderberg</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrea</forename><surname>Vedaldi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrew</forename><surname>Zisserman</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1405.3866</idno>\n",
       " <title level=\"m\">Speeding up convolutional neural networks with low rank expansions</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b24\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Initialization and regularization of factorized neural layers</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mikhail</forename><surname>Khodak</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Neil</forename><surname>Tenenholtz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lester</forename><surname>Mackey</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">NicolÃ²</forename><surname>Fusi</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b25\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Adam: A method for stochastic optimization</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Diederik</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jimmy</forename><surname>Kingma</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Ba</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b26\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Gshard: Scaling giant models with conditional computation and automatic sharding</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dmitry</forename><surname>Lepikhin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hyoukjoong</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yuanzhong</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dehao</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Orhan</forename><surname>Firat</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yanping</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Maxim</forename><surname>Krikun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noam</forename><surname>Shazeer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhifeng</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b27\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">The Power of Scale for Parameter-Efficient Prompt Tuning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Brian</forename><surname>Lester</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rami</forename><surname>Al-Rfou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noah</forename><surname>Constant</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2104.08691</idno>\n",
       " <idno>arXiv: 2104.08691</idno>\n",
       " <ptr target=\"http://arxiv.org/abs/2104.08691\"/>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021-04\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b28\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Measuring the Intrinsic Dimension of Objective Landscapes</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chunyuan</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Heerad</forename><surname>Farkhoor</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rosanne</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jason</forename><surname>Yosinski</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1804.08838</idno>\n",
       " <idno>arXiv: 1804.08838</idno>\n",
       " <ptr target=\"http://arxiv.org/abs/1804.08838\"/>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018-04\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>cs, stat</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b29\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Prefix-Tuning: Optimizing Continuous Prompts for Generation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lisa</forename><surname>Xiang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Percy</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Liang</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2101.00190</idno>\n",
       " <ptr target=\"http://arxiv.org/abs/2101.00190\"/>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021-01\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b30\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Learning overparameterized neural networks via stochastic gradient descent on structured data</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yuanzhi</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yingyu</forename><surname>Liang</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in Neural Information Processing Systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b31\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Recovery guarantee of weighted low-rank approximation via alternating minimization</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yuanzhi</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yingyu</forename><surname>Liang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrej</forename><surname>Risteski</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">International Conference on Machine Learning</title>\n",
       " <imprint>\n",
       " <publisher>PMLR</publisher>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " <biblScope from=\"2358\" to=\"2367\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b32\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yuanzhi</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tengyu</forename><surname>Ma</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hongyang</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Conference On Learning Theory</title>\n",
       " <imprint>\n",
       " <publisher>PMLR</publisher>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " <biblScope from=\"2\" to=\"47\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b33\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Exploring versatile generative language model via parameter-efficient transfer learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhaojiang</forename><surname>Lin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrea</forename><surname>Madotto</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pascale</forename><surname>Fung</surname></persName>\n",
       " </author>\n",
       " <idno type=\"DOI\">10.18653/v1/2020.findings-emnlp.41</idno>\n",
       " <ptr target=\"https://aclanthology.org/2020.findings-emnlp.41\"/>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Findings of the Association for Computational Linguistics: EMNLP 2020</title>\n",
       " <meeting><address><addrLine>Online</addrLine></address></meeting>\n",
       " <imprint>\n",
       " <publisher>Association for Computational Linguistics</publisher>\n",
       " <date type=\"published\" when=\"2020-11\"/>\n",
       " <biblScope from=\"441\" to=\"459\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b34\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xiao</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yanan</forename><surname>Zheng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhengxiao</forename><surname>Du</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ming</forename><surname>Ding</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yujie</forename><surname>Qian</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhilin</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jie</forename><surname>Tang</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2103.10385</idno>\n",
       " <idno>arXiv: 2103.10385</idno>\n",
       " <ptr target=\"http://arxiv.org/abs/2103.10385\"/>\n",
       " <title level=\"m\">GPT Understands, Too</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021-03\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b35\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yinhan</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Myle</forename><surname>Ott</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Naman</forename><surname>Goyal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jingfei</forename><surname>Du</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mandar</forename><surname>Joshi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Danqi</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Omer</forename><surname>Levy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mike</forename><surname>Lewis</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Veselin</forename><surname>Stoyanov</surname></persName>\n",
       " </author>\n",
       " <title level=\"m\">A robustly optimized bert pretraining approach</title>\n",
       " <meeting><address><addrLine>Roberta</addrLine></address></meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b36\">\n",
       " <monogr>\n",
       " <title/>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ilya</forename><surname>Loshchilov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Frank</forename><surname>Hutter</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1711.05101</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">Decoupled weight decay regularization. arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b37\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Decoupled weight decay regularization</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ilya</forename><surname>Loshchilov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Frank</forename><surname>Hutter</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b38\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Compacter: Efficient low-rank hypercomplex adapter layers</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">James</forename><surname>Rabeeh Karimi Mahabadi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sebastian</forename><surname>Henderson</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Ruder</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b39\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Linyong</forename><surname>Nan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dragomir</forename><surname>Radev</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rui</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Amrit</forename><surname>Rau</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Abhinand</forename><surname>Sivaprasad</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chiachun</forename><surname>Hsieh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xiangru</forename><surname>Tang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Aadit</forename><surname>Vyas</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Neha</forename><surname>Verma</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pranav</forename><surname>Krishna</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2007.02871</idno>\n",
       " <title level=\"m\">Open-domain structured data record to text generation</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b40\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">The e2e dataset: New challenges for endto-end generation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jekaterina</forename><surname>Novikova</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">OndÅ™ej</forename><surname>DuÅ¡ek</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Verena</forename><surname>Rieser</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1706.09254</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b41\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samet</forename><surname>Oymak</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zalan</forename><surname>Fabian</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mingchen</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mahdi</forename><surname>Soltanolkotabi</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1906.05392</idno>\n",
       " <title level=\"m\">Generalization guarantees for neural networks via harnessing the low-rank structure of the jacobian</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b42\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Adapterfusion: Non-destructive task composition for transfer learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jonas</forename><surname>Pfeiffer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Aishwarya</forename><surname>Kamath</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andreas</forename><surname>RÃ¼cklÃ©</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kyunghyun</forename><surname>Cho</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Iryna</forename><surname>Gurevych</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b43\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Semi-orthogonal low-rank matrix factorization for deep neural networks</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Daniel</forename><surname>Povey</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Gaofeng</forename><surname>Cheng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yiming</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ke</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hainan</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mahsa</forename><surname>Yarmohammadi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sanjeev</forename><surname>Khudanpur</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " <biblScope from=\"3743\" to=\"3747\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">In Interspeech</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b44\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Improving Language Understanding by Generative Pre-Training</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alec</forename><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Karthik</forename><surname>Narasimhan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tim</forename><surname>Salimans</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ilya</forename><surname>Sutskever</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <biblScope unit=\"page\">12</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b45\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Language Models are Unsupervised Multitask Learners</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alec</forename><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeffrey</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rewon</forename><surname>Child</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">David</forename><surname>Luan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dario</forename><surname>Amodei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ilya</forename><surname>Sutskever</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <biblScope unit=\"page\">24</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b46\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Know what you don't know: Unanswerable questions for squad. CoRR, abs/1806.03822</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pranav</forename><surname>Rajpurkar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Robin</forename><surname>Jia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Percy</forename><surname>Liang</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"http://arxiv.org/abs/1806.03822\"/>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b47\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Learning multiple visual domains with residual adapters</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrea</forename><surname>Bilen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Vedaldi</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1705.08045</idno>\n",
       " <idno>arXiv: 1705.08045</idno>\n",
       " <ptr target=\"http://arxiv.org/abs/1705.08045\"/>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017-11\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>cs, stat</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b48\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Adapterdrop: On the efficiency of adapters in transformers</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andreas</forename><surname>RÃ¼cklÃ©</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Gregor</forename><surname>Geigle</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Max</forename><surname>Glockner</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tilman</forename><surname>Beck</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jonas</forename><surname>Pfeiffer</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Nils Reimers, and Iryna Gurevych</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b49\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Lowrank matrix factorization for deep neural network training with high-dimensional output targets</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Tara</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Brian</forename><surname>Sainath</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Vikas</forename><surname>Kingsbury</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ebru</forename><surname>Sindhwani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bhuvana</forename><surname>Arisoy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Ramabhadran</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">2013 IEEE international conference on acoustics, speech and signal processing</title>\n",
       " <imprint>\n",
       " <publisher>IEEE</publisher>\n",
       " <date type=\"published\" when=\"2013\"/>\n",
       " <biblScope from=\"6655\" to=\"6659\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b50\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mohammad</forename><surname>Shoeybi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mostofa</forename><surname>Patwary</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Raul</forename><surname>Puri</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Patrick</forename><surname>Legresley</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jared</forename><surname>Casper</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bryan</forename><surname>Catanzaro</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b51\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Recursive deep models for semantic compositionality over a sentiment treebank</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Richard</forename><surname>Socher</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Perelygin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jean</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jason</forename><surname>Chuang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><forename type=\"middle\">D</forename><surname>Manning</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrew</forename><surname>Ng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><surname>Potts</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"https://aclanthology.org/D13-1170\"/>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>\n",
       " <meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2013-10\"/>\n",
       " <biblScope from=\"1631\" to=\"1642\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>Association for Computational Linguistics</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b52\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Attention is all you need</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ashish</forename><surname>Vaswani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noam</forename><surname>Shazeer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Niki</forename><surname>Parmar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jakob</forename><surname>Uszkoreit</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Llion</forename><surname>Jones</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Aidan</forename><forename type=\"middle\">N</forename><surname>Gomez</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Åukasz</forename><surname>Kaiser</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Illia</forename><surname>Polosukhin</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>\n",
       " <meeting>the 31st International Conference on Neural Information Processing Systems</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " <biblScope from=\"6000\" to=\"6010\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b53\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Amanpreet</forename><surname>Singh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Julian</forename><surname>Michael</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Felix</forename><surname>Hill</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Omer</forename><surname>Levy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samuel</forename><forename type=\"middle\">R</forename><surname>Bowman</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b54\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Superglue: A stickier benchmark for general-purpose language understanding systems</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yada</forename><surname>Pruksachatkun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nikita</forename><surname>Nangia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Amanpreet</forename><surname>Singh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Julian</forename><surname>Michael</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Felix</forename><surname>Hill</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Omer</forename><surname>Levy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samuel</forename><forename type=\"middle\">R</forename><surname>Bowman</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b55\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Warstadt</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Amanpreet</forename><surname>Singh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samuel R</forename><surname>Bowman</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1805.12471</idno>\n",
       " <title level=\"m\">Neural network acceptability judgments</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b56\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A broad-coverage challenge corpus for sentence understanding through inference</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Adina</forename><surname>Williams</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nikita</forename><surname>Nangia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samuel</forename><surname>Bowman</surname></persName>\n",
       " </author>\n",
       " <idno type=\"DOI\">10.18653/v1/N18-1101</idno>\n",
       " <ptr target=\"https://www.aclweb.org/anthology/N18-1101\"/>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>\n",
       " <meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>\n",
       " <imprint>\n",
       " <publisher>Association for Computational Linguistics</publisher>\n",
       " <date type=\"published\" when=\"2018-06\"/>\n",
       " <biblScope unit=\"volume\">1</biblScope>\n",
       " <biblScope from=\"1112\" to=\"1122\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b57\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Transformers: State-of-the-art natural language processing</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Thomas</forename><surname>Wolf</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lysandre</forename><surname>Debut</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Victor</forename><surname>Sanh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Julien</forename><surname>Chaumond</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Clement</forename><surname>Delangue</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Anthony</forename><surname>Moi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pierric</forename><surname>Cistac</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tim</forename><surname>Rault</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">RÃ©mi</forename><surname>Louf</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Morgan</forename><surname>Funtowicz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Joe</forename><surname>Davison</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sam</forename><surname>Shleifer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Clara</forename><surname>Patrick Von Platen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yacine</forename><surname>Ma</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Julien</forename><surname>Jernite</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Canwen</forename><surname>Plu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Teven</forename><forename type=\"middle\">Le</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sylvain</forename><surname>Scao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mariama</forename><surname>Gugger</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Quentin</forename><surname>Drame</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alexander</forename><forename type=\"middle\">M</forename><surname>Lhoest</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Rush</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"https://www.aclweb.org/anthology/2020.emnlp-demos.6\"/>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>\n",
       " <meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020-10\"/>\n",
       " <biblScope from=\"38\" to=\"45\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>Association for Computational Linguistics</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b58\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Greg</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Edward</forename><forename type=\"middle\">J</forename><surname>Hu</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2011.14522</idno>\n",
       " <idno>arXiv: 2011.14522</idno>\n",
       " <ptr target=\"http://arxiv.org/abs/2011.14522\"/>\n",
       " <title level=\"m\">Feature Learning in Infinite-Width Neural Networks</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021-05\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>cond-mat</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b59\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Elad</forename><surname>Ben Zaken</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Shauli</forename><surname>Ravfogel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yoav</forename><surname>Goldberg</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b60\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Extracting deep neural network bottleneck features using low-rank matrix factorization</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yu</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ekapol</forename><surname>Chuangsuwanich</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">James</forename><surname>Glass</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">2014 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>\n",
       " <imprint>\n",
       " <publisher>IEEE</publisher>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " <biblScope from=\"185\" to=\"189\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b61\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Low-rank plus diagonal adaptation for deep neural networks</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yong</forename><surname>Zhao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jinyu</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yifan</forename><surname>Gong</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>\n",
       " <imprint>\n",
       " <publisher>IEEE</publisher>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " <biblScope from=\"5005\" to=\"5009\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b62\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Seq2sql: Generating structured queries from natural language using reinforcement learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Victor</forename><surname>Zhong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Caiming</forename><surname>Xiong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Richard</forename><surname>Socher</surname></persName>\n",
       " </author>\n",
       " <idno>abs/1709.00103</idno>\n",
       " <ptr target=\"http://arxiv.org/abs/1709.00103\"/>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " </listBibl>\n",
       " </div>\n",
       " </back>\n",
       " </text>\n",
       " </TEI>,\n",
       " <?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       " <TEI xml:space=\"preserve\" xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd\">\n",
       " <teiHeader xml:lang=\"en\">\n",
       " <fileDesc>\n",
       " <titleStmt>\n",
       " <title level=\"a\" type=\"main\">\"Why Should I Trust You?\"</title>\n",
       " </titleStmt>\n",
       " <publicationStmt>\n",
       " <publisher>ACM</publisher>\n",
       " <availability status=\"unknown\"><p>Copyright ACM</p>\n",
       " </availability>\n",
       " <date type=\"published\" when=\"2016-08-13\">2016-08-13</date>\n",
       " </publicationStmt>\n",
       " <sourceDesc>\n",
       " <biblStruct>\n",
       " <analytic>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Marco</forename><forename type=\"middle\">Tulio</forename><surname>Ribeiro</surname></persName>\n",
       " <email>marcotcr@cs.uw.edu</email>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">University of Washington Seattle</orgName>\n",
       " <address>\n",
       " <postCode>98105</postCode>\n",
       " <region>WA</region>\n",
       " <country key=\"US\">USA</country>\n",
       " </address>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sameer</forename><surname>Singh</surname></persName>\n",
       " <email>sameer@cs.uw.edu</email>\n",
       " <affiliation key=\"aff1\">\n",
       " <orgName type=\"institution\">University of Washington Seattle</orgName>\n",
       " <address>\n",
       " <postCode>98105</postCode>\n",
       " <region>WA</region>\n",
       " <country key=\"US\">USA</country>\n",
       " </address>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Carlos</forename><surname>Guestrin</surname></persName>\n",
       " <email>guestrin@cs.uw.edu</email>\n",
       " <affiliation key=\"aff2\">\n",
       " <orgName type=\"institution\">University of Washington Seattle</orgName>\n",
       " <address>\n",
       " <postCode>98105</postCode>\n",
       " <region>WA</region>\n",
       " <country key=\"US\">USA</country>\n",
       " </address>\n",
       " </affiliation>\n",
       " </author>\n",
       " <title level=\"a\" type=\"main\">\"Why Should I Trust You?\"</title>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>\n",
       " <meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\t\t\t\t\t\t</meeting>\n",
       " <imprint>\n",
       " <publisher>ACM</publisher>\n",
       " <date type=\"published\" when=\"2016-08-13\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <idno type=\"DOI\">10.1145/2939672.2939778</idno>\n",
       " </biblStruct>\n",
       " </sourceDesc>\n",
       " </fileDesc>\n",
       " <encodingDesc>\n",
       " <appInfo>\n",
       " <application ident=\"GROBID\" version=\"0.7.0-SNAPSHOT\" when=\"2024-04-27T16:15+0000\">\n",
       " <desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>\n",
       " <ref target=\"https://github.com/kermitt2/grobid\"/>\n",
       " </application>\n",
       " </appInfo>\n",
       " </encodingDesc>\n",
       " <profileDesc>\n",
       " <abstract>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.</p><p>In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</p></div>\n",
       " </abstract>\n",
       " </profileDesc>\n",
       " </teiHeader>\n",
       " <text xml:lang=\"en\">\n",
       " <body>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1.\">INTRODUCTION</head><p>Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e. whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Determining trust in individual predictions is an important problem when the model is used for decision making. When using machine learning for medical diagnosis <ref target=\"#b5\" type=\"bibr\">[6]</ref> or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic.</p><p>Apart from trusting individual predictions, there is also a need to evaluate the model as a whole before deploying it \"in the wild\". To make this decision, users need to be confident that the model will perform well on real-world data, according to the metrics of interest. Currently, models are evaluated using accuracy metrics on an available validation dataset. However, real-world data is often significantly different, and further, the evaluation metric may not be indicative of the product's goal. Inspecting individual predictions and their explanations is a worthwhile solution, in addition to such metrics. In this case, it is important to aid users by suggesting which instances to inspect, especially for large datasets.</p><p>In this paper, we propose providing explanations for individual predictions as a solution to the \"trusting a prediction\" problem, and selecting multiple such predictions (and explanations) as a solution to the \"trusting the model\" problem. Our main contributions are summarized as follows.</p><p>â€¢ LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model.</p><p>â€¢ SP-LIME, a method that selects a set of representative instances with explanations to address the \"trusting the model\" problem, via submodular optimization.</p><p>â€¢ Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on trust and associated tasks. In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.\">THE CASE FOR EXPLANATIONS</head><p>By \"explaining a prediction\", we mean presenting textual or visual artifacts that provide qualitative understanding of the relationship between the instance's components (e.g. words in text, patches in an image) and the model's prediction. We argue that explaining predictions is an important aspect in  A model predicts that a patient has the flu, and LIME highlights the symptoms in the patient's history that led to the prediction. Sneeze and headache are portrayed as contributing to the \"flu\" prediction, while \"no fatigue\" is evidence against it. With these, a doctor can make an informed decision about whether to trust the model's prediction.</p><p>getting humans to trust and use machine learning effectively, if the explanations are faithful and intelligible. The process of explaining individual predictions is illustrated in Figure <ref target=\"#fig_2\" type=\"figure\">1</ref>. It is clear that a doctor is much better positioned to make a decision with the help of a model if intelligible explanations are provided. In this case, an explanation is a small list of symptoms with relative weightssymptoms that either contribute to the prediction (in green) or are evidence against it (in red). Humans usually have prior knowledge about the application domain, which they can use to accept (trust) or reject a prediction if they understand the reasoning behind it. It has been observed, for example, that providing explanations can increase the acceptance of movie recommendations <ref target=\"#b11\" type=\"bibr\">[12]</ref> and other automated systems <ref target=\"#b7\" type=\"bibr\">[8]</ref>.</p><p>Every machine learning application also requires a certain measure of overall trust in the model. Development and evaluation of a classification model often consists of collecting annotated data, of which a held-out subset is used for automated evaluation. Although this is a useful pipeline for many applications, evaluation on validation data may not correspond to performance \"in the wild\", as practitioners often overestimate the accuracy of their models <ref target=\"#b19\" type=\"bibr\">[20]</ref>, and thus trust cannot rely solely on it. Looking at examples offers an alternative method to assess truth in the model, especially if the examples are explained. We thus propose explaining several representative individual predictions of a model as a way to provide a global understanding.</p><p>There are several ways a model or its evaluation can go wrong. Data leakage, for example, defined as the unintentional leakage of signal into the training (and validation) data that would not appear when deployed <ref target=\"#b13\" type=\"bibr\">[14]</ref>, potentially increases accuracy. A challenging example cited by Kaufman et al. <ref target=\"#b13\" type=\"bibr\">[14]</ref> is one where the patient ID was found to be heavily correlated with the target class in the training and validation data. This issue would be incredibly challenging to identify just by observing the predictions and the raw data, but much easier if explanations such as the one in Figure <ref target=\"#fig_2\" type=\"figure\">1</ref> are provided, as patient ID would be listed as an explanation for predictions. Another particularly hard to detect problem is dataset shift <ref target=\"#b4\" type=\"bibr\">[5]</ref>, where training data is different than test data (we give an example in the famous 20 newsgroups dataset later on). The insights given by explanations are particularly helpful in identifying what must be done to convert an untrustworthy model into a trustworthy one -for example, removing leaked data or changing the training data to avoid dataset shift.</p><p>Machine learning practitioners often have to select a model from a number of alternatives, requiring them to assess the relative trust between two or more models. In Figure <ref type=\"figure\">Figure 2</ref>: Explaining individual predictions of competing classifiers trying to determine if a document is about \"Christianity\" or \"Atheism\". The bar chart represents the importance given to the most relevant words, also highlighted in the text. Color indicates which class the word contributes to (green for \"Christianity\", magenta for \"Atheism\").</p><p>2, we show how individual prediction explanations can be used to select between models, in conjunction with accuracy. In this case, the algorithm with higher accuracy on the validation set is actually much worse, a fact that is easy to see when explanations are provided (again, due to human prior knowledge), but hard otherwise. Further, there is frequently a mismatch between the metrics that we can compute and optimize (e.g. accuracy) and the actual metrics of interest such as user engagement and retention. While we may not be able to measure such metrics, we have knowledge about how certain model behaviors can influence them. Therefore, a practitioner may wish to choose a less accurate model for content recommendation that does not place high importance in features related to \"clickbait\" articles (which may hurt user retention), even if exploiting such features increases the accuracy of the model in cross validation. We note that explanations are particularly useful in these (and other) scenarios if a method can produce them for any model, so that a variety of models can be compared.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Desired Characteristics for Explainers</head><p>We now outline a number of desired characteristics from explanation methods.</p><p>An essential criterion for explanations is that they must be interpretable, i.e., provide qualitative understanding between the input variables and the response. We note that interpretability must take into account the user's limitations. Thus, a linear model <ref target=\"#b23\" type=\"bibr\">[24]</ref>, a gradient vector <ref target=\"#b1\" type=\"bibr\">[2]</ref> or an additive model <ref target=\"#b5\" type=\"bibr\">[6]</ref> may or may not be interpretable. For example, if hundreds or thousands of features significantly contribute to a prediction, it is not reasonable to expect any user to comprehend why the prediction was made, even if individual weights can be inspected. This requirement further implies that explanations should be easy to understand, which is not necessarily true of the features used by the model, and thus the \"input variables\" in the explanations may need to be different than the features. Finally, we note that the notion of interpretability also depends on the target audience. Machine learning practitioners may be able to interpret small Bayesian networks, but laymen may be more comfortable with a small number of weighted features as an explanation.</p><p>Another essential criterion is local fidelity. Although it is often impossible for an explanation to be completely faithful unless it is the complete description of the model itself, for an explanation to be meaningful it must at least be locally faithful, i.e. it must correspond to how the model behaves in the vicinity of the instance being predicted. We note that local fidelity does not imply global fidelity: features that are globally important may not be important in the local context, and vice versa. While global fidelity would imply local fidelity, identifying globally faithful explanations that are interpretable remains a challenge for complex models.</p><p>While there are models that are inherently interpretable <ref target=\"#b5\" type=\"bibr\">[6,</ref><ref target=\"#b16\" type=\"bibr\">17,</ref><ref target=\"#b25\" type=\"bibr\">26,</ref><ref target=\"#b26\" type=\"bibr\">27]</ref>, an explainer should be able to explain any model, and thus be model-agnostic (i.e. treat the original model as a black box). Apart from the fact that many state-ofthe-art classifiers are not currently interpretable, this also provides flexibility to explain future classifiers.</p><p>In addition to explaining predictions, providing a global perspective is important to ascertain trust in the model. As mentioned before, accuracy may often not be a suitable metric to evaluate the model, and thus we want to explain the model. Building upon the explanations for individual predictions, we select a few explanations to present to the user, such that they are representative of the model.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.\">LOCAL INTERPRETABLE MODEL-AGNOSTIC EXPLANATIONS</head><p>We now present Local Interpretable Model-agnostic Explanations (LIME). The overall goal of LIME is to identify an interpretable model over the interpretable representation that is locally faithful to the classifier.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1\">Interpretable Data Representations</head><p>Before we present the explanation system, it is important to distinguish between features and interpretable data representations. As mentioned before, interpretable explanations need to use a representation that is understandable to humans, regardless of the actual features used by the model. For example, a possible interpretable representation for text classification is a binary vector indicating the presence or absence of a word, even though the classifier may use more complex (and incomprehensible) features such as word embeddings. Likewise for image classification, an interpretable representation may be a binary vector indicating the \"presence\" or \"absence\" of a contiguous patch of similar pixels (a super-pixel), while the classifier may represent the image as a tensor with three color channels per pixel. We denote x âˆˆ R d be the original representation of an instance being explained, and we use x âˆˆ {0, 1} d to denote a binary vector for its interpretable representation.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Fidelity-Interpretability Trade-off</head><p>Formally, we define an explanation as a model g âˆˆ G, where G is a class of potentially interpretable models, such as linear models, decision trees, or falling rule lists <ref target=\"#b26\" type=\"bibr\">[27]</ref>, i.e. a model g âˆˆ G can be readily presented to the user with visual or textual artifacts. The domain of g is {0, 1} d , i.e. g acts over absence/presence of the interpretable components. As not every g âˆˆ G may be simple enough to be interpretablethus we let â„¦(g) be a measure of complexity (as opposed to interpretability) of the explanation g âˆˆ G. For example, for decision trees â„¦(g) may be the depth of the tree, while for linear models, â„¦(g) may be the number of non-zero weights.</p><p>Let the model being explained be denoted f : R d â†’ R. In classification, f (x) is the probability (or a binary indicator) that x belongs to a certain class 1 . We further use Ï€x(z) as a proximity measure between an instance z to x, so as to define locality around x. Finally, let L(f, g, Ï€x) be a measure of how unfaithful g is in approximating f in the locality defined by Ï€x. In order to ensure both interpretability and local fidelity, we must minimize L(f, g, Ï€x) while having â„¦(g) be low enough to be interpretable by humans. The explanation produced by LIME is obtained by the following:</p><formula xml:id=\"formula_0\">Î¾(x) = argmin gâˆˆG L(f, g, Ï€x) + â„¦(g) (1)</formula><p>This formulation can be used with different explanation families G, fidelity functions L, and complexity measures â„¦.</p><p>Here we focus on sparse linear models as explanations, and on performing the search using perturbations.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.3\">Sampling for Local Exploration</head><p>We want to minimize the locality-aware loss L(f, g, Ï€x) without making any assumptions about f , since we want the explainer to be model-agnostic. Thus, in order to learn the local behavior of f as the interpretable inputs vary, we approximate L(f, g, Ï€x) by drawing samples, weighted by Ï€x. We sample instances around x by drawing nonzero elements of x uniformly at random (where the number of such draws is also uniformly sampled). Given a perturbed sample z âˆˆ {0, 1} d (which contains a fraction of the nonzero elements of x ), we recover the sample in the original representation z âˆˆ R d and obtain f (z), which is used as a label for the explanation model. Given this dataset Z of perturbed samples with the associated labels, we optimize Eq. (1) to get an explanation Î¾(x). The primary intuition behind LIME is presented in Figure <ref target=\"#fig_3\" type=\"figure\">3</ref>, where we sample instances both in the vicinity of x (which have a high weight due to Ï€x) and far away from x (low weight from Ï€x). Even though the original model may be too complex to explain globally, LIME presents an explanation that is locally faithful (linear in this case), where the locality is captured by Ï€x. It is worth noting that our method is fairly robust to sampling noise since the samples are weighted by Ï€x in Eq. <ref target=\"#b0\" type=\"bibr\">(1)</ref>. We now present a concrete instance of this general framework.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.4\">Sparse Linear Explanations</head><p>For the rest of this paper, we let G be the class of linear models, such that g(z ) = wg â€¢ z . We use the locally weighted square loss as L, as defined in Eq. (2), where we let Ï€x(z) = exp(âˆ’D(x, z) 2 /Ïƒ 2 ) be an exponential kernel defined on some The black-box model's complex decision function f (unknown to LIME) is represented by the blue/pink background, which cannot be approximated well by a linear model. The bold red cross is the instance being explained. LIME samples instances, gets predictions using f , and weighs them by the proximity to the instance being explained (represented here by size). The dashed line is the learned explanation that is locally (but not globally) faithful.</p><p>distance function D (e.g. cosine distance for text, L2 distance for images) with width Ïƒ.</p><formula xml:id=\"formula_1\">L(f, g, Ï€x) = z,z âˆˆZ Ï€x(z) f (z) âˆ’ g(z ) 2 (2)</formula><p>For text classification, we ensure that the explanation is interpretable by letting the interpretable representation be a bag of words, and by setting a limit K on the number of words, i.e. â„¦(g) = âˆž1[ wg 0 &gt; K]. Potentially, K can be adapted to be as big as the user can handle, or we could have different values of K for different instances. In this paper we use a constant value for K, leaving the exploration of different values to future work. We use the same â„¦ for image classification, using \"super-pixels\" (computed using any standard algorithm) instead of words, such that the interpretable representation of an image is a binary vector where 1 indicates the original super-pixel and 0 indicates a grayed out super-pixel. This particular choice of â„¦ makes directly solving Eq. (1) intractable, but we approximate it by first selecting K features with Lasso (using the regularization path <ref target=\"#b8\" type=\"bibr\">[9]</ref>) and then learning the weights via least squares (a procedure we call K-LASSO in Algorithm 1). Since Algorithm 1 produces an explanation for an individual prediction, its complexity does not depend on the size of the dataset, but instead on time to compute f (x) and on the number of samples N . In practice, explaining random forests with 1000 trees using scikit-learn (http://scikit-learn.org) on a laptop with N = 5000 takes under 3 seconds without any optimizations such as using gpus or parallelization. Explaining each prediction of the Inception network <ref target=\"#b24\" type=\"bibr\">[25]</ref> for image classification takes around 10 minutes.</p><p>Any choice of interpretable representations and G will have some inherent drawbacks. First, while the underlying model can be treated as a black-box, certain interpretable representations will not be powerful enough to explain certain behaviors. For example, a model that predicts sepia-toned images to be retro cannot be explained by presence of absence of super pixels. Second, our choice of G (sparse linear models) means that if the underlying model is highly non-linear even in the locality of the prediction, there may not be a faithful explanation. However, we can estimate the faithfulness of Algorithm 1 Sparse Linear Explanations using LIME Require: Classifier f , Number of samples N Require: Instance x, and its interpretable version x Require: Similarity kernel Ï€x, Length of explanation K</p><formula xml:id=\"formula_2\">Z â† {} for i âˆˆ {1, 2, 3, ..., N } do z i â† sample around(x ) Z â† Z âˆª z i , f (zi), Ï€x(zi) end for w â† K-Lasso(Z, K)</formula><p>with z i as features, f (z) as target return w the explanation on Z, and present this information to the user. This estimate of faithfulness can also be used for selecting an appropriate family of explanations from a set of multiple interpretable model classes, thus adapting to the given dataset and the classifier. We leave such exploration for future work, as linear explanations work quite well for multiple black-box models in our experiments.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.5\">Example 1: Text classification with SVMs</head><p>In Figure <ref type=\"figure\">2</ref> (right side), we explain the predictions of a support vector machine with RBF kernel trained on unigrams to differentiate \"Christianity\" from \"Atheism\" (on a subset of the 20 newsgroup dataset). Although this classifier achieves 94% held-out accuracy, and one would be tempted to trust it based on this, the explanation for an instance shows that predictions are made for quite arbitrary reasons (words \"Posting\", \"Host\", and \"Re\" have no connection to either Christianity or Atheism). The word \"Posting\" appears in 22% of examples in the training set, 99% of them in the class \"Atheism\". Even if headers are removed, proper names of prolific posters in the original newsgroups are selected by the classifier, which would also not generalize.</p><p>After getting such insights from explanations, it is clear that this dataset has serious issues (which are not evident just by studying the raw data or predictions), and that this classifier, or held-out evaluation, cannot be trusted. It is also clear what the problems are, and the steps that can be taken to fix these issues and train a more trustworthy classifier.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.6\">Example 2: Deep networks for images</head><p>When using sparse linear explanations for image classifiers, one may wish to just highlight the super-pixels with positive weight towards a specific class, as they give intuition as to why the model would think that class may be present. We explain the prediction of Google's pre-trained Inception neural network <ref target=\"#b24\" type=\"bibr\">[25]</ref> in this fashion on an arbitrary image (Figure <ref target=\"#fig_4\" type=\"figure\">4a</ref>). Figures <ref target=\"#fig_4\" type=\"figure\">4b, 4c, 4d</ref> show the superpixels explanations for the top 3 predicted classes (with the rest of the image grayed out), having set K = 10. What the neural network picks up on for each of the classes is quite natural to humans -Figure <ref target=\"#fig_4\" type=\"figure\">4b</ref> in particular provides insight as to why acoustic guitar was predicted to be electric: due to the fretboard. This kind of explanation enhances trust in the classifier (even if the top predicted class is wrong), as it shows that it is not acting in an unreasonable manner. </p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.\">SUBMODULAR PICK FOR EXPLAINING MODELS</head><p>Although an explanation of a single prediction provides some understanding into the reliability of the classifier to the user, it is not sufficient to evaluate and assess trust in the model as a whole. We propose to give a global understanding of the model by explaining a set of individual instances. This approach is still model agnostic, and is complementary to computing summary statistics such as held-out accuracy.</p><p>Even though explanations of multiple instances can be insightful, these instances need to be selected judiciously, since users may not have the time to examine a large number of explanations. We represent the time/patience that humans have by a budget B that denotes the number of explanations they are willing to look at in order to understand a model. Given a set of instances X, we define the pick step as the task of selecting B instances for the user to inspect.</p><p>The pick step is not dependent on the existence of explanations -one of the main purpose of tools like Modeltracker <ref target=\"#b0\" type=\"bibr\">[1]</ref> and others <ref target=\"#b10\" type=\"bibr\">[11]</ref> is to assist users in selecting instances themselves, and examining the raw data and predictions. However, since looking at raw data is not enough to understand predictions and get insights, the pick step should take into account the explanations that accompany each prediction. Moreover, this method should pick a diverse, representative set of explanations to show the user -i.e. non-redundant explanations that represent how the model behaves globally.</p><p>Given the explanations for a set of instances X (|X| = n), we construct an n Ã— d explanation matrix W that represents the local importance of the interpretable components for each instance. When using linear models as explanations, for an instance xi and explanation gi = Î¾(xi), we set Wij = |wg ij |. Further, for each component (column) j in W, we let Ij denote the global importance of that component in the explanation space. Intuitively, we want I such that features that explain many different instances have higher importance scores. In Figure <ref target=\"#fig_6\" type=\"figure\">5</ref>, we show a toy example W, with n = d = 5, where W is binary (for simplicity). The importance function I should score feature f2 higher than feature f1, i.e. I2 &gt; I1, since feature f2 is used to explain more instances. Concretely for the text applications, we set Ij = n i=1 Wij. For images, I must measure something that is comparable across the super-pixels in different images,  Algorithm 2 Submodular pick (SP) algorithm Require:</p><formula xml:id=\"formula_3\">Instances X, Budget B for all xi âˆˆ X do Wi â† explain(xi, x i ) Using Algorithm 1 end for for j âˆˆ {1 . . . d } do Ij â† n i=1 |Wij| Compute feature importances end for V â† {} while |V | &lt; B do</formula><p>Greedy optimization of Eq (4</p><formula xml:id=\"formula_4\">) V â† V âˆª argmax i c(V âˆª {i}, W, I) end while return V</formula><p>such as color histograms or other features of super-pixels; we leave further exploration of these ideas for future work.</p><p>While we want to pick instances that cover the important components, the set of explanations must not be redundant in the components they show the users, i.e. avoid selecting instances with similar explanations. In Figure <ref target=\"#fig_6\" type=\"figure\">5</ref>, after the second row is picked, the third row adds no value, as the user has already seen features f2 and f3 -while the last row exposes the user to completely new features. Selecting the second and last row results in the coverage of almost all the features. We formalize this non-redundant coverage intuition in Eq. (3), where we define coverage as the set function c that, given W and I, computes the total importance of the features that appear in at least one instance in a set V .</p><formula xml:id=\"formula_5\">c(V, W, I) = d j=1 1 [âˆƒiâˆˆV :W ij &gt;0] Ij (3)</formula><p>The pick problem, defined in Eq. (4), consists of finding the set V, |V | â‰¤ B that achieves highest coverage.</p><formula xml:id=\"formula_6\">P ick(W, I) = argmax V,|V |â‰¤B c(V, W, I)<label>(4)</label></formula><p>The problem in Eq. ( <ref target=\"#formula_6\" type=\"formula\">4</ref>) is maximizing a weighted coverage function, and is NP-hard <ref target=\"#b9\" type=\"bibr\">[10]</ref>. Let c(V âˆª{i}, W, I)âˆ’c(V, W, I) be the marginal coverage gain of adding an instance i to a set V . Due to submodularity, a greedy algorithm that iteratively adds the instance with the highest marginal coverage gain to the solution offers a constant-factor approximation guarantee of 1âˆ’1/e to the optimum <ref target=\"#b14\" type=\"bibr\">[15]</ref>. We outline this approximation in Algorithm 2, and call it submodular pick.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.\">SIMULATED USER EXPERIMENTS</head><p>In this section, we present simulated user experiments to evaluate the utility of explanations in trust-related tasks. In particular, we address the following questions: (1) Are the explanations faithful to the model, (2) Can the explanations aid users in ascertaining trust in predictions, and (3) Are the explanations useful for evaluating the model as a whole. Code and data for replicating our experiments are available at https://github.com/marcotcr/lime-experiments.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">Experiment Setup</head><p>We use two sentiment analysis datasets (books and DVDs, 2000 instances each) where the task is to classify product reviews as positive or negative <ref target=\"#b3\" type=\"bibr\">[4]</ref>. We train decision trees (DT), logistic regression with L2 regularization (LR), nearest neighbors (NN), and support vector machines with RBF kernel (SVM), all using bag of words as features. We also include random forests (with 1000 trees) trained with the average word2vec embedding <ref target=\"#b18\" type=\"bibr\">[19]</ref> (RF), a model that is impossible to interpret without a technique like LIME. We use the implementations and default parameters of scikitlearn, unless noted otherwise. We divide each dataset into train (1600 instances) and test (400 instances).</p><p>To explain individual predictions, we compare our proposed approach (LIME), with parzen [2], a method that approximates the black box classifier globally with Parzen windows, and explains individual predictions by taking the gradient of the prediction probability function. For parzen, we take the K features with the highest absolute gradients as explanations. We set the hyper-parameters for parzen and LIME using cross validation, and set N = 15, 000. We also compare against a greedy procedure (similar to Martens and Provost <ref target=\"#b17\" type=\"bibr\">[18]</ref>) in which we greedily remove features that contribute the most to the predicted class until the prediction changes (or we reach the maximum of K features), and a random procedure that randomly picks K features as an explanation. We set K to 10 for our experiments.</p><p>For experiments where the pick procedure applies, we either do random selection (random pick, RP) or the procedure described in Â§4 (submodular pick, SP). We refer to pickexplainer combinations by adding RP or SP as a prefix.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.2\">Are explanations faithful to the model?</head><p>We measure faithfulness of explanations on classifiers that are by themselves interpretable (sparse logistic regression   and decision trees). In particular, we train both classifiers such that the maximum number of features they use for any instance is 10, and thus we know the gold set of features that the are considered important by these models. For each prediction on the test set, we generate explanations and compute the fraction of these gold features that are recovered by the explanations. We report this recall averaged over all the test instances in Figures <ref target=\"#fig_8\" type=\"figure\">6 and 7</ref>. We observe that the greedy approach is comparable to parzen on logistic regression, but is substantially worse on decision trees since changing a single feature at a time often does not have an effect on the prediction. The overall recall by parzen is low, likely due to the difficulty in approximating the original highdimensional classifier. LIME consistently provides &gt; 90% recall for both classifiers on both datasets, demonstrating that LIME explanations are faithful to the models.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.3\">Should I trust this prediction?</head><p>In order to simulate trust in individual predictions, we first randomly select 25% of the features to be \"untrustworthy\", and assume that the users can identify and would not want to trust these features (such as the headers in 20 newsgroups, leaked data, etc). We thus develop oracle \"trustworthiness\" by labeling test set predictions from a black box classifier as \"untrustworthy\" if the prediction changes when untrustworthy features are removed from the instance, and \"trustworthy\" otherwise. In order to simulate users, we assume that users deem predictions untrustworthy from LIME and parzen explanations if the prediction from the linear approximation changes when all untrustworthy features that appear in the explanations are removed (the simulated human \"discounts\" the effect of untrustworthy features). For greedy and random, the prediction is mistrusted if any untrustworthy features are present in the explanation, since these methods do not provide a notion of the contribution of each feature to the prediction. Thus for each test set prediction, we can evaluate whether the simulated user trusts it using each explanation method, and compare it to the trustworthiness oracle.</p><p>Using this setup, we report the F1 on the trustworthy   predictions for each explanation method, averaged over 100 runs, in Table <ref target=\"#tab_0\" type=\"table\">1</ref>. The results indicate that LIME dominates others (all results are significant at p = 0.01) on both datasets, and for all of the black box models. The other methods either achieve a lower recall (i.e. they mistrust predictions more than they should) or lower precision (i.e. they trust too many predictions), while LIME maintains both high precision and high recall. Even though we artificially select which features are untrustworthy, these results indicate that LIME is helpful in assessing trust in individual predictions.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.4\">Can I trust this model?</head><p>In the final simulated user experiment, we evaluate whether the explanations can be used for model selection, simulating the case where a human has to decide between two competing models with similar accuracy on validation data. For this purpose, we add 10 artificially \"noisy\" features. Specifically, on training and validation sets (80/20 split of the original training data), each artificial feature appears in 10% of the examples in one class, and 20% of the other, while on the test instances, each artificial feature appears in 10% of the examples in each class. This recreates the situation where the models use not only features that are informative in the real world, but also ones that introduce spurious correlations. We create pairs of competing classifiers by repeatedly training pairs of random forests with 30 trees until their validation accuracy is within 0.1% of each other, but their test accuracy differs by at least 5%. Thus, it is not possible to identify the better classifier (the one with higher test accuracy) from the accuracy on the validation data.</p><p>The goal of this experiment is to evaluate whether a user can identify the better classifier based on the explanations of B instances from the validation set. The simulated human marks the set of artificial features that appear in the B explanations as untrustworthy, following which we evaluate how many total predictions in the validation set should be trusted (as in the previous section, treating only marked features as untrustworthy). Then, we select the classifier with fewer untrustworthy predictions, and compare this choice to the classifier with higher held-out test set accuracy.</p><p>We present the accuracy of picking the correct classifier as B varies, averaged over 800 runs, in Figure <ref target=\"#fig_11\" type=\"figure\">8</ref>. We omit SP-parzen and RP-parzen from the figure since they did not produce useful explanations, performing only slightly better than random. LIME is consistently better than greedy, irrespective of the pick method. Further, combining submodular pick with LIME outperforms all other methods, in particular it is much better than RP-LIME when only a few examples are shown to the users. These results demonstrate that the trust assessments provided by SP-selected LIME explanations are good indicators of generalization, which we validate with human experiments in the next section.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6.\">EVALUATION WITH HUMAN SUBJECTS</head><p>In this section, we recreate three scenarios in machine learning that require trust and understanding of predictions and models. In particular, we evaluate LIME and SP-LIME in the following settings: (1) Can users choose which of two classifiers generalizes better ( Â§ 6.2), (2) based on the explanations, can users perform feature engineering to improve the model ( Â§ 6.3), and (3) are users able to identify and describe classifier irregularities by looking at explanations ( Â§ 6.4).</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6.1\">Experiment setup</head><p>For experiments in Â§6.2 and Â§6.3, we use the \"Christianity\" and \"Atheism\" documents from the 20 newsgroups dataset mentioned beforehand. This dataset is problematic since it contains features that do not generalize (e.g. very informative header information and author names), and thus validation accuracy considerably overestimates real-world performance.</p><p>In order to estimate the real world performance, we create a new religion dataset for evaluation. We download Atheism and Christianity websites from the DMOZ directory and human curated lists, yielding 819 webpages in each class. High accuracy on this dataset by a classifier trained on 20 newsgroups indicates that the classifier is generalizing using semantic content, instead of placing importance on the data specific issues outlined above. Unless noted otherwise, we use SVM with RBF kernel, trained on the 20 newsgroups data with hyper-parameters tuned via the cross-validation.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6.2\">Can users select the best classifier?</head><p>In this section, we want to evaluate whether explanations can help users decide which classifier generalizes better, i.e., which classifier would the user deploy \"in the wild\". Specifically, users have to decide between two classifiers: SVM trained on the original 20 newsgroups dataset, and a version of the same classifier trained on a \"cleaned\" dataset where many of the features that do not generalize have been manually removed. The original classifier achieves an accuracy score of 57.3% on the religion dataset, while the \"cleaned\" classifier achieves a score of 69.0%. In contrast, the test accuracy on the original 20 newsgroups split is 94.0% and 88.6%, respectively -suggesting that the worse classifier would be selected if accuracy alone is used as a measure of trust.</p><p>We recruit human subjects on Amazon Mechanical Turkby no means machine learning experts, but instead people with basic knowledge about religion. We measure their ability to choose the better algorithm by seeing side-byside explanations with the associated raw data (as shown in Figure <ref type=\"figure\">2</ref>). We restrict both the number of words in each explanation (K) and the number of documents that each person inspects (B) to 6. The position of each algorithm and the order of the instances seen are randomized between subjects. After examining the explanations, users are asked to select which algorithm will perform best in the real world. The explanations are produced by either greedy (chosen as a baseline due to its performance in the simulated user experiment) or LIME, and the instances are selected either by random (RP) or submodular pick (SP). We modify the greedy step in Algorithm 2 slightly so it alternates between explanations of the two classifiers. For each setting, we repeat the experiment with 100 users. The results are presented in Figure <ref type=\"figure\">9</ref>. Note that all of the methods are good at identifying the better classifier, demonstrating that the explanations are useful in determining which classifier to trust, while using test set accuracy would result in the selection of the wrong classifier. Further, we see that the submodular pick (SP) greatly improves the user's ability to select the best classifier when compared to random pick (RP), with LIME outperforming greedy in both cases.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6.3\">Can non-experts improve a classifier?</head><p>If one notes that a classifier is untrustworthy, a common task in machine learning is feature engineering, i.e. modifying the set of features and retraining in order to improve generalization. Explanations can aid in this process by presenting the important features, particularly for removing features that the users feel do not generalize.</p><p>We use the 20 newsgroups data here as well, and ask Amazon Mechanical Turk users to identify which words from the explanations should be removed from subsequent training, for the worse classifier from the previous section ( Â§6.2). In each round, the subject marks words for deletion after observing B = 10 instances with K = 10 words in each explanation (an interface similar to Figure <ref type=\"figure\">2</ref>, but with a single algorithm). As a reminder, the users here are not experts in machine learning and are unfamiliar with feature engineering, thus are only identifying words based on their semantic content. Further, users do not have any access to the religion dataset -they do not even know of its existence. We start the experiment with 10 subjects. After they mark words for deletion, we train 10 different classifiers, one for each subject (with the corresponding words removed). The explanations for each classifier are then presented to a set of 5 users in a new round of interaction, which results in 50 new classifiers. We do a final round, after which we have 250 classifiers, each with a path of interaction tracing back to the first 10 subjects.</p><p>The explanations and instances shown to each user are produced by SP-LIME or RP-LIME. We show the average accuracy on the religion dataset at each interaction round for the paths originating from each of the original 10 subjects (shaded lines), and the average across all paths (solid lines) in Figure <ref target=\"#fig_2\" type=\"figure\">10</ref>. It is clear from the figure that the crowd workers are able to improve the model by removing features they deem unimportant for the task. Further, SP-LIME outperforms RP-LIME, indicating selection of the instances to show the users is crucial for efficient feature engineering.</p><p>Each subject took an average of 3.6 minutes per round of cleaning, resulting in just under 11 minutes to produce a classifier that generalizes much better to real world data. Each path had on average 200 words removed with SP, and 157 with RP, indicating that incorporating coverage of important features is useful for feature engineering. Further, out of an average of 200 words selected with SP, 174 were selected by at least half of the users, while 68 by all the users. Along with the fact that the variance in the accuracy decreases across rounds, this high agreement demonstrates that the users are converging to similar correct models. This evaluation is an example of how explanations make it easy to improve an untrustworthy classifier -in this case easy enough that machine learning knowledge is not required.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6.4\">Do explanations lead to insights?</head><p>Often artifacts of data collection can induce undesirable correlations that the classifiers pick up during training. These issues can be very difficult to identify just by looking at the raw data and predictions. In an effort to reproduce such a setting, we take the task of distinguishing between photos of Wolves and Eskimo Dogs (huskies). We train a logistic regression classifier on a training set of 20 images, hand selected such that all pictures of wolves had snow in the background, while pictures of huskies did not. As the features for the images, we use the first max-pooling layer of Google's pre-trained Inception neural network <ref target=\"#b24\" type=\"bibr\">[25]</ref>. On a collection of additional 60 images, the classifier predicts \"Wolf\" if there is snow (or light background at the bottom), and \"Husky\" otherwise, regardless of animal color, position, pose, etc. We trained this bad classifier intentionally, to evaluate whether subjects are able to detect it.</p><p>The experiment proceeds as follows: we first present a balanced set of 10 test predictions (without explanations), where one wolf is not in a snowy background (and thus the prediction is \"Husky\") and one husky is (and is thus predicted as \"Wolf\"). We show the \"Husky\" mistake in Figure <ref target=\"#fig_2\" type=\"figure\">11a</ref>. The other 8 examples are classified correctly. We then ask the subject three questions: (1) Do they trust this algorithm   to work well in the real world, (2) why, and (3) how do they think the algorithm is able to distinguish between these photos of wolves and huskies. After getting these responses, we show the same images with the associated explanations, such as in Figure <ref target=\"#fig_2\" type=\"figure\">11b</ref>, and ask the same questions.</p><p>Since this task requires some familiarity with the notion of spurious correlations and generalization, the set of subjects for this experiment were graduate students who have taken at least one graduate machine learning course. After gathering the responses, we had 3 independent evaluators read their reasoning and determine if each subject mentioned snow, background, or equivalent as a feature the model may be using. We pick the majority to decide whether the subject was correct about the insight, and report these numbers before and after showing the explanations in Table <ref target=\"#tab_1\" type=\"table\">2</ref>.</p><p>Before observing the explanations, more than a third trusted the classifier, and a little less than half mentioned the snow pattern as something the neural network was using -although all speculated on other patterns. After examining the explanations, however, almost all of the subjects identified the correct insight, with much more certainty that it was a determining factor. Further, the trust in the classifier also dropped substantially. Although our sample size is small, this experiment demonstrates the utility of explaining individual predictions for getting insights into classifiers knowing when not to trust them and why.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"7.\">RELATED WORK</head><p>The problems with relying on validation set accuracy as the primary measure of trust have been well studied. Practitioners consistently overestimate their model's accuracy <ref target=\"#b19\" type=\"bibr\">[20]</ref>, propagate feedback loops <ref target=\"#b22\" type=\"bibr\">[23]</ref>, or fail to notice data leaks <ref target=\"#b13\" type=\"bibr\">[14]</ref>. In order to address these issues, researchers have proposed tools like Gestalt <ref target=\"#b20\" type=\"bibr\">[21]</ref> and Modeltracker <ref target=\"#b0\" type=\"bibr\">[1]</ref>, which help users navigate individual instances. These tools are complementary to LIME in terms of explaining models, since they do not address the problem of explaining individual predictions. Further, our submodular pick procedure can be incorporated in such tools to aid users in navigating larger datasets. Some recent work aims to anticipate failures in machine learning, specifically for vision tasks <ref target=\"#b2\" type=\"bibr\">[3,</ref><ref target=\"#b28\" type=\"bibr\">29]</ref>. Letting users know when the systems are likely to fail can lead to an increase in trust, by avoiding \"silly mistakes\" <ref target=\"#b7\" type=\"bibr\">[8]</ref>. These solutions either require additional annotations and feature engineering that is specific to vision tasks or do not provide insight into why a decision should not be trusted. Furthermore, they assume that the current evaluation metrics are reliable, which may not be the case if problems such as data leakage are present. Other recent work <ref target=\"#b10\" type=\"bibr\">[11]</ref> focuses on exposing users to different kinds of mistakes (our pick step). Interestingly, the subjects in their study did not notice the serious problems in the 20 newsgroups data even after looking at many mistakes, suggesting that examining raw data is not sufficient. Note that Groce et al. <ref target=\"#b10\" type=\"bibr\">[11]</ref> are not alone in this regard, many researchers in the field have unwittingly published classifiers that would not generalize for this task.</p><p>Using LIME, we show that even non-experts are able to identify these irregularities when explanations are present. Further, LIME can complement these existing systems, and allow users to assess trust even when a prediction seems \"correct\" but is made for the wrong reasons.</p><p>Recognizing the utility of explanations in assessing trust, many have proposed using interpretable models <ref target=\"#b26\" type=\"bibr\">[27]</ref>, especially for the medical domain <ref target=\"#b5\" type=\"bibr\">[6,</ref><ref target=\"#b16\" type=\"bibr\">17,</ref><ref target=\"#b25\" type=\"bibr\">26]</ref>. While such models may be appropriate for some domains, they may not apply equally well to others (e.g. a supersparse linear model <ref target=\"#b25\" type=\"bibr\">[26]</ref> with 5 âˆ’ 10 features is unsuitable for text applications). Interpretability, in these cases, comes at the cost of flexibility, accuracy, or efficiency. For text, EluciDebug <ref target=\"#b15\" type=\"bibr\">[16]</ref> is a full human-in-the-loop system that shares many of our goals (interpretability, faithfulness, etc). However, they focus on an already interpretable model (Naive Bayes). In computer vision, systems that rely on object detection to produce candidate alignments <ref target=\"#b12\" type=\"bibr\">[13]</ref> or attention <ref target=\"#b27\" type=\"bibr\">[28]</ref> are able to produce explanations for their predictions. These are, however, constrained to specific neural network architectures or incapable of detecting \"non object\" parts of the images. Here we focus on general, model-agnostic explanations that can be applied to any classifier or regressor that is appropriate for the domain -even ones that are yet to be proposed.</p><p>A common approach to model-agnostic explanation is learning a potentially interpretable model on the predictions of the original model <ref target=\"#b1\" type=\"bibr\">[2,</ref><ref target=\"#b6\" type=\"bibr\">7,</ref><ref target=\"#b21\" type=\"bibr\">22]</ref>. Having the explanation be a gradient vector <ref target=\"#b1\" type=\"bibr\">[2]</ref> captures a similar locality intuition to that of LIME. However, interpreting the coefficients on the gradient is difficult, particularly for confident predictions (where gradient is near zero). Further, these explanations approximate the original model globally, thus maintaining local fidelity becomes a significant challenge, as our experiments demonstrate. In contrast, LIME solves the much more feasible task of finding a model that approximates the original model locally. The idea of perturbing inputs for explanations has been explored before <ref target=\"#b23\" type=\"bibr\">[24]</ref>, where the authors focus on learning a specific contribution model, as opposed to our general framework. None of these approaches explicitly take cognitive limitations into account, and thus may produce non-interpretable explanations, such as a gradients or linear models with thousands of non-zero weights. The problem becomes worse if the original features are nonsensical to humans (e.g. word embeddings). In contrast, LIME incorporates interpretability both in the optimization and in our notion of interpretable representation, such that domain and task specific interpretability criteria can be accommodated.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"8.\">CONCLUSION AND FUTURE WORK</head><p>In this paper, we argued that trust is crucial for effective human interaction with machine learning systems, and that explaining individual predictions is important in assessing trust. We proposed LIME, a modular and extensible approach to faithfully explain the predictions of any model in an interpretable manner. We also introduced SP-LIME, a method to select representative and non-redundant predictions, providing a global view of the model to users. Our experiments demonstrated that explanations are useful for a variety of models in trust-related tasks in the text and image domains, with both expert and non-expert users: deciding between models, assessing trust, improving untrustworthy models, and getting insights into predictions.</p><p>There are a number of avenues of future work that we would like to explore. Although we describe only sparse linear models as explanations, our framework supports the exploration of a variety of explanation families, such as decision trees; it would be interesting to see a comparative study on these with real users. One issue that we do not mention in this work was how to perform the pick step for images, and we would like to address this limitation in the future. The domain and model agnosticism enables us to explore a variety of applications, and we would like to investigate potential uses in speech, video, and medical domains, as well as recommendation systems. Finally, we would like to explore theoretical properties (such as the appropriate number of samples) and computational optimizations (such as using parallelization and GPU processing), in order to provide the accurate, real-time explanations that are critical for any human-in-the-loop machine learning system.</p></div><figure xml:id=\"fig_0\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>KDD 2016 San Francisco, CA, USA c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4232-2/16/08. . . $15.00 DOI: http://dx.doi.org/10.1145/2939672.2939778 how much the human understands a model's behaviour, as opposed to seeing it as a black box.</figDesc></figure>\n",
       " <figure xml:id=\"fig_2\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Explaining individual predictions.A model predicts that a patient has the flu, and LIME highlights the symptoms in the patient's history that led to the prediction. Sneeze and headache are portrayed as contributing to the \"flu\" prediction, while \"no fatigue\" is evidence against it. With these, a doctor can make an informed decision about whether to trust the model's prediction.</figDesc></figure>\n",
       " <figure xml:id=\"fig_3\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Toy example to present intuition for LIME.The black-box model's complex decision function f (unknown to LIME) is represented by the blue/pink background, which cannot be approximated well by a linear model. The bold red cross is the instance being explained. LIME samples instances, gets predictions using f , and weighs them by the proximity to the instance being explained (represented here by size). The dashed line is the learned explanation that is locally (but not globally) faithful.</figDesc></figure>\n",
       " <figure xml:id=\"fig_4\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Explaining an image classification prediction made by Google's Inception neural network. The top 3 classes predicted are \"Electric Guitar\" (p = 0.32), \"Acoustic guitar\" (p = 0.24) and \"Labrador\" (p = 0.21)</figDesc></figure>\n",
       " <figure xml:id=\"fig_6\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Toy example W. Rows represent instances (documents) and columns represent features (words). Feature f2 (dotted blue) has the highest importance. Rows 2 and 5 (in red) would be selected by the pick procedure, covering all but feature f1.</figDesc></figure>\n",
       " <figure xml:id=\"fig_8\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Recall on truly important features for two interpretable classifiers on the books dataset.</figDesc></figure>\n",
       " <figure xml:id=\"fig_9\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Recall on truly important features for two interpretable classifiers on the DVDs dataset.</figDesc></figure>\n",
       " <figure xml:id=\"fig_11\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Choosing between two classifiers, as the number of instances shown to a simulated user is varied. Averages and standard errors from 800 runs.</figDesc></figure>\n",
       " <figure xml:id=\"fig_12\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Figure 9: Average accuracy of human subject (with standard errors) in choosing between two classifiers.</figDesc></figure>\n",
       " <figure xml:id=\"fig_14\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 11 :</head><label>11</label><figDesc>Figure11: Raw data and explanation of a bad model's prediction in the \"Husky vs Wolf \" task.</figDesc></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_0\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 1 :</head><label>1</label><figDesc>Average F1 of trustworthiness for different explainers on a collection of classifiers and datasets.</figDesc><table><row><cell/><cell/><cell/><cell cols=\"2\">Books</cell><cell>DVDs</cell></row><row><cell/><cell/><cell/><cell cols=\"2\">LR NN RF SVM</cell><cell>LR NN RF SVM</cell></row><row><cell cols=\"5\">Random 14.6 14.8 14.7 14.7</cell><cell>14.2 14.3 14.5 14.4</cell></row><row><cell cols=\"2\">Parzen</cell><cell cols=\"3\">84.0 87.6 94.3 92.3</cell><cell>87.0 81.7 94.2 87.3</cell></row><row><cell cols=\"2\">Greedy</cell><cell cols=\"3\">53.7 47.4 45.0 53.3</cell><cell>52.4 58.1 46.6 55.1</cell></row><row><cell cols=\"2\">LIME</cell><cell cols=\"3\">96.6 94.5 96.2 96.7 96.6 91.8 96.1 95.6</cell></row><row><cell/><cell>85</cell><cell/><cell/></row><row><cell>% correct choice</cell><cell>65 45</cell><cell/><cell/><cell>SP-LIME RP-LIME SP-greedy RP-greedy</cell></row><row><cell/><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell></row><row><cell/><cell cols=\"4\"># of instances seen by the user</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_1\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 2 :</head><label>2</label><figDesc>\"Husky vs Wolf \" experiment results.</figDesc><table/></figure>\n",
       " <note n=\"1\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">For multiple classes, we explain each class separately, thus f (x) is the prediction of the relevant class.</note>\n",
       " </body>\n",
       " <back>\n",
       " <div type=\"acknowledgement\">\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Acknowledgements</head><p>We would like to thank Scott Lundberg, Tianqi Chen, and Tyler Johnson for helpful discussions and feedback. This work was supported in part by ONR awards #W911NF-13-1-0246 and #N00014-13-1-0023, and in part by TerraSwarm, one of six centers of STARnet, a Semiconductor Research Corporation program sponsored by MARCO and DARPA.</p></div>\n",
       " </div>\n",
       " <div type=\"references\">\n",
       " <listBibl>\n",
       " <biblStruct xml:id=\"b0\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Modeltracker: Redesigning performance analysis tools for machine learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Amershi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Chickering</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><forename type=\"middle\">M</forename><surname>Drucker</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Simard</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Suh</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Human Factors in Computing Systems (CHI)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b1\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">How to explain individual classification decisions</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Baehrens</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Schroeter</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Harmeling</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Kawanabe</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Hansen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K.-R</forename><surname>MÃ¼ller</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Journal of Machine Learning Research</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">11</biblScope>\n",
       " <date type=\"published\" when=\"2010\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b2\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Towards transparent systems: Semantic characterization of failure modes</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Bansal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Farhadi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Parikh</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">European Conference on Computer Vision (ECCV)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b3\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Blitzer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Dredze</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Pereira</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Association for Computational Linguistics (ACL)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2007\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b4\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Dataset Shift in Machine Learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><forename type=\"middle\">Q</forename><surname>Candela</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Sugiyama</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Schwaighofer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><forename type=\"middle\">D</forename><surname>Lawrence</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2009\"/>\n",
       " <publisher>MIT</publisher>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b5\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Caruana</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Lou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Gehrke</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Koch</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Sturm</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Elhadad</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Knowledge Discovery and Data Mining (KDD)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b6\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Extracting tree-structured representations of trained networks</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><forename type=\"middle\">W</forename><surname>Craven</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><forename type=\"middle\">W</forename><surname>Shavlik</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Neural information processing systems (NIPS)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"1996\"/>\n",
       " <biblScope from=\"24\" to=\"30\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b7\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">The role of trust in automation reliance</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><forename type=\"middle\">T</forename><surname>Dzindolet</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><forename type=\"middle\">A</forename><surname>Peterson</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><forename type=\"middle\">A</forename><surname>Pomranky</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><forename type=\"middle\">G</forename><surname>Pierce</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><forename type=\"middle\">P</forename><surname>Beck</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Int. J. Hum.-Comput. Stud</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">58</biblScope>\n",
       " <biblScope unit=\"issue\">6</biblScope>\n",
       " <date type=\"published\" when=\"2003\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b8\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Least angle regression</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Efron</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Hastie</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><surname>Johnstone</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Tibshirani</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Annals of Statistics</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">32</biblScope>\n",
       " <biblScope from=\"407\" to=\"499\" unit=\"page\"/>\n",
       " <date type=\"published\" when=\"2004\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b9\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A threshold of ln n for approximating set cover</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">U</forename><surname>Feige</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">J. ACM</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">45</biblScope>\n",
       " <biblScope unit=\"issue\">4</biblScope>\n",
       " <date type=\"published\" when=\"1998-07\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b10\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">You are the only possible oracle: Effective test selection for end users of interactive machine learning systems</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Groce</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Kulesza</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Shamasunder</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Burnett</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W.-K</forename><surname>Wong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Stumpf</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Das</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Shinsel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Bice</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Mcintosh</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">IEEE Trans. Softw. Eng</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">40</biblScope>\n",
       " <biblScope unit=\"issue\">3</biblScope>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b11\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Explaining collaborative filtering recommendations</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><forename type=\"middle\">L</forename><surname>Herlocker</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><forename type=\"middle\">A</forename><surname>Konstan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Riedl</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Conference on Computer Supported Cooperative Work</title>\n",
       " <imprint>\n",
       " <publisher>CSCW</publisher>\n",
       " <date type=\"published\" when=\"2000\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b12\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Deep visual-semantic alignments for generating image descriptions</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Karpathy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Computer Vision and Pattern Recognition (CVPR)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b13\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Leakage in data mining: Formulation, detection, and avoidance</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Kaufman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Rosset</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Perlich</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Knowledge Discovery and Data Mining (KDD)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2011\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b14\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Submodular function maximization</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Krause</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Golovin</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Tractability: Practical Approaches to Hard Problems</title>\n",
       " <imprint>\n",
       " <publisher>Cambridge University Press</publisher>\n",
       " <date type=\"published\" when=\"2014-02\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b15\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Principles of explanatory debugging to personalize interactive machine learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Kulesza</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Burnett</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W.-K</forename><surname>Wong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Stumpf</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Intelligent User Interfaces (IUI)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b16\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Letham</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Rudin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><forename type=\"middle\">H</forename><surname>Mccormick</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Madigan</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Annals of Applied Statistics</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b17\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Explaining data-driven document classifications</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Martens</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Provost</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " <publisher>MIS Q</publisher>\n",
       " <biblScope unit=\"volume\">38</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b18\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Distributed representations of words and phrases and their compositionality</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Mikolov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><surname>Sutskever</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><forename type=\"middle\">S</forename><surname>Corrado</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Dean</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Neural Information Processing Systems (NIPS)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2013\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b19\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Investigating statistical machine learning as a tool for software development</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Patel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Fogarty</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><forename type=\"middle\">A</forename><surname>Landay</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Harrison</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Human Factors in Computing Systems (CHI)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2008\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b20\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Gestalt: Integrated support for implementation and analysis in machine learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Patel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Bancroft</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><forename type=\"middle\">M</forename><surname>Drucker</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Fogarty</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><forename type=\"middle\">J</forename><surname>Ko</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Landay</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">User Interface Software and Technology (UIST)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2010\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b21\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Towards extracting faithful and descriptive representations of latent variable models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><surname>Sanchez</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Rocktaschel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Riedel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Singh</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">AAAI Spring Syposium on Knowledge Representation and Reasoning (KRR): Integrating Symbolic and Neural Approaches</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b22\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Hidden technical debt in machine learning systems</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Sculley</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Holt</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Golovin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">E</forename><surname>Davydov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Phillips</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Ebner</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><surname>Chaudhary</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Young</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J.-F</forename><surname>Crespo</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Neural Information Processing Systems (NIPS)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b23\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">An efficient explanation of individual classifications using game theory</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">E</forename><surname>Strumbelj</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><surname>Kononenko</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Journal of Machine Learning Research</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">11</biblScope>\n",
       " <date type=\"published\" when=\"2010\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b24\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Going deeper with convolutions</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Szegedy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Jia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Sermanet</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Reed</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Anguelov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Erhan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><surname>Vanhoucke</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Rabinovich</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Computer Vision and Pattern Recognition (CVPR)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b25\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Supersparse linear integer models for optimized medical scoring systems</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Ustun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Rudin</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Machine Learning</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b26\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Falling rule lists</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Rudin</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Artificial Intelligence and Statistics (AISTATS)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b27\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Show, attend and tell: Neural image caption generation with visual attention</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Ba</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Kiros</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Cho</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Courville</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Salakhutdinov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Zemel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Bengio</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">International Conference on Machine Learning (ICML)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b28\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Predicting failures of vision systems</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Farhadi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Hebert</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Parikh</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Computer Vision and Pattern Recognition (CVPR)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " </listBibl>\n",
       " </div>\n",
       " </back>\n",
       " </text>\n",
       " </TEI>,\n",
       " <?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       " <TEI xml:space=\"preserve\" xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd\">\n",
       " <teiHeader xml:lang=\"en\">\n",
       " <fileDesc>\n",
       " <titleStmt>\n",
       " <title level=\"a\" type=\"main\">Preface to the book Fast Processes in Large Scale Atmospheric Models: Progress, Challenges and Opportunities</title>\n",
       " </titleStmt>\n",
       " <publicationStmt>\n",
       " <publisher>Authorea, Inc.</publisher>\n",
       " <availability status=\"unknown\"><p>Copyright Authorea, Inc.</p>\n",
       " </availability>\n",
       " <date type=\"published\" when=\"2024-02-28\">28 Feb 2024</date>\n",
       " </publicationStmt>\n",
       " <sourceDesc>\n",
       " <biblStruct>\n",
       " <analytic>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yixin</forename><surname>Liu</surname></persName>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Lehigh University</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kai</forename><surname>Zhang</surname></persName>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Lehigh University</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yuan</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhiling</forename><surname>Yan</surname></persName>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Lehigh University</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chujie</forename><surname>Gao</surname></persName>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Lehigh University</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ruoxi</forename><surname>Chen</surname></persName>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Lehigh University</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhengqing</forename><surname>Yuan</surname></persName>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Lehigh University</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yue</forename><surname>Huang</surname></persName>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Lehigh University</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hanchi</forename><surname>Sun</surname></persName>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Lehigh University</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jianfeng</forename><surname>Gao</surname></persName>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Lehigh University</orgName>\n",
       " </affiliation>\n",
       " <affiliation key=\"aff1\">\n",
       " <orgName type=\"department\">Microsoft Research</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lifang</forename><surname>He</surname></persName>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Lehigh University</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lichao</forename><surname>Sun</surname></persName>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Lehigh University</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <title level=\"a\" type=\"main\">Preface to the book Fast Processes in Large Scale Atmospheric Models: Progress, Challenges and Opportunities</title>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <imprint>\n",
       " <publisher>Authorea, Inc.</publisher>\n",
       " <date type=\"published\" when=\"2024-02-28\">28 Feb 2024</date>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <idno type=\"DOI\">10.22541/essoar.167252588.84737644/v2</idno>\n",
       " <idno type=\"arXiv\">arXiv:2402.17177v2[cs.CV]</idno>\n",
       " </biblStruct>\n",
       " </sourceDesc>\n",
       " </fileDesc>\n",
       " <encodingDesc>\n",
       " <appInfo>\n",
       " <application ident=\"GROBID\" version=\"0.7.0-SNAPSHOT\" when=\"2024-04-27T16:15+0000\">\n",
       " <desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>\n",
       " <ref target=\"https://github.com/kermitt2/grobid\"/>\n",
       " </application>\n",
       " </appInfo>\n",
       " </encodingDesc>\n",
       " <profileDesc>\n",
       " <abstract>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</p></div>\n",
       " </abstract>\n",
       " </profileDesc>\n",
       " </teiHeader>\n",
       " <text xml:lang=\"en\">\n",
       " <body>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>Since the release of ChatGPT in November 2022, the advent of AI technologies has marked a significant transformation, reshaping interactions and integrating deeply into various facets of daily life and industry <ref target=\"#b0\" type=\"bibr\">[1,</ref><ref target=\"#b1\" type=\"bibr\">2]</ref>. Building on this momentum, OpenAI released, in February 2024, Sora, a text-to-video generative AI model that can generate videos of realistic or imaginative scenes from text prompts. Compared to previous video generation models, Sora is distinguished by its ability to produce up to 1-minute long videos with high quality while maintaining adherence to user's text instructions <ref target=\"#b2\" type=\"bibr\">[3]</ref>. This progression of Sora is the embodiment of the long-standing AI research mission of equipping AI systems (or AI Agents) with the capability of understanding and interacting with the physical world in motion. This involves developing AI models that are capable of not only interpreting complex user instructions but also applying this understanding to solve real-world problems through dynamic and contextually rich simulations. Sora demonstrates a remarkable ability to accurately interpret and execute complex human instructions, as illustrated in Figure <ref target=\"#fig_0\" type=\"figure\">2</ref>. The model can generate detailed scenes that include multiple characters that perform specific actions against intricate backgrounds. Researchers attribute Sora's proficiency to not only processing user-generated textual prompts but also discerning the complicated interplay of elements within a scenario. One of the most striking aspects of Sora is its capacity for up to a minute-long video while maintaining high visual quality and compelling visual coherency. Unlike earlier models that can only generate short video clips, Sora's minute-long video creation possesses a sense of progression and a visually consistent journey from its first frame to the last. In addition, Sora's advancements are evident in its ability to produce extended video sequences with nuanced depictions of motion and interaction, overcoming the constraints of shorter clips and simpler visual renderings that characterized earlier video generation models. This capability represents a leap forward in AI-driven creative tools, allowing users to convert text narratives to rich visual stories. Overall, these advances show the potential of Sora as a world simulator to provide nuanced insights into the physical and contextual dynamics of the depicted scenes. <ref target=\"#b2\" type=\"bibr\">[3]</ref>.</p><p>Technology. At the heart of Sora is a pre-trained diffusion transformer <ref target=\"#b3\" type=\"bibr\">[4]</ref>. Transformer models have proven scalable and effective for many natural language tasks. Similar to powerful large language models (LLMs) such as GPT-4, Sora can parse text and comprehend complex user instructions. To make video generation computationally efficient, Sora employs spacetime latent patches as its building blocks. Specifically, Sora compresses a raw input video into a latent spacetime representation. Then, a sequence of latent spacetime patches is extracted from the compressed video to encapsulate both the visual appearance and motion dynamics over brief intervals. These patches, analogous to word tokens in language models, provide Sora with detailed visual phrases to be used to construct videos. Sora's text-to-video generation is performed by a diffusion transformer model. Starting with a frame filled with visual noise, the model iteratively denoises the image and introduces specific details according to the provided text prompt. In essence, the generated video emerges through a multi-step refinement process, with each step refining the video to be more aligned with the desired content and quality.</p><p>Highlights of Sora. Sora's capabilities have profound implications in various aspects:</p><p>â€¢ Improving simulation abilities: Training Sora at scale is attributed to its remarkable ability to simulate various aspects of the physical world. Despite lacking explicit 3D modeling, Sora exhibits 3D consistency with dynamic camera motion and long-range coherence that includes object persistence and simulates simple interactions with the world. Moreover, Sora intriguingly simulates digital environments like Minecraft, controlled by a basic policy while maintaining visual fidelity. These emergent abilities suggest that scaling video models is effective in creating AI models to simulate the complexity of physical and digital worlds. â€¢ Boosting creativity: Imagine outlining a concept through text, whether a simple object or a full scene, and seeing a realistic or highly stylized video rendered within seconds. Sora allows an accelerated design process for faster exploration and refinement of ideas, thus significantly boosting the creativity of artists, filmmakers, and designers. â€¢ Driving educational innovations: Visual aids have long been integral to understanding important concepts in education. With Sora, educators can easily turn a class plan from text to videos to captivate students' attention and improve learning efficiency. From scientific simulations to historical dramatizations, the possibilities are boundless. â€¢ Enhancing Accessibility: Enhancing accessibility in the visual domain is paramount. Sora offers an innovative solution by converting textual descriptions to visual content. This capability empowers all individuals, including those with visual impairments, to actively engage in content creation and interact with others in more effective ways. Consequently, it allows for a more inclusive environment where everyone has the opportunity to express his or her ideas through videos. â€¢ Fostering emerging applications: The applications of Sora are vast. For example, marketers might use it to create dynamic advertisements tailored to specific audience descriptions. Game developers might use it to generate customized visuals or even character actions from player narratives.</p><p>Limitations and Opportunities. While Sora's achievements highlight significant advancements in AI, challenges remain. Depicting complex actions or capturing subtle facial expressions are among the areas where the model could be enhanced. In addition, ethical considerations such as mitigating biases in generated content and preventing harmful visual outputs underscore the importance of responsible usage by developers, researchers, and the broader community. Ensuring that Sora's outputs are consistently safe and unbiased is a principal challenge. The field of video generation is advancing swiftly, with academic and industry research teams making relentless strides. The advent of competing text-to-video models suggests that Sora may soon be part of a dynamic ecosystem. This collaborative and competitive environment fosters innovation, leading to improved video quality and new applications that help improve the productivity of workers and make people's lives more entertaining.</p><p>Our Contributions. Based on published technical reports and our reverse engineering, this paper presents the first comprehensive review of Sora's background, related technologies, emerging applications, current limitations, and future opportunities.</p><p>2 Background</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.1\">History</head><p>In the realm of computer vision (CV), prior to the deep learning revolution, traditional image generation techniques relied on methods like texture synthesis <ref target=\"#b4\" type=\"bibr\">[5]</ref> and texture mapping <ref target=\"#b5\" type=\"bibr\">[6]</ref>, based on hand-crafted features. However, these methods were limited in their capacity to produce complex and vivid images. The introduction of Generative Adversarial Networks (GANs) <ref target=\"#b6\" type=\"bibr\">[7]</ref> and Variational Autoencoders (VAEs) <ref target=\"#b7\" type=\"bibr\">[8]</ref> marked a significant turning point due to its remarkable capabilities across various applications. Subsequent developments, such as flow models <ref target=\"#b8\" type=\"bibr\">[9]</ref> and diffusion models <ref target=\"#b9\" type=\"bibr\">[10]</ref>, further enhanced image generation with greater detail and quality. The recent progress in Artificial Intelligence Generated Content (AIGC) technologies has democratized content creation, enabling users to generate desired content through simple textual instructions <ref target=\"#b10\" type=\"bibr\">[11]</ref>.</p><p>Over the past decade, the development of generative CV models has taken various routes, as shown in Figure <ref target=\"#fig_1\" type=\"figure\">3</ref>. This landscape began to shift notably following the successful application of the transformer architecture <ref target=\"#b11\" type=\"bibr\">[12]</ref> in NLP, as demonstrated by BERT <ref target=\"#b12\" type=\"bibr\">[13]</ref> and GPT <ref target=\"#b13\" type=\"bibr\">[14]</ref>. In CV, researchers take this concept even further by combining the transformer architecture with visual components, allowing it to be applied to downstream CV tasks, such as Vision Transformer (ViT) <ref target=\"#b14\" type=\"bibr\">[15]</ref> and Swin Transformer <ref target=\"#b15\" type=\"bibr\">[16]</ref>. Parallel to the transformer's success, diffusion models have also made significant strides in the fields of image and video generation <ref target=\"#b9\" type=\"bibr\">[10]</ref>. Diffusion models offer a mathematically sound framework for converting noise into images with U-Nets <ref target=\"#b16\" type=\"bibr\">[17]</ref>, where U-Nets facilitate this process by learning to predict and mitigate noise at each step. Since 2021, a paramount focus in AI has been on generative language and vision models that are capable of interpreting human instructions, known as multimodal models. For example, CLIP <ref target=\"#b17\" type=\"bibr\">[18]</ref> is a pioneering vision-language model that combines transformer architecture with visual elements, facilitating its training on vast datasets of text and images. By integrating visual and linguistic knowledge from the outset, CLIP can function as an image encoder within multimodal generation frameworks. Another notable example is Stable Diffusion <ref target=\"#b18\" type=\"bibr\">[19]</ref>, a versatile text-to-image AI model celebrated for its adaptability and ease of use. It employs transformer architecture and latent diffusion techniques to decode textual inputs and produce images of a wide array of styles, further illustrating the advancements in multimodal AI.</p><p>Following the release of ChatGPT in November 2022, we have witnessed the emergence of commercial text-to-image products in 2023, such as Stable Diffusion <ref target=\"#b18\" type=\"bibr\">[19]</ref>, Midjourney <ref target=\"#b19\" type=\"bibr\">[20]</ref>, DALL-E 3 <ref target=\"#b20\" type=\"bibr\">[21]</ref>. These tools enable users to generate new images of high resolution and quality with simple text prompts, showcasing the potential of AI in creative image generation. However, transitioning from text-to-image to text-to-video is challenging due to the temporal complexity of videos. Despite numerous efforts in industry and academia, most existing video generation tools, such as Pika <ref target=\"#b21\" type=\"bibr\">[22]</ref> and Gen-2 <ref target=\"#b22\" type=\"bibr\">[23]</ref>, are limited to producing only short video clips of a few seconds. In this context, Sora represents a significant breakthrough, akin to ChatGPT's impact in the NLP domain. Sora is the first model that is capable of generating videos up to one minute long based on human instructions, marking a milestone that profoundly influences research and development in generative AI. To facilitate easy access to the latest advancements in vision generation models, the most recent works have been compiled and provided in the Appendix and our GitHub.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Advanced Concepts</head><p>Scaling Laws for Vision Models. With scaling laws for LLMs, it is natural to ask whether the development of vision models follows similar scaling laws. Recently, Zhai et al. <ref target=\"#b23\" type=\"bibr\">[24]</ref> have demonstrated that the performance-compute frontier for ViT models with enough training data roughly follows a (saturating) power law. Following them, Google Research <ref target=\"#b24\" type=\"bibr\">[25]</ref> presented a recipe for highly efficient and stable training of a 22B-parameter ViT. Results show that great performance can be achieved using the frozen model to produce embeddings, and then training thin layers on top. Sora, as a large vision model (LVM), aligns with these scaling principles, uncovering several emergent abilities in text-to-video generation. This significant progression underscores the potential for LVMs to achieve advancements like those seen in LLMs.</p><p>Emergent Abilities. Emergent abilities in LLMs are sophisticated behaviors or functions that manifest at certain scales-often linked to the size of the model's parameters-that were not explicitly programmed or anticipated by their developers. These abilities are termed \"emergent\" because they emerge from the model's comprehensive training across varied datasets, coupled with its extensive parameter count. This combination enables the model to form connections and draw inferences that surpass mere pattern recognition or rote memorization. Typically, the emergence of these abilities cannot be straightforwardly predicted by extrapolating from the performance of smaller-scale models. While numerous LLMs, such as Chat-GPT and GPT-4, exhibit emergent abilities, vision models demonstrating comparable capabilities have been scarce until the advent of Sora. According to Sora's technical report, it is the first vision model to exhibit confirmed emergent abilities, marking a significant milestone in the field of computer vision.</p><p>In addition to its emergent abilities, Sora exhibits other notable capabilities, including instruction following, visual prompt engineering, and video understanding. These aspects of Sora's functionality represent significant advancements in the vision domain and will be explored and discussed in the rest sections.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3\">Technology</head></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1\">Overview of Sora</head><p>In the core essence, Sora is a diffusion transformer <ref target=\"#b3\" type=\"bibr\">[4]</ref> with flexible sampling dimensions as shown in Figure <ref type=\"figure\">4</ref>. It has three parts: (1) A time-space compressor first maps the original video into latent space.</p><p>(2) A ViT then processes the tokenized latent representation and outputs the denoised latent representation.</p><p>(3) A CLIP-like <ref target=\"#b25\" type=\"bibr\">[26]</ref> conditioning mechanism receives LLM-augmented user instructions and potentially visual prompts to guide the diffusion model to generate styled or themed videos. After many denoising Figure <ref type=\"figure\">4</ref>: Reverse Engineering: Overview of Sora framework steps, the latent representation of the generated video is obtained and then mapped back to pixel space with the corresponding decoder. In this section, we aim to reverse engineer the technology used by Sora and discuss a wide range of related works.</p><p>3.2 Data Pre-processing 3.2.1 Variable Durations, Resolutions, Aspect Ratios One distinguishing feature of Sora is its ability to train on, understand, and generate videos and images at their native sizes <ref target=\"#b2\" type=\"bibr\">[3]</ref> as illustrated in Figure <ref target=\"#fig_2\" type=\"figure\">5</ref>. Traditional methods often resize, crop, or adjust the aspect ratios of videos to fit a uniform standard-typically short clips with square frames at fixed low resolutions <ref target=\"#b26\" type=\"bibr\">[27]</ref>[28] <ref target=\"#b28\" type=\"bibr\">[29]</ref>. Those samples are often generated at a wider temporal stride and rely on separately trained frame-insertion and resolution-rendering models as the final step, creating inconsistency across the video. Utilizing the diffusion transformer architecture <ref target=\"#b3\" type=\"bibr\">[4]</ref> (see Section 3.2.4), Sora is the first model to embrace the diversity of visual data and can sample in a wide array of video and image formats, ranging from widescreen 1920x1080p videos to vertical 1080x1920p videos and everything in between without compromising their original dimensions.  Training on data in their native sizes significantly improves composition and framing in the generated videos. Empirical findings suggest that by maintaining the original aspect ratios, Sora achieves a more natural and coherent visual narrative. The comparison between Sora and a model trained on uniformly cropped square videos demonstrates a clear advantage as shown in Figure <ref target=\"#fig_3\" type=\"figure\">6</ref>. Videos produced by Sora exhibit better framing, ensuring subjects are fully captured in the scene, as opposed to the sometimes truncated views resulting from square cropping. This nuanced understanding and preservation of original video and image characteristics mark a significant advancement in the field of generative models. Sora's approach not only showcases the potential for more authentic and engaging video generation but also highlights the importance of diversity in training data for achieving high-quality results in generative AI. The training approach of Sora aligns with the core tenet of Richard Sutton's THE BITTER LESSON <ref target=\"#b29\" type=\"bibr\">[30]</ref>, which states that leveraging computation over human-designed features leads to more effective and flexible AI systems. Just as the original design of diffusion transformers seeks simplicity and scalability <ref target=\"#b30\" type=\"bibr\">[31]</ref>, Sora's strategy of training on data at their native sizes eschews traditional AI reliance on human-derived abstractions, favoring instead a generalist method that scales with computational power. In the rest of this section, we try to reverse engineer the architecture design of Sora and discuss related technologies to achieve this amazing feature.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2.2\">Unified Visual Representation</head><p>To effectively process diverse visual inputs including images and videos with varying durations, resolutions, and aspect ratios, a crucial approach involves transforming all forms of visual data into a unified representation, which facilitates the large-scale training of generative models. Specifically, Sora patchifies videos by initially compressing videos into a lower-dimensional latent space, followed by decomposing the representation into spacetime patches. However, Sora's technical report <ref target=\"#b2\" type=\"bibr\">[3]</ref> merely presents a high-level idea, making reproduction challenging for the research community. In this section, we try to reverse-engineer the potential ingredients and technical pathways. Additionally, we will discuss viable alternatives that could replicate Sora's functionalities, drawing upon insights from existing literature. Figure <ref type=\"figure\">7</ref>: At a high level, Sora turns videos into patches by first compressing videos into a lowerdimensional latent space, and subsequently decomposing the representation into spacetime patches. Source: Sora's technical report <ref target=\"#b2\" type=\"bibr\">[3]</ref>. Sora's video compression network (or visual encoder) aims to reduce the dimensionality of input data, especially a raw video, and output a latent representation that is compressed both temporally and spatially as shown in Figure <ref type=\"figure\">7</ref>. According to the references in the technical report, the compression network is built upon VAE or Vector Quantised-VAE (VQ-VAE) <ref target=\"#b31\" type=\"bibr\">[32]</ref>. However, it is challenging for VAE to map visual data of any size to a unified and fixed-sized latent space if resizing and cropping are not used as mentioned in the technical report. We summarize two distinct implementations to address this issue: Spatial-patch Compression. This involves transforming video frames into fixed-size patches, akin to the methodologies employed in ViT <ref target=\"#b14\" type=\"bibr\">[15]</ref> and MAE <ref target=\"#b32\" type=\"bibr\">[33]</ref> (see Figure <ref target=\"#fig_4\" type=\"figure\">8</ref>), before encoding them into a latent space. This approach is particularly effective for accommodating videos of varying resolutions and aspect ratios, as it encodes entire frames through the processing of individual patches. Subsequently, these spatial tokens are organized in a temporal sequence to create a spatial-temporal latent representation. This technique highlights several critical considerations: Temporal dimension variability -given the varying durations of training videos, the temporal dimension of the latent space representation cannot be fixed. To address this, one can either sample a specific number of frames (padding or temporal interpolation <ref target=\"#b33\" type=\"bibr\">[34]</ref> may be needed for much shorter videos) or define a universally extended (super long) input length for subsequent processing (more details are described in Section 3.2.4); Utilization of pre-trained visual encoders -for processing videos of high resolution, leveraging existing pre-trained visual encoders, such as the VAE encoder from Stable Diffusion <ref target=\"#b18\" type=\"bibr\">[19]</ref>, is advisable for most researchers while Sora's team is expected to train their own compression network with a decoder (the video generator) from scratch via the manner employed in training latent diffusion models <ref target=\"#b18\" type=\"bibr\">[19,</ref><ref target=\"#b34\" type=\"bibr\">35,</ref><ref target=\"#b35\" type=\"bibr\">36]</ref>. These encoders can efficiently compress large-size patches (e.g., 256 Ã— 256), facilitating the management of large-scale data; Temporal information aggregation -since this method primarily focuses on spatial patch compression, it necessitates an additional mechanism for aggregating temporal information within the model. This aspect is crucial for capturing dynamic changes over time and is further elaborated in subsequent sections (see details in Section 3.3.1 and Figure <ref target=\"#fig_11\" type=\"figure\">14</ref>).</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2.3\">Video Compression Network</head></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Spatial-temporal-patch Compression.</head><p>This technique is designed to encapsulate both spatial and temporal dimensions of video data, offering a comprehensive representation. This technique extends beyond merely analyzing static frames by considering the movement and changes across frames, thereby capturing the video's dynamic aspects. The utilization of 3D convolution emerges as a straightforward and potent method for achieving this integration <ref target=\"#b36\" type=\"bibr\">[37]</ref>. The graphical illustration and the comparison against pure spatial-pachifying are depicted in Figure <ref target=\"#fig_5\" type=\"figure\">9</ref>. Similar to spatial-patch compression, employing spatialtemporal-patch compression with predetermined convolution kernel parameters -such as fixed kernel sizes, strides, and output channels -results in variations in the dimensions of the latent space due to the differing characteristics of video inputs. This variability is primarily driven by the diverse durations and resolutions of the videos being processed. To mitigate this challenge, the approaches adopted for spatial patchification are equally applicable and effective in this context. In summary, we reverse engineer the two patch-level compression approaches based on VAE or its variant like VQ-VQE because operations on patches are more flexible to process different types of videos. Since Sora aims to generate high-fidelity videos, a large patch size or kernel size is used for efficient compression. Here, we expect that fixed-size patches are used for simplicity, scalability, and training stability. But varying-size patches could also be used <ref target=\"#b38\" type=\"bibr\">[39]</ref> to make the dimension of the whole frames or videos in latent space consistent. However, it may result in invalid positional encoding, and cause challenges for the decoder to generate videos with varying-size latent patches. </p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2.4\">Spacetime Latent Patches</head><p>There is a pivotal concern remaining in the compression network part: How to handle the variability in latent space dimensions (i.e., the number of latent feature chunks or patches from different video types) before feeding patches into the input layers of the diffusion transformer. Here, we discuss several solutions.</p><p>Based on Sora's technical report and the corresponding references, patch n' pack (PNP) <ref target=\"#b39\" type=\"bibr\">[40]</ref> is likely the solution. PNP packs multiple patches from different images in a single sequence as shown in Figure <ref target=\"#fig_6\" type=\"figure\">10</ref>. This method is inspired by example packing used in natural language processing <ref target=\"#b40\" type=\"bibr\">[41]</ref> that accommodates efficient training on variable length inputs by dropping tokens. Here the patchification and token embedding steps need to be completed in the compression network, but Sora may further patchify the latent for transformer token as Diffusion Transformer does <ref target=\"#b3\" type=\"bibr\">[4]</ref>. Regardless there is a second-round patchification or not, we need to address two concerns, how to pack those tokens in a compact manner and how to control which tokens should be dropped. For the first concern, a simple greedy approach is used which adds examples to the first sequence with enough remaining space. Once no more example can fit, sequences are filled with padding tokens, yielding the fixed sequence lengths needed for batched operations. Such a simple packing algorithm can lead to significant padding, depending on the distribution of the length of inputs. On the other hand, we can control the resolutions and frames we sample to ensure efficient packing by tuning the sequence length and limiting padding. For the second concern, an intuitive approach is to drop the similar tokens <ref target=\"#b41\" type=\"bibr\">[42,</ref><ref target=\"#b42\" type=\"bibr\">43,</ref><ref target=\"#b32\" type=\"bibr\">33,</ref><ref target=\"#b43\" type=\"bibr\">44]</ref> or, like PNP, apply dropping rate schedulers. However, it is worth noting that 3D Consistency is one of the nice properties of Sora. Dropping tokens may ignore fine-grained details during training. Thus, we believe that OpenAI is likely to use a super long context window and pack all tokens from videos although doing so is computationally expensive e.g., the multi-head attention <ref target=\"#b44\" type=\"bibr\">[45,</ref><ref target=\"#b45\" type=\"bibr\">46]</ref> operator exhibits quadratic cost in sequence length. Specifically, spacetime latent patches from a long-duration video can be packed in one sequence while the ones from several short-duration videos are concatenated in the other sequence.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2.5\">Discussion</head><p>We discuss two technical solutions to data pre-processing that Sora may use. Both solutions are performed at the patch level due to the characteristics of flexibility and scalability for modeling. Different from previous approaches where videos are resized, cropped, or trimmed to a standard size, Sora trains on data at its native size. Although there are several benefits (see detailed analysis in Section 3.2.1), it brings some technical challenges, among which one of the most significant is that neural networks cannot inherently process visual data of variable durations, resolutions, and aspect ratios. Through reverse engineering, we believe that Sora firstly compresses visual patches into low-dimensional latent representations, and arranges such latent patches or further patchified latent patches in a sequence, then injects noise into these latent patches .</p><p>before feeding them to the input layer of diffusion transformer. Spatial-temporal patchification is adopted by Sora because it is simple to implement, and it can effectively reduce the context length with highinformation-density tokens and decrease the complexity of subsequent modeling of temporal information.</p><p>To the research community, we recommend using cost-efficient alternative solutions for video compression and representation, including utilizing pre-trained checkpoints (e.g., compression network) <ref target=\"#b46\" type=\"bibr\">[47]</ref>, shortening the context window, using light-weight modeling mechanisms such as (grouped) multi-query attention <ref target=\"#b47\" type=\"bibr\">[48,</ref><ref target=\"#b48\" type=\"bibr\">49]</ref> or efficient architectures (e.g. Mamba <ref target=\"#b49\" type=\"bibr\">[50]</ref>), downsampling data and dropping tokens if necessary. The trade-off between effectiveness and efficiency for video modeling is an important research topic to be explored.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.3\">Modeling</head></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.3.1\">Diffusion Transformer</head><p>Image Diffusion Transformer. Traditional diffusion models <ref target=\"#b50\" type=\"bibr\">[51,</ref><ref target=\"#b51\" type=\"bibr\">52,</ref><ref target=\"#b52\" type=\"bibr\">53]</ref> mainly leverage convolutional U-Nets that include downsampling and upsampling blocks for the denoising network backbone. However, recent studies show that the U-Net architecture is not crucial to the good performance of the diffusion model. By incorporating a more flexible transformer architecture, the transformer-based diffusion models can use more training data and larger model parameters. Along this line, DiT <ref target=\"#b3\" type=\"bibr\">[4]</ref> and U-ViT <ref target=\"#b53\" type=\"bibr\">[54]</ref> are among the first works to employ vision transformers for latent diffusion models. As in ViT, DiT employs a multi-head selfattention layer and a pointwise feed-forward network interlaced with some layer norm and scaling layers. Moreover, as shown in Figure <ref target=\"#fig_7\" type=\"figure\">11</ref>, DiT incorporates conditioning via adaptive layer norm (AdaLN) with an additional MLP layer for zero-initializing, which initializes each residual block as an identity function and thus greatly stabilizes the training process. The scalability and flexibility of DiT is empirically validated. DiT becomes the new backbone for diffusion models. In U-ViT, as shown in Figure <ref target=\"#fig_7\" type=\"figure\">11</ref>, they treat all inputs, including time, condition, and noisy image patches, as tokens and propose long skip connections between the shallow and deep transformer layers. The results suggest that the downsampling and upsampling operators in CNN-based U-Net are not always necessary, and U-ViT achieves record-breaking FID scores in image and text-to-image generation.</p><p>Like Masked AutoEncoder (MAE) <ref target=\"#b32\" type=\"bibr\">[33]</ref>, Masked Diffusion Transformer (MDT) <ref target=\"#b54\" type=\"bibr\">[55]</ref> incorporates mask latent modeling into the diffusion process to explicitly enhance contextual relation learning among object semantic parts in image synthesis. Specifically, as shown in Figure <ref target=\"#fig_0\" type=\"figure\">12</ref>, MDT uses a side-interpolated for an additional masked token reconstruction task during training to boost the training efficiency and learn powerful context-aware positional embedding for inference. Compared to DiT <ref target=\"#b3\" type=\"bibr\">[4]</ref>, MDT achieves better performance and faster learning speed. Instead of using AdaLN (i.e., shifting and scaling) for time-conditioning modeling, Hatamizadeh et al. <ref target=\"#b55\" type=\"bibr\">[56]</ref> introduce Diffusion Vision Transformers (DiffiT), which uses a timedependent self-attention (TMSA) module to model dynamic denoising behavior over sampling time steps. Besides, DiffiT uses two hybrid hierarchical architectures for efficient denoising in the pixel space and the latent space, respectively, and achieves new state-of-the-art results across various generation tasks. Overall, these studies show promising results in employing vision transformers for image latent diffusion, paving the way for future studies for other modalities. Video Diffusion Transformer. Building upon the foundational works in text-to-image (T2I) diffusion models, recent research has been focused on realizing the potential of diffusion transformers for text-to-video (T2V) generation tasks. Due to the temporal nature of videos, key challenges for applying DiTs in the video domain are: i) how to compress the video spatially and temporally to a latent space for efficient denoising; ii) how to convert the compressed latent to patches and feed them to the transformer; and iii) how to handle long-range temporal and spatial dependencies and ensure content consistency. Please refer to Section 3.2.3 for the first challenge. In this Section, we focus our discussion on transformer-based denoising network architectures designed to operate in the spatially and temporally compressed latent space. We give a detailed review of the two important works (Imagen Video <ref target=\"#b28\" type=\"bibr\">[29]</ref> and Video LDM <ref target=\"#b35\" type=\"bibr\">[36]</ref>) described in the reference list of the OpenAI Sora technique report.</p><p>Imagen Video <ref target=\"#b28\" type=\"bibr\">[29]</ref>, a text-to-video generation system developed by Google Research, utilizes a cascade of diffusion models, which consists of 7 sub-models that perform text-conditional video generation, spatial super-resolution, and temporal super-resolution, to transform textual prompts into high-definition videos. As shown in Figure <ref target=\"#fig_1\" type=\"figure\">13</ref>, firstly, a frozen T5 text encoder generates contextual embeddings from the input text prompt. These embeddings are critical for aligning the generated video with the text prompt and are injected into all models in the cascade, in addition to the base model. Subsequently, the embedding is fed to the base model for low-resolution video generation, which is then refined by cascaded diffusion models to increase the resolution. The base video and super-resolution models use a 3D U-Net architecture in a spacetime separable fashion. This architecture weaves temporal attention and convolution layers with spatial counterparts to efficiently capture inter-frame dependencies. It employs v-prediction parameterization for numerical stability and conditioning augmentation to facilitate parallel training across models. The process involves joint training on both images and videos, treating each image as a frame to leverage larger datasets, and using classifier-free guidance <ref target=\"#b56\" type=\"bibr\">[57]</ref> to enhance prompt fidelity. Progressive distillation <ref target=\"#b57\" type=\"bibr\">[58]</ref> is applied to streamline the sampling process, significantly reducing the computational load while maintaining perceptual quality. Combining these methods and techniques allows Imagen Video to generate videos with not only high fidelity but also remarkable controllability, as demonstrated by its ability to produce diverse videos, text animations, and content in various artistic styles.  Blattmann et al. <ref target=\"#b35\" type=\"bibr\">[36]</ref> propose to turn a 2D Latent Diffusion Model into a Video Latent Diffusion Model (Video LDM). They achieve this by adding some post-hoc temporal layers among the existing spatial layers into both the U-Net backbone and the VAE decoder that learns to align individual frames. These temporal layers are trained on encoded video data, while the spatial layers remain fixed, allowing the model to leverage large image datasets for pre-training. The LDM's decoder is fine-tuned for temporal consistency in pixel space and temporally aligning diffusion model upsamplers for enhanced spatial resolution. To generate very long videos, models are trained to predict a future frame given a number of context frames, allowing for classifier-free guidance during sampling. To achieve high temporal resolution, the video synthesis process is divided into key frame generation and interpolation between these key frames. Following cascaded LDMs, a DM is used to further scale up the Video LDM outputs by 4 times, ensuring high spatial resolution while maintaining temporal consistency. This approach enables the generation of globally coherent long videos in a computationally efficient manner. Additionally, the authors demonstrate the ability to transform pre-trained image LDMs (e.g., Stable Diffusion) into text-to-video models by training only the temporal alignment layers, achieving video synthesis with resolutions up to 1280 Ã— 2048.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.3.2\">Discussion</head><p>Cascade diffusion models for spatial and temporal up-sampling. Sora can generate high-resolution videos. By reviewing existing works and our reverse engineering, we speculate that Sora also leverages cascade diffusion model architecture <ref target=\"#b58\" type=\"bibr\">[59]</ref> which is composed of a base model and many space-time refiner models. The attention modules are unlikely to be heavily used in the based diffusion model and lowresolution diffusion model, considering the high computation cost and limited performance gain of using attention machines in high-resolution cases. For spatial and temporal scene consistency, as previous works show that temporal consistency is more important than spatial consistency for video/scene generation, Sora is likely to leverage an efficient training strategy by using longer video (for temporal consistency) with lower resolution. Moreover, Sora is likely to use a v-parameterization diffusion model <ref target=\"#b57\" type=\"bibr\">[58]</ref>, considering its superior performance compared to other variants that predict the original latent x or the noise Ïµ.</p><p>On the latent encoder. For training efficiency, most of the existing works leverage the pre-trained VAE encoder of Stable Diffusions <ref target=\"#b59\" type=\"bibr\">[60,</ref><ref target=\"#b60\" type=\"bibr\">61]</ref>, a pre-trained 2D diffusion model, as an initialized model checkpoint. However, the encoder lacks the temporal compression ability. Even though some works propose to only fine-tune the decoder for handling temporal information, the decoder's performance of dealing with video temporal data in the compressed latent space remains sub-optimal. Based on the technique report, our reverse engineering shows that, instead of using an existing pre-trained VAE encoder, it is likely that Sora uses a space-time VAE encoder, trained from scratch on video data, which performs better than existing ones with a video-orient compressed latent space.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.4\">Language Instruction Following</head><p>Users primarily engage with generative AI models through natural language instructions, known as text prompts <ref target=\"#b61\" type=\"bibr\">[62,</ref><ref target=\"#b62\" type=\"bibr\">63]</ref>. Model instruction tuning aims to enhance AI models' capability to follow prompts accurately. This improved capability in prompt following enables models to generate output that more closely resembles human responses to natural language queries. We start our discussion with a review of instruction following techniques for large language models (LLMs) and text-to-image models such as DALLâ€¢E 3. To enhance the text-to-video model's ability to follow text instructions, Sora utilizes an approach similar to that of DALLâ€¢E 3. The approach involves training a descriptive captioner and utilizing the captioner's generated data for fine-tuning. As a result of instruction tuning, Sora is able to accommodate a wide range of user requests, ensuring meticulous attention to the details in the instructions and generating videos that precisely meet users' needs.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.4.1\">Large Language Models</head><p>The capability of LLMs to follow instructions has been extensively explored <ref target=\"#b63\" type=\"bibr\">[64,</ref><ref target=\"#b64\" type=\"bibr\">65,</ref><ref target=\"#b65\" type=\"bibr\">66]</ref>. This ability allows LLMs to read, understand, and respond appropriately to instructions describing an unseen task without examples. Prompt following ability is obtained and enhanced by fine-tuning LLMs on a mixture of tasks formatted as instructions <ref target=\"#b63\" type=\"bibr\">[64,</ref><ref target=\"#b65\" type=\"bibr\">66]</ref>, known as instruction tuning. Wei et al. <ref target=\"#b64\" type=\"bibr\">[65]</ref> showed that instruction-tuned LLMs significantly outperform the untuned ones on unseen tasks. The instruction-following ability transforms LLMs into general-purpose task solvers, marking a paradigm shift in the history of AI development.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.4.2\">Text-to-Image</head><p>The instruction following in DALLâ€¢E 3 is addressed by a caption improvement method with a hypothesis that the quality of text-image pairs that the model is trained on determines the performance of the resultant text-to-image model <ref target=\"#b66\" type=\"bibr\">[67]</ref>. The poor quality of data, particularly the prevalence of noisy data and short captions that omit a large amount of visual information, leads to many issues such as neglecting keywords and word order, and misunderstanding the user intentions <ref target=\"#b20\" type=\"bibr\">[21]</ref>. The caption improvement approach addresses these issues by re-captioning existing images with detailed, descriptive captions. The approach first trains an image captioner, which is a vision-language model, to generate precise and descriptive image captions. The resulting descriptive image captions by the captioner are then used to fine-tune text-to-image models. Specifically, DALLâ€¢E 3 follows contrastive captioners (CoCa) <ref target=\"#b67\" type=\"bibr\">[68]</ref> to jointly train an image captioner with a CLIP <ref target=\"#b25\" type=\"bibr\">[26]</ref> architecture and a language model objective. This image captioner incorporates an image encoder a unimodal text encoder for extracting language information, and a multimodal text decoder. It first employs a contrastive loss between unimodal image and text embeddings, followed by a captioning loss for the multimodal decoder's outputs. The resulting image captioner is further fine-tuned on a highly detailed description of images covering main objects, surroundings, backgrounds, texts, styles, and colorations. With this step, the image captioner is able to generate detailed descriptive captions for the images. The training dataset for the text-to-image model is a mixture of the re-captioned dataset generated by the image captioner and ground-truth human-written data to ensure that the model captures user inputs. This image caption improvement method introduces a potential issue: a mismatch between the actual user prompts and descriptive image descriptions from the training data. DALLâ€¢E 3 addresses this by upsampling, where LLMs are used to re-write short user prompts into detailed and lengthy instructions. This ensures that the model's text inputs received in inference time are consistent with those in model training.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.4.3\">Text-to-Video</head><p>To enhance the ability of instruction following, Sora adopts a similar caption improvement approach. This method is achieved by first training a video captioner capable of producing detailed descriptions for videos. Then, this video captioner is applied to all videos in the training data to generate high-quality (video, descriptive caption) pairs, which are used to fine-tune Sora to improve its instruction following ability.</p><p>Sora's technical report <ref target=\"#b2\" type=\"bibr\">[3]</ref> does not reveal the details about how the video captioner is trained. Given that the video captioner is a video-to-text model, there are many approaches to building it. A straightforward approach is to utilize CoCa architecture for video captioning by taking multiple frames of a video and feeding each frame into the image encoder <ref target=\"#b67\" type=\"bibr\">[68]</ref>, known as VideoCoCa <ref target=\"#b68\" type=\"bibr\">[69]</ref>. VideoCoCa builds upon CoCa and re-uses the image encoder pre-trained weights and applies it independently on sampled video frames. The resulting frame token embeddings are flattened and concatenated into a long sequence of video representations. These flattened frame tokens are then processed by a generative pooler and a contrastive pooler, which are jointly trained with the contrastive loss and captioning loss. Other alternatives to building video captioners include mPLUG-2 <ref target=\"#b69\" type=\"bibr\">[70]</ref>, GIT <ref target=\"#b70\" type=\"bibr\">[71]</ref>, FrozenBiLM <ref target=\"#b71\" type=\"bibr\">[72]</ref>, and more. Finally, to ensure that user prompts align with the format of those descriptive captions in training data, Sora performs an additional prompt extension step, where GPT-4V is used to expand user inputs to detailed descriptive prompts.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.4.4\">Discussion</head><p>The instruction-following ability is critical for Sora to generate one-minute-long videos with intricate scenes that are faithful to user intents. According to Sora's technical report <ref target=\"#b2\" type=\"bibr\">[3]</ref>, this ability is obtained by developing a captioner that can generate long and detailed captions, which are then used to train the model. However, the process of collecting data for training such a captioner is unknown and likely laborintensive, as it may require detailed descriptions of videos. Moreover, the descriptive video captioner might hallucinate important details of the videos. We believe that how to improve the video captioner warrants further investigation and is critical to enhance the instruction-following ability of text-to-image models.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.5\">Prompt Engineering</head><p>Prompt engineering refers to the process of designing and refining the input given to an AI system, particularly in the context of generative models, to achieve specific or optimized outputs <ref target=\"#b72\" type=\"bibr\">[73,</ref><ref target=\"#b73\" type=\"bibr\">74,</ref><ref target=\"#b74\" type=\"bibr\">75]</ref>. The art and science of prompt engineering involve crafting these inputs in a way that guides the model to produce the most accurate, relevant, and coherent responses possible.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.5.1\">Text Prompt</head><p>Text prompt engineering is vital in directing text-to-video models (e.g., Sora <ref target=\"#b2\" type=\"bibr\">[3]</ref>) to produce videos that are visually striking while precisely meeting user specifications. This involves crafting detailed descriptions to instruct the model to effectively bridge the gap between human creativity and AI's execution capabilities <ref target=\"#b75\" type=\"bibr\">[76]</ref>. The prompts for Sora cover a wide range of scenarios. Recent works (e.g., VoP <ref target=\"#b76\" type=\"bibr\">[77]</ref>, Make-A-Video <ref target=\"#b27\" type=\"bibr\">[28]</ref>, and Tune-A-Video <ref target=\"#b77\" type=\"bibr\">[78]</ref>) have shown how prompt engineering leverages model's natural language understanding ability to decode complex instructions and render them into cohesive, lively, and high-quality video narratives. As shown in Figure <ref target=\"#fig_2\" type=\"figure\">15</ref>, \"a stylish woman walking down a neon-lit Tokyo street...\" is such a meticulously crafted text prompt that it ensures Sora to generate a video that aligns well with the expected vision. The quality of prompt engineering depends on the careful selection of words, the specificity of the details provided, and comprehension of their impact on the model's output. For example, the prompt in Figure <ref target=\"#fig_2\" type=\"figure\">15</ref> specifies in detail the actions, settings, character appearances, and even the desired mood and atmosphere of the scene. Figure <ref target=\"#fig_2\" type=\"figure\">15:</ref> A case study on prompt engineering for text-to-video generation, employing color coding to delineate the creative process. The text highlighted in blue describes the elements generated by Sora, such as the depiction of a stylish woman. In contrast, the text in yellow accentuates the model's interpretation of actions, settings, and character appearances, demonstrating how a meticulously crafted prompt is transformed into a vivid and dynamic video narrative.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.5.2\">Image Prompt</head><p>An image prompt serves as a visual anchor for the to-be-generated video's content and other elements such as characters, setting, and mood <ref target=\"#b78\" type=\"bibr\">[79]</ref>. In addition, a text prompt can instruct the model to animate these elements by e.g., adding layers of movement, interaction, and narrative progression that bring the static image to life <ref target=\"#b26\" type=\"bibr\">[27,</ref><ref target=\"#b79\" type=\"bibr\">80,</ref><ref target=\"#b80\" type=\"bibr\">81]</ref>. The use of image prompts allows Sora to convert static images into dynamic, narrative-driven videos by leveraging both visual and textual information. In Figure <ref target=\"#fig_3\" type=\"figure\">16</ref>, we show AI-generated videos of \"a Shiba Inu wearing a beret and turtleneck\", \"a unique monster family\", \"a cloud forming the word 'SORA\"' and \"surfers navigating a tidal wave inside a historic hall\". These examples demonstrate what can be achieved by prompting Sora with DALLâ€¢E-generated images. Figure <ref target=\"#fig_3\" type=\"figure\">16</ref>: This example illustrates the image prompts to guide Sora's text-to-video model to generation. The red boxes visually anchor the key elements of each scene-monsters of varied designs, a cloud formation spelling \"SORA\", and surfers in an ornate hall facing a massive tidal wave.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.5.3\">Video Prompt</head><p>Video prompts can also be used for video generation as demonstrated in <ref target=\"#b81\" type=\"bibr\">[82,</ref><ref target=\"#b82\" type=\"bibr\">83]</ref>. Recent works (e.g., Moonshot <ref target=\"#b83\" type=\"bibr\">[84]</ref> and Fast-Vid2Vid <ref target=\"#b84\" type=\"bibr\">[85]</ref>) show that good video prompts need to be specific and flexible. This ensures that the model receives clear direction on specific objectives, like the portrayal of particular objects and visual themes, and also allows for imaginative variations in the final output. For example, in the video extension tasks, a prompt could specify the direction (forward or backward in time) and the context or theme of the extension. In Figure <ref target=\"#fig_12\" type=\"figure\">17</ref>(a), the video prompt instructs Sora to extend a video backward in time to explore the events leading up to the original starting point. When performing video-to-video editing through video prompts, as shown in Figure <ref target=\"#fig_12\" type=\"figure\">17(b</ref>), the model needs to clearly understand the desired transformation, such as changing the video's style, setting or atmosphere, or altering subtle aspects like lighting or mood. In Figure <ref target=\"#fig_12\" type=\"figure\">17(c</ref>), the prompt instructs Sora to connect videos while ensuring smooth transitions between objects in different scenes across videos.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.5.4\">Discussion</head><p>Prompt engineering allows users to guide AI models to generate content that aligns with their intent. As an example, the combined use of text, image, and video prompts enables Sora to create content that is not only visually compelling but also aligned well with users' expectations and intent. While previous studies on prompt engineering have been focused on text and image prompts for LLMs and LVMs <ref target=\"#b85\" type=\"bibr\">[86,</ref><ref target=\"#b86\" type=\"bibr\">87,</ref><ref target=\"#b87\" type=\"bibr\">88]</ref>, we expect that there will be a growing interest in video prompts for video generation models.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.6\">Trustworthiness</head><p>With the rapid advancement of sophisticated models such as ChatGPT <ref target=\"#b88\" type=\"bibr\">[89]</ref>, GPT4-V <ref target=\"#b89\" type=\"bibr\">[90]</ref>, and Sora <ref target=\"#b2\" type=\"bibr\">[3]</ref>, the capabilities of these models have seen remarkable enhancements. These developments have made sig- nificant contributions to improving work efficiency and propelling technological progress. However, these advancements also raise concerns about the potential for misuse of these technologies, including the generation of fake news <ref target=\"#b90\" type=\"bibr\">[91,</ref><ref target=\"#b91\" type=\"bibr\">92]</ref>, privacy breaches <ref target=\"#b92\" type=\"bibr\">[93]</ref>, and ethical dilemmas <ref target=\"#b93\" type=\"bibr\">[94,</ref><ref target=\"#b94\" type=\"bibr\">95]</ref>. Consequently, the issue of trustworthiness in large models has garnered extensive attention from both the academic and industrial spheres, emerging as a focal point of contemporary research discussions.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.6.1\">Safety Concern</head><p>One primary area of focus is the model's safety, specifically its resilience against misuse and so-called \"jailbreak\" attacks, where users attempt to exploit vulnerabilities to generate prohibited or harmful content <ref target=\"#b95\" type=\"bibr\">[96,</ref><ref target=\"#b96\" type=\"bibr\">97,</ref><ref target=\"#b97\" type=\"bibr\">98,</ref><ref target=\"#b98\" type=\"bibr\">99,</ref><ref target=\"#b99\" type=\"bibr\">100,</ref><ref target=\"#b100\" type=\"bibr\">101,</ref><ref target=\"#b101\" type=\"bibr\">102,</ref><ref target=\"#b102\" type=\"bibr\">103,</ref><ref target=\"#b103\" type=\"bibr\">104,</ref><ref target=\"#b104\" type=\"bibr\">105]</ref>. For instance, AutoDAN <ref target=\"#b102\" type=\"bibr\">[103]</ref>, a novel and interpretable adversarial attack method based on gradient techniques, is introduced to enable system bypass. In a recent study, researchers explore two reasons why LLMs struggle to resist jailbreak attacks: competing objectives and mismatched generalization <ref target=\"#b105\" type=\"bibr\">[106]</ref>. Besides textual attacks, visual jailbreak also threatens the safety of multimodal models (e.g., GPT-4V <ref target=\"#b89\" type=\"bibr\">[90]</ref>, and Sora <ref target=\"#b2\" type=\"bibr\">[3]</ref>). A recent study <ref target=\"#b106\" type=\"bibr\">[107]</ref> found that large multimodal models are more vulnerable since the continuous and high-dimensional nature of the additional visual input makes it weaker against adversarial attacks, representing an expanded attack surface.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.6.2\">Other Exploitation</head><p>Due to the large scale of the training dataset and training methodology of large foundation models (e.g., ChatGPT <ref target=\"#b88\" type=\"bibr\">[89]</ref> and Sora <ref target=\"#b2\" type=\"bibr\">[3]</ref>), the truthfulness of these models needs to be enhanced as the related issues like hallucination have been discussed widely <ref target=\"#b107\" type=\"bibr\">[108]</ref>. Hallucination in this context refers to the models' tendency to generate responses that may appear convincing but are unfounded or false <ref target=\"#b95\" type=\"bibr\">[96]</ref>. This phenomenon raises critical questions about the reliability and trustworthiness of model outputs, necessitating a comprehensive approach to both evaluate and address the issue. Amount of studies have been dedicated to dissecting the problem of hallucination from various angles. This includes efforts aimed at evaluating the extent and nature of hallucination across different models and scenarios <ref target=\"#b108\" type=\"bibr\">[109,</ref><ref target=\"#b95\" type=\"bibr\">96,</ref><ref target=\"#b109\" type=\"bibr\">110,</ref><ref target=\"#b110\" type=\"bibr\">111]</ref>. These evaluations provide invaluable insights into how and why hallucinations occur, laying the groundwork for developing strategies to mitigate their incidence. Concurrently, a significant body of research is focused on devising and implementing methods to reduce hallucinations in these large models <ref target=\"#b111\" type=\"bibr\">[112,</ref><ref target=\"#b112\" type=\"bibr\">113,</ref><ref target=\"#b113\" type=\"bibr\">114]</ref>.</p><p>Another vital aspect of trustworthiness is fairness and bias. The critical importance of developing models that do not perpetuate or exacerbate societal biases is a paramount concern. This priority stems from the recognition that biases encoded within these models can reinforce existing social inequities, leading to discriminatory outcomes. Studies in this area, as evidenced by the work of Gallegos et al. <ref target=\"#b114\" type=\"bibr\">[115]</ref>, Zhang et al. <ref target=\"#b115\" type=\"bibr\">[116]</ref>, Liang et al. <ref target=\"#b116\" type=\"bibr\">[117]</ref>, and Friedrich et al. <ref target=\"#b117\" type=\"bibr\">[118]</ref>, are dedicated to the meticulous identification and rectification of these inherent biases. The goal is to cultivate models that operate fairly, treating all individuals equitably without bias towards race, gender, or other sensitive attributes. This involves not only detecting and mitigating bias in datasets but also designing algorithms that can actively counteract the propagation of such biases <ref target=\"#b118\" type=\"bibr\">[119,</ref><ref target=\"#b119\" type=\"bibr\">120]</ref>.</p><p>Privacy preservation emerges as another foundational pillar when these models are deployed. In an era where data privacy concerns are escalating, the emphasis on protecting user data has never been more critical. The increasing public awareness and concern over how personal data is handled have prompted more rigorous evaluations of large models. These evaluations focus on the models' capacity to protect user data, ensuring that personal information remains confidential and is not inadvertently disclosed. Research by Mireshghallah et al. <ref target=\"#b120\" type=\"bibr\">[121]</ref>, Plant et al. <ref target=\"#b121\" type=\"bibr\">[122]</ref>, and Li et al. <ref target=\"#b122\" type=\"bibr\">[123]</ref> exemplify efforts to advance methodologies and technologies that safeguard privacy.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.6.3\">Alignment</head><p>In addressing these challenges, ensuring the trustworthiness of large models has become one of the primary concerns for researchers <ref target=\"#b123\" type=\"bibr\">[124,</ref><ref target=\"#b95\" type=\"bibr\">96,</ref><ref target=\"#b98\" type=\"bibr\">99,</ref><ref target=\"#b124\" type=\"bibr\">125]</ref>. Among the most important technologies is model alignment <ref target=\"#b124\" type=\"bibr\">[125,</ref><ref target=\"#b125\" type=\"bibr\">126]</ref>, which refers to the process and goal of ensuring that the behavior and outputs of models are consistent with the intentions and ethical standards of human designers. This concerns the development of technology, its moral responsibilities, and social values. In the domain of LLMs, the method of Reinforcement Learning with Human Feedback (RLHF) <ref target=\"#b126\" type=\"bibr\">[127,</ref><ref target=\"#b127\" type=\"bibr\">128]</ref> has been widely applied for model alignment. This method combines Reinforcement Learning (RL) with direct human feedback, allowing models to better align with human expectations and standards in understanding and performing tasks.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.6.4\">Discussion</head><p>From Sora (specifically its technical report), we summarize some insightful findings that potentially offer an informative guideline for future work:</p><p>(1) Integrated Protection of Model and External Security: As models become more powerful, especially in generating content, ensuring that they are not misused to produce harmful content (such as hate speech <ref target=\"#b128\" type=\"bibr\">[129]</ref> and false information <ref target=\"#b91\" type=\"bibr\">[92,</ref><ref target=\"#b90\" type=\"bibr\">91]</ref>) has become a serious challenge. In addition to aligning the model itself, external security protections are equally important. This includes content filtering and review mechanisms, usage permissions and access control, data privacy protection, as well as enhancements in transparency and explainability. For instance, OpenAI now uses a detection classifier to tell whether a given video is generated by Sora <ref target=\"#b129\" type=\"bibr\">[130]</ref>. Moreover, a text classifier is deployed to detect the potentially harmful textual input <ref target=\"#b129\" type=\"bibr\">[130]</ref>.</p><p>(2) Security Challenges of Multimodal Models: Multimodal models, such as text-to-video models like Sora bring additional complexity to security due to their ability to understand and generate various types of content (text, images, videos, etc.). Multimodal models can produce content in various forms, increasing the ways and scope of misuse and copyright issues. As the content generated by multimodal models is more complex and diverse, traditional methods of content verification and authenticity may no longer be effective. This requires the development of new technologies and methods to identify and filter harmful content generated by these models, increasing the difficulty of regulation and management. (3) The Need for Interdisciplinary Collaboration: Ensuring the safety of models is not just a technical issue but also requires cross-disciplinary cooperation. To address these challenges, experts from various fields such as law <ref target=\"#b130\" type=\"bibr\">[131]</ref> and psychology <ref target=\"#b131\" type=\"bibr\">[132]</ref> need to work together to develop appropriate norms (e.g., what's the safety and what's unsafe?), policies, and technological solutions. The need for interdisciplinary collaboration significantly increases the complexity of solving these issues.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4\">Applications</head><p>As video diffusion models, exemplified by Sora, emerge as a forefront technology, their adoption across diverse research fields and industries is rapidly accelerating. The implications of this technology extend far beyond mere video creation, offering transformative potential for tasks ranging from automated content generation to complex decision-making processes. In this section, we delve into a comprehensive examination of the current applications of video diffusion models, highlighting key areas where Sora has not only demonstrated its capabilities but also revolutionized the approach to solving complex problems. We aim to offer a broad perspective for the practical deployment scenarios (see Figure <ref target=\"#fig_4\" type=\"figure\">18</ref>).</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1\">Movie</head><p>Traditionally, creating cinematic masterpieces has been an arduous and expensive process, often requiring decades of effort, cutting-edge equipment, and substantial financial investments. However, the advent of advanced video generation technologies heralds a new era in film-making, one where the dream of autonomously producing movies from simple text inputs is becoming a reality. Researchers have ventured into the realm of movie generation by extending video generation models into creating movies. MovieFactory <ref target=\"#b132\" type=\"bibr\">[133]</ref> applies diffusion models to generate film-style videos from elaborate scripts produced by Chat-GPT <ref target=\"#b88\" type=\"bibr\">[89]</ref>, representing a significant leap forward. In the follow-up, MobileVidFactory <ref target=\"#b133\" type=\"bibr\">[134]</ref> can automatically generate vertical mobile videos with only simple texts provided by users. Vlogger <ref target=\"#b134\" type=\"bibr\">[135]</ref> makes it feasible for users to compose a minute-long vlog. These developments, epitomized by Sora's ability to generate captivating movie content effortlessly, mark a pivotal moment in the democratization of movie production. They offer a glimpse into a future where anyone can be a filmmaker, significantly lowering the barriers to entry in the film industry and introducing a novel dimension to movie production that blends traditional storytelling with AI-driven creativity. The implications of these technologies extend beyond simplification. They promise to reshape the landscape of film production, making it more accessible and versatile in the face of evolving viewer preferences and distribution channels.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2\">Education</head><p>The landscape of educational content has long been dominated by static resources, which, despite their value, often fall short of catering to the diverse needs and learning styles of today's students. Video diffusion models stand at the forefront of an educational revolution, offering unprecedented opportunities to customize and animate educational materials in ways that significantly enhance learner engagement and understanding. These advanced technologies enable educators to transform text descriptions or curriculum outlines into dynamic, engaging video content tailored to the specific style, and interests of individual learners <ref target=\"#b135\" type=\"bibr\">[136,</ref><ref target=\"#b136\" type=\"bibr\">137,</ref><ref target=\"#b137\" type=\"bibr\">138,</ref><ref target=\"#b138\" type=\"bibr\">139]</ref>. Moreover, image-to-video editing techniques <ref target=\"#b139\" type=\"bibr\">[140,</ref><ref target=\"#b140\" type=\"bibr\">141,</ref><ref target=\"#b141\" type=\"bibr\">142]</ref> present innovative avenues for converting static educational assets into interactive videos, thereby supporting a range of learning preferences and potentially increasing student engagement. By integrating these models into educational content creation, educators can produce videos on a myriad of subjects, making complex concepts more accessible and captivating for students. The use of Sora in revolutionizing the educational domain exemplifies the transformative potential of these technologies. This shift towards personalized, dynamic educational content heralds a new era in education.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.3\">Gaming</head><p>The gaming industry constantly seeks ways to push the boundaries of realism and immersion, yet traditional game development often grapples with the limitations of pre-rendered environments and scripted events. The generation of dynamic, high-fidelity video content and realistic sound by diffusion models effects in real-time, promise to overcome existing constraints, offering developers the tools to create evolving game environments that respond organically to player actions and game events <ref target=\"#b142\" type=\"bibr\">[143,</ref><ref target=\"#b143\" type=\"bibr\">144]</ref>. This could include generating changing weather conditions, transforming landscapes, or even creating entirely new settings on the fly, making game worlds more immersive and responsive. Some methods <ref target=\"#b144\" type=\"bibr\">[145,</ref><ref target=\"#b145\" type=\"bibr\">146]</ref> also synthesize realistic impact sounds from video inputs, enhancing game audio experiences. With the integration of Sora within the gaming domain, unparalleled immersive experiences that captivate and engage players can be created. How games are developed, played, and experienced will be innovated, as well as opening new possibilities for storytelling, interaction, and immersion.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.4\">Healthcare</head><p>Despite generative capabilities, video diffusion models excel in understanding and generating complex video sequences, making them particularly suited for identifying dynamic anomalies within the body, such as early cellular apoptosis <ref target=\"#b146\" type=\"bibr\">[147]</ref>, skin lesion progression <ref target=\"#b147\" type=\"bibr\">[148]</ref>, and irregular human movements <ref target=\"#b148\" type=\"bibr\">[149]</ref>, which are crucial for early disease detection and intervention strategies. Additionally, models like MedSegDiff-V2 <ref target=\"#b149\" type=\"bibr\">[150]</ref> and <ref target=\"#b150\" type=\"bibr\">[151]</ref> leverage the power of transformers to segment medical images with unprecedented precision, enabling clinicians to pinpoint areas of interest across various imaging modalities with enhanced accuracy. The integration of Sora into clinical practice promises not only to refine diagnostic processes but also to personalize patient care, offering tailored treatment plans based on precise medical imaging analysis. However, this technological integration comes with its own set of challenges, including the need for robust data privacy measures and addressing ethical considerations in healthcare.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.5\">Robotics</head><p>Video diffusion models now play important roles in robotics, showing a new era where robots can generate and interpret complex video sequences for enhanced perception <ref target=\"#b151\" type=\"bibr\">[152,</ref><ref target=\"#b152\" type=\"bibr\">153]</ref> and decision-making <ref target=\"#b153\" type=\"bibr\">[154,</ref><ref target=\"#b154\" type=\"bibr\">155,</ref><ref target=\"#b155\" type=\"bibr\">156]</ref>. These models unlock new capabilities in robots, enabling them to interact with their environment and execute tasks with unprecedented complexity and precision. The introduction of web-scale diffusion models to robotics <ref target=\"#b151\" type=\"bibr\">[152]</ref> showcases the potential for leveraging large-scale models to enhance robotic vision and understanding. Latent diffusion models are employed for language-instructed video prediction <ref target=\"#b156\" type=\"bibr\">[157]</ref>, allowing robots to understand and execute tasks by predicting the outcome of actions in video format. Furthermore, the reliance on simulated environments for robotics research has been innovatively addressed by video diffusion models capable of creating highly realistic video sequences <ref target=\"#b157\" type=\"bibr\">[158,</ref><ref target=\"#b158\" type=\"bibr\">159]</ref>. This enables the generation of diverse training scenarios for robots, mitigating the limitations imposed by the scarcity of realworld data. We believe, the integration of technologies like Sora into the robotics field holds the promise of groundbreaking developments. By harnessing the power of Sora, the future of robotics is poised for unprecedented advancements, where robots can seamlessly navigate and interact with their environments.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5\">Discussion</head><p>Sora shows a remarkable talent for precisely understanding and implementing complex instructions from humans. This model excels at creating detailed videos with various characters, all set within elaborately crafted settings. A particularly impressive attribute of Sora is its ability to produce videos up to one minute in length while ensuring consistent and engaging storytelling. This marks a significant improvement over previous attempts that focused on shorter video pieces, as Sora's extended sequences exhibit a clear narrative flow and maintain visual consistency from start to finish. Furthermore, Sora distinguishes itself by generating longer video sequences that capture complex movements and interactions, advancing past the restrictions of earlier models that could only handle short clips and basic images. This advancement signifies a major step forward in AI-powered creative tools, enabling users to transform written stories into vivid videos with a level of detail and sophistication that was previously unattainable.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">Limitations</head><p>Challenges in Physical Realism. Sora, as a simulation platform, exhibits a range of limitations that undermine its effectiveness in accurately depicting complex scenarios. Most important is its inconsistent handling of physical principles within complex scenes, leading to a failure in accurately copying specific examples of cause and effect. For instance, consuming a portion of a cookie might not result in a corresponding bite mark, illustrating the system's occasional departure from physical plausibility. This issue extends to the simulation of motion, where Sora generates movements that challenge realistic physical modeling, such as unnatural transformations of objects or the incorrect simulation of rigid structures like chairs, leading to unrealistic physical interactions. The challenge further increases when simulating complex interactions among objects and characters, occasionally producing outcomes that lean towards the humorous.</p><p>Spatial and Temporal Complexities. Sora occasionally misunderstands instructions related to the placement or arrangement of objects and characters within a given prompt, leading to confusion about directions (e.g., confusing left for right). Additionally, it faces challenges in maintaining the temporal accuracy of events, particularly when it comes to adhering to designated camera movements or sequences. This can result in deviating from the intended temporal flow of scenes. In complex scenarios that involve a multitude of characters or elements, Sora has a tendency to insert irrelevant animals or people. Such additions can significantly change the originally envisioned composition and atmosphere of the scene, moving away from the planned narrative or visual layout. This issue not only affects the model's ability to accurately recreate specific scenes or narratives but also impacts its reliability in generating content that closely aligns with the user's expectations and the coherence of the generated output.</p><p>Limitations in Human-computer Interaction (HCI). Sora, while showing potential in the video generation domain, faces significant limitations in HCI. These limitations are primarily evident in the coherence and efficiency of user-system interactions, especially when making detailed modifications or optimizations to generated content. For instance, users might find it difficult to precisely specify or adjust the presentation of specific elements within a video, such as action details and scene transitions. Additionally, Sora's limi-tations in understanding complex language instructions or capturing subtle semantic differences could result in video content that does not fully meet user expectations or needs. These shortcomings restrict Sora's potential in video editing and enhancement, also impacting the overall satisfaction of the user experience.</p><p>Usage Limitation. Regarding usage limitations, OpenAI has not yet set a specific release date for public access to Sora, emphasizing a cautious approach towards safety and readiness before broad deployment. This indicates that further improvements and testing in areas such as security, privacy protection, and content review may still be necessary for Sora. Moreover, at present, Sora can only generate videos up to one minute in length, and according to published cases, most generated videos are only a few dozen seconds long. This limitation restricts its use in applications requiring longer content display, such as detailed instructional videos or in-depth storytelling. This limitation reduces Sora's flexibility in the content creation.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.2\">Opportunities</head><p>Academy. (1) The introduction of Sora by OpenAI marks a strategic shift towards encouraging the broader AI community to delve deeper into the exploration of text-to-video models, leveraging both diffusion and transformer technologies. This initiative aims to redirect the focus toward the potential of creating highly sophisticated and nuanced video content directly from textual descriptions, a frontier that promises to revolutionize content creation, storytelling, and information sharing. <ref target=\"#b1\" type=\"bibr\">(2)</ref> The innovative approach of training Sora on data at its native size, as opposed to the traditional methods of resizing or cropping, serves as a groundbreaking inspiration for the academic community. It opens up new pathways by highlighting the benefits of utilizing unmodified datasets, which leads to the creation of more advanced generative models.</p><p>Industry. (1) The current capabilities of Sora signal a promising path for the advancement of video simulation technologies, highlighting the potential to significantly enhance realism within both physical and digital areas. The prospect of Sora enabling the creation of highly realistic environments through textual descriptions presents a promising future for content creation. This potential extends to revolutionizing game development, offering a glimpse into a future where immersive-generated worlds can be crafted with unprecedented ease and accuracy. (2) Companies may leverage Sora to produce advertising videos that swiftly adapt to market changes and create customized marketing content. This not only reduces production costs but also enhances the appeal and effectiveness of advertisements. The ability of Sora to generate highly realistic video content from textual descriptions alone could revolutionize how brands engage with their audience, allowing for the creation of immersive and compelling videos that capture the essence of their products or services in unprecedented ways.</p><p>Society. (1) While the prospect of utilizing text-to-video technology to replace traditional filmmaking remains distant, Sora and similar platforms hold transformative potential for content creation on social media. The constraints of current video lengths do not diminish the impact these tools can have in making high-quality video production accessible to everyone, enabling individuals to produce compelling content without the need for expensive equipment. It represents a significant shift towards empowering content creators across platforms like TikTok and Reels, bringing in a new age of creativity and engagement. (2) Screenwriters and creative professionals can use Sora to transform written scripts into videos, assisting them in better showing and sharing their creative concepts, and even in producing short films and animations. The ability to create detailed, vivid videos from scripts can fundamentally change the pre-production process of filmmaking and animation, offering a glimpse into how future storytellers might pitch, develop, and refine their narratives. This technology opens up possibilities for a more dynamic and interactive form of script development, where ideas can be visualized and assessed in real time, providing a powerful tool for creativity and collaboration. (3) Journalists and news organizations can also utilize Sora to quickly generate news reports or explanatory videos, making the news content more vivid and engaging. This can significantly increase the coverage and audience engagement of news reports. By providing a tool that can simulate realistic environments and scenarios, Sora offers a powerful solution for visual storytelling, enabling journalists to convey complex stories through engaging videos that were previously difficult or expensive to produce. In summary, Sora's potential to revolutionize content creation across marketing, journalism, and entertainment is immense.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6\">Conclusion</head><p>We present a comprehensive review of Sora to help developers and researchers study the capabilities and related works of Sora. The review is based on our survey of published technical reports and reverse engineering based on existing literature. We will continue to update the paper when Sora's API is available and further details about Sora are revealed. We hope that this review paper will prove a valuable resource for the open-source research community and lay a foundation for the community to jointly develop an opensource version of Sora in the near future to democratize video auto-creation in the era of AIGC. To achieve this goal, we invite discussions, suggestions, and collaborations on all fronts.</p></div><figure xml:id=\"fig_0\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of Sora in text-to-video generation. Text instructions are given to the OpenAI Sora model, and it generates three videos according to the instructions.</figDesc></figure>\n",
       " <figure xml:id=\"fig_1\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: History of Generative AI in Vision Domain.</figDesc></figure>\n",
       " <figure xml:id=\"fig_2\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Sora can generate images in flexible sizes or resolutions ranging from 1920x1080p to 1080x1920p and anything in between.</figDesc></figure>\n",
       " <figure xml:id=\"fig_3\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: A comparison between Sora (right) and a modified version of the model (left), which crops videos to square shapes-a common practice in model training-highlights the advantages.</figDesc></figure>\n",
       " <figure xml:id=\"fig_4\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: ViT splits an image into fixedsize patches, linearly embeds each of them, adds position embeddings, and feeds the resulting sequence of vectors to a standard Transformer encoder.</figDesc></figure>\n",
       " <figure xml:id=\"fig_5\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Comparison between different patchification for video compression. Source: ViViT [38]. (Left) Spatial patchification simply samples n t frames and embeds each 2D frame independently following ViT. (Right) Spatial-temporal patchification extracts and linearly embeds non-overlapping or overlapping tubelets that span the spatiotemporal input volume.</figDesc></figure>\n",
       " <figure xml:id=\"fig_6\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 10 :</head><label>10</label><figDesc>Figure10: Patch packing enables variable resolution images or videos with preserved aspect ratio.6 Token dropping somehow could be treated as data augmentation. Source: NaViT<ref target=\"#b39\" type=\"bibr\">[40]</ref>.</figDesc></figure>\n",
       " <figure xml:id=\"fig_7\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: The overall framework of DiT (left) and U-ViT (right).</figDesc></figure>\n",
       " <figure xml:id=\"fig_8\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: The overall framework of Masked Diffusion Transformer (MDT). A solid/dotted line indicates the training/inference process for each time step. Masking and side-interpolater are only used during training and are removed during inference.</figDesc></figure>\n",
       " <figure xml:id=\"fig_9\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: The overall framework of Imagen Video. Source: Imagen Video [29].</figDesc></figure>\n",
       " <figure xml:id=\"fig_10\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>( a )</head><label>a</label><figDesc>Additional temporal layer. A pre-trained LDM is turned into a video generator by inserting temporal layers that learn to align frames into temporally consistent sequences. During optimization, the image backbone Î¸ remains fixed and only the parameters Ï• of the temporal layers l i Ï• are trained. Video LDM stack. Video LDM first generates sparse key frames and then temporally interpolates twice with the same latent diffusion models to achieve a high frame rate. Finally, the latent video is decoded to pixel space, and optionally, a video upsampler diffusion model is applied.</figDesc></figure>\n",
       " <figure xml:id=\"fig_11\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: The overall framework of Video LDM. Source: Video LDM [36].</figDesc></figure>\n",
       " <figure xml:id=\"fig_12\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: These examples illustrate the video prompt techniques for Sora models: (a) Video Extension, where the model extrapolates the sequence backward the original footage, (b) Video Editing, where specific elements like the setting are transformed as per the text prompt, and (c) Video Connection, where two distinct video prompts are seamlessly blended to create a coherent narrative. Each process is guided by a visual anchor, marked by a red box, ensuring continuity and precision in the generated video content.</figDesc></figure>\n",
       " <figure xml:id=\"fig_13\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Applications of Sora.</figDesc></figure>\n",
       " </body>\n",
       " <back>\n",
       " <div type=\"annex\">\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A Related Works</head><p>We show some related works about the video generation tasks in Table <ref type=\"table\">1</ref>. </p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Model name</head><p>Year Backbone Task Group Imagen Video <ref target=\"#b28\" type=\"bibr\">[29]</ref> 2022 Diffusion Generation Google Pix2Seq-D <ref target=\"#b159\" type=\"bibr\">[160]</ref> 2022 Diffusion Segmentation Google Deepmind FDM <ref target=\"#b160\" type=\"bibr\">[161]</ref> 2022 Diffusion Prediction UBC MaskViT <ref target=\"#b161\" type=\"bibr\">[162]</ref> 2022 Masked Vision Models Prediction Stanford, Salesforce CogVideo <ref target=\"#b162\" type=\"bibr\">[163]</ref> 2022 Auto-regressive Generation THU Make-a-video <ref target=\"#b163\" type=\"bibr\">[164]</ref> 2022 Diffusion Generation Meta MagicVideo <ref target=\"#b164\" type=\"bibr\">[165]</ref> 2022 Diffusion Generation ByteDance TATS <ref target=\"#b165\" type=\"bibr\">[166]</ref> 2022 Auto-regressive Generation University of Maryland, Meta Phenaki <ref target=\"#b166\" type=\"bibr\">[167]</ref> 2022 Masked Vision Models Generation Google Brain Gen-1 <ref target=\"#b167\" type=\"bibr\">[168]</ref> 2023 Diffusion Generation, Editing RunwayML LFDM <ref target=\"#b139\" type=\"bibr\">[140]</ref> 2023 Diffusion Generation PSU, UCSD Text2video-Zero <ref target=\"#b168\" type=\"bibr\">[169]</ref> 2023 Diffusion Generation Picsart Video Fusion <ref target=\"#b169\" type=\"bibr\">[170]</ref> 2023 Diffusion Generation USAC, Alibaba PYoCo <ref target=\"#b33\" type=\"bibr\">[34]</ref> 2023 Diffusion Generation Nvidia Video LDM <ref target=\"#b35\" type=\"bibr\">[36]</ref> 2023 Diffusion Generation University of Maryland, Nvidia RIN <ref target=\"#b170\" type=\"bibr\">[171]</ref> 2023 Diffusion Generation Google Brain LVD <ref target=\"#b171\" type=\"bibr\">[172]</ref> 2023 Diffusion Generation UCB Dreamix <ref target=\"#b172\" type=\"bibr\">[173]</ref> 2023 Diffusion Editing Google MagicEdit <ref target=\"#b173\" type=\"bibr\">[174]</ref> 2023 Diffusion Editing ByteDance Control-A-Video <ref target=\"#b174\" type=\"bibr\">[175]</ref> 2023 Diffusion Editing Sun Yat-Sen University StableVideo <ref target=\"#b175\" type=\"bibr\">[176]</ref> 2023 Diffusion Editing ZJU, MSRA Tune-A-Video <ref target=\"#b77\" type=\"bibr\">[78]</ref> 2023 Diffusion Editing NUS Rerender-A-Video <ref target=\"#b176\" type=\"bibr\">[177]</ref> 2023 Diffusion Editing NTU Pix2Video <ref target=\"#b177\" type=\"bibr\">[178]</ref> 2023 Diffusion Editing Adobe, UCL InstructVid2Vid <ref target=\"#b178\" type=\"bibr\">[179]</ref> 2023 Diffusion Editing ZJU DiffAct <ref target=\"#b179\" type=\"bibr\">[180]</ref> 2023 Diffusion Action Detection University of Sydney DiffPose <ref target=\"#b180\" type=\"bibr\">[181]</ref> 2023 Diffusion Pose Estimation Jilin University MAGVIT <ref target=\"#b181\" type=\"bibr\">[182]</ref> 2023 Masked Vision Models Generation Google AnimateDiff <ref target=\"#b137\" type=\"bibr\">[138]</ref> 2023 Diffusion Generation CUHK MAGVIT V2 <ref target=\"#b46\" type=\"bibr\">[47]</ref> 2023 Masked Vision Models Generation Google Generative Dynamics <ref target=\"#b182\" type=\"bibr\">[183]</ref> 2023 Diffusion Generation Google VideoCrafter <ref target=\"#b80\" type=\"bibr\">[81]</ref> 2023 Diffusion Generation Tencent Zeroscope <ref target=\"#b183\" type=\"bibr\">[184]</ref> 2023 -Generation EasyWithAI ModelScope 2023 -Generation Damo Gen-2 <ref target=\"#b22\" type=\"bibr\">[23]</ref> 2023 -Generation RunwayML Pika <ref target=\"#b21\" type=\"bibr\">[22]</ref> 2023 -Generation Pika Labs Emu Video <ref target=\"#b184\" type=\"bibr\">[185]</ref> 2023 Diffusion Generation Meta PixelDance <ref target=\"#b185\" type=\"bibr\">[186]</ref> 2023 Diffusion Generation ByteDance Stable Video Diffusion <ref target=\"#b26\" type=\"bibr\">[27]</ref> 2023 Diffusion Generation Stability AI W.A.L.T <ref target=\"#b186\" type=\"bibr\">[187]</ref> 2023 Diffusion Generation Stanford, Google Fairy <ref target=\"#b187\" type=\"bibr\">[188]</ref> 2023 Diffusion Generation, Editing Meta VideoPoet <ref target=\"#b188\" type=\"bibr\">[189]</ref> 2023 Auto-regressive Generation, Editing Google LGVI <ref target=\"#b189\" type=\"bibr\">[190]</ref> 2024 Diffusion Editing PKU, NTU Lumiere <ref target=\"#b190\" type=\"bibr\">[191]</ref> 2024 Diffusion Generation Google Sora <ref target=\"#b2\" type=\"bibr\">[3]</ref> 2024 Diffusion Generation, Editing OpenAI</p></div> </div>\n",
       " <div type=\"references\">\n",
       " <listBibl>\n",
       " <biblStruct xml:id=\"b0\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Chatgpt: Get instant answers, find creative inspiration, learn something new</title>\n",
       " <author>\n",
       " <persName><surname>Openai</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"https://openai.com/chatgpt\"/>\n",
       " <imprint>\n",
       " <biblScope unit=\"page\">2022</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b1\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Gpt-4 technical report</title>\n",
       " <author>\n",
       " <persName><surname>Openai</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b2\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Sora: Creating video from text</title>\n",
       " <author>\n",
       " <persName><surname>Openai</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"https://openai.com/sora\"/>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2024\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b3\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Scalable diffusion models with transformers</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Peebles</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Xie</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>\n",
       " <meeting>the IEEE/CVF International Conference on Computer Vision</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " <biblScope from=\"4195\" to=\"4205\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b4\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Texture synthesis by non-parametric sampling</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><forename type=\"middle\">A</forename><surname>Efros</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><forename type=\"middle\">K</forename><surname>Leung</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the seventh IEEE international conference on computer vision</title>\n",
       " <meeting>the seventh IEEE international conference on computer vision</meeting>\n",
       " <imprint>\n",
       " <publisher>IEEE</publisher>\n",
       " <date type=\"published\" when=\"1999\"/>\n",
       " <biblScope unit=\"volume\">2</biblScope>\n",
       " <biblScope from=\"1033\" to=\"1038\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b5\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Survey of texture mapping</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><forename type=\"middle\">S</forename><surname>Heckbert</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">IEEE computer graphics and applications</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">6</biblScope>\n",
       " <biblScope unit=\"issue\">11</biblScope>\n",
       " <biblScope from=\"56\" to=\"67\" unit=\"page\"/>\n",
       " <date type=\"published\" when=\"1986\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b6\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><forename type=\"middle\">J</forename><surname>Goodfellow</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Pouget-Abadie</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Mirza</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Warde-Farley</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Ozair</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Courville</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Bengio</surname></persName>\n",
       " </author>\n",
       " <title level=\"m\">Generative adversarial networks</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b7\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Auto-encoding variational bayes</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><forename type=\"middle\">P</forename><surname>Kingma</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Welling</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1312.6114</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2013\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b8\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Nice: Non-linear independent components estimation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Dinh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Krueger</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Bengio</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1410.8516</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b9\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Generative modeling by estimating gradients of the data distribution</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Song</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Ermon</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in Neural Information Processing Systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " <biblScope unit=\"volume\">32</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b10\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Cao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Yan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Dai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><forename type=\"middle\">S</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Sun</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2303.04226</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b11\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Attention is all you need</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Vaswani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Shazeer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Parmar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Uszkoreit</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Jones</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><forename type=\"middle\">N</forename><surname>Gomez</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><forename type=\"middle\">U</forename><surname>Kaiser</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><surname>Polosukhin</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in Neural Information Processing Systems</title>\n",
       " <editor>\n",
       " <persName><forename type=\"first\">I</forename><surname>Guyon</surname></persName>\n",
       " <persName><forename type=\"first\">U</forename><forename type=\"middle\">V</forename><surname>Luxburg</surname></persName>\n",
       " <persName><forename type=\"first\">S</forename><surname>Bengio</surname></persName>\n",
       " <persName><forename type=\"first\">H</forename><surname>Wallach</surname></persName>\n",
       " <persName><forename type=\"first\">R</forename><surname>Fergus</surname></persName>\n",
       " <persName><forename type=\"first\">S</forename><surname>Vishwanathan</surname></persName>\n",
       " <persName><forename type=\"first\">R</forename><surname>Garnett</surname></persName>\n",
       " </editor>\n",
       " <imprint>\n",
       " <publisher>Curran Associates, Inc</publisher>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " <biblScope unit=\"volume\">30</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b12\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Bert: Pre-training of deep bidirectional transformers for language understanding</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Devlin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M.-W</forename><surname>Chang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Toutanova</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1810.04805</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b13\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Improving language understanding by generative pre-training</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Narasimhan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Salimans</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><surname>Sutskever</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b14\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">An image is worth 16x16 words: Transformers for image recognition at scale</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Dosovitskiy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Beyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Kolesnikov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Weissenborn</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Zhai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Unterthiner</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Dehghani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Minderer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Heigold</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Gelly</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2010.11929</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b15\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Swin transformer: Hierarchical vision transformer using shifted windows</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Lin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Cao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Hu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Wei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Lin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Guo</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF international conference on computer vision</title>\n",
       " <meeting>the IEEE/CVF international conference on computer vision</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021\"/>\n",
       " <biblScope from=\"10012\" to=\"10022\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b16\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">U-net: Convolutional networks for biomedical image segmentation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">O</forename><surname>Ronneberger</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Fischer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Brox</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference</title>\n",
       " <meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>\n",
       " <imprint>\n",
       " <publisher>Springer</publisher>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " <biblScope from=\"234\" to=\"241\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>Proceedings, Part III 18</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b17\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Learning transferable visual models from natural language supervision</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><forename type=\"middle\">W</forename><surname>Kim</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Hallacy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Ramesh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Goh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Agarwal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Sastry</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Askell</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Mishkin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Clark</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Krueger</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><surname>Sutskever</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b18\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">High-resolution image synthesis with latent diffusion models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Rombach</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Blattmann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Lorenz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Esser</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Ommer</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>\n",
       " <meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " <biblScope from=\"10684\" to=\"10695\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b19\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Midjourney: Text to image with ai art generator</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Ai</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"https://www.midjourneyai.ai/en\"/>\n",
       " <imprint>\n",
       " <biblScope unit=\"page\">2023</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b20\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Improving image generation with better captions</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Betker</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Goh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Jing</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Brooks</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Ouyang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Zhuang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Guo</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"https://cdn.openai.com/papers/dall-e-3.pdf\"/>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Computer Science</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">2</biblScope>\n",
       " <biblScope unit=\"page\">3</biblScope>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b21\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Pika is the idea-to-video platform that sets your creativity in motion</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Ai</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"https://pika.art/home\"/>\n",
       " <imprint>\n",
       " <biblScope unit=\"page\">2023</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b22\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Gen-2: Gen-2: The next step forward for generative ai</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Ai</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"https://research.runwayml.com/gen2\"/>\n",
       " <imprint>\n",
       " <biblScope unit=\"page\">2023</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b23\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Scaling vision transformers</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Zhai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Kolesnikov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Houlsby</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Beyer</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>\n",
       " <meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " <biblScope from=\"12104\" to=\"12113\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b24\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Scaling vision transformers to 22 billion parameters</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Dehghani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Djolonga</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Mustafa</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Padlewski</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Heek</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Gilmer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><forename type=\"middle\">P</forename><surname>Steiner</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Caron</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Geirhos</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><surname>Alabdulmohsin</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">International Conference on Machine Learning</title>\n",
       " <imprint>\n",
       " <publisher>PMLR</publisher>\n",
       " <biblScope unit=\"page\">2023</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b25\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Learning transferable visual models from natural language supervision</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><forename type=\"middle\">W</forename><surname>Kim</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Hallacy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Ramesh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Goh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Agarwal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Sastry</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Askell</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Mishkin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Clark</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">International conference on machine learning</title>\n",
       " <imprint>\n",
       " <publisher>PMLR</publisher>\n",
       " <biblScope unit=\"page\">2021</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b26\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Stable video diffusion: Scaling latent video diffusion models to large datasets</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Blattmann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Dockhorn</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Kulal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Mendelevitch</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Kilian</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Lorenz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Levi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>English</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><surname>Voleti</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Letts</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2311.15127</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b27\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Make-a-video: Text-to-video generation without text-video data</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">U</forename><surname>Singer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Polyak</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Hayes</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Yin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>An</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Q</forename><surname>Hu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">O</forename><surname>Ashual</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">O</forename><surname>Gafni</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Parikh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Gupta</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Taigman</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b28\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Imagen video: High definition video generation with diffusion models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Ho</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Chan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Saharia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Whang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Gritsenko</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><forename type=\"middle\">P</forename><surname>Kingma</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Poole</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Norouzi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><forename type=\"middle\">J</forename><surname>Fleet</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2210.02303</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b29\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">The bitter lesson</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Sutton</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"http://www.incompleteideas.net/IncIdeas/BitterLesson.html\"/>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019-03\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>Accessed: Your Access Date Here</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b30\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Take on sora technical report</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Xie</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"https://twitter.com/sainingxie/status/1758433676105310543\"/>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2024\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b31\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Neural discrete representation learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Van Den Oord</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">O</forename><surname>Vinyals</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in neural information processing systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " <biblScope unit=\"volume\">30</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b32\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Masked autoencoders are scalable vision learners</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Xie</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>DollÃ¡r</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Girshick</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>\n",
       " <meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " <biblScope from=\"16000\" to=\"16009\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b33\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Preserve your own correlation: A noise prior for video diffusion models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Ge</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Nah</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Poon</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Tao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Catanzaro</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Jacobs</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J.-B</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M.-Y</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Balaji</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>\n",
       " <meeting>the IEEE/CVF International Conference on Computer Vision</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " <biblScope from=\"22930\" to=\"22941\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b34\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Adversarial diffusion distillation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Sauer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Lorenz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Blattmann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Rombach</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2311.17042</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b35\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Align your latents: High-resolution video synthesis with latent diffusion models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Blattmann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Rombach</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Ling</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Dockhorn</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><forename type=\"middle\">W</forename><surname>Kim</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Fidler</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Kreis</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>\n",
       " <meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " <biblScope from=\"22563\" to=\"22575\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b36\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Tokenlearner: Adaptive space-time tokenization for videos</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Ryoo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Piergiovanni</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Arnab</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Dehghani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Angelova</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in Neural Information Processing Systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021\"/>\n",
       " <biblScope unit=\"volume\">34</biblScope>\n",
       " <biblScope from=\"12786\" to=\"12797\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b37\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Vivit: A video vision transformer</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Arnab</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Dehghani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Heigold</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Sun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>LuÄiÄ‡</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Schmid</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2103.15691</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b38\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Flexivit: One model for all patch sizes</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Beyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Izmailov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Kolesnikov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Caron</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Kornblith</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Zhai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Minderer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Tschannen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><surname>Alabdulmohsin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Pavetic</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>\n",
       " <meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " <biblScope from=\"14496\" to=\"14506\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b39\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Patch n'pack: Navit, a vision transformer for any aspect ratio and resolution</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Dehghani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Mustafa</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Djolonga</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Heek</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Minderer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Caron</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Steiner</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Puigcerver</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Geirhos</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><forename type=\"middle\">M</forename><surname>Alabdulmohsin</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Advances in Neural Information Processing Systems</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">36</biblScope>\n",
       " <date type=\"published\" when=\"2024\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b40\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Efficient sequence packing without crosscontamination: Accelerating large language models without impacting performance</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><forename type=\"middle\">M</forename><surname>Krell</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Kosec</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><forename type=\"middle\">P</forename><surname>Perez</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Fitzgibbon</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2107.02027</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b41\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A-vit: Adaptive tokens for efficient vision transformer</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Yin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Vahdat</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><forename type=\"middle\">M</forename><surname>Alvarez</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Mallya</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Kautz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Molchanov</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>\n",
       " <meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " <biblScope from=\"10809\" to=\"10818\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b42\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Token merging: Your vit but faster</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Bolya</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C.-Y</forename><surname>Fu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Dai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Feichtenhofer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Hoffman</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">The Eleventh International Conference on Learning Representations</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b43\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Adaptive token sampling for efficient vision transformers</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Fayyaz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><forename type=\"middle\">A</forename><surname>Koohpayegani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><forename type=\"middle\">R</forename><surname>Jafari</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Sengupta</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><forename type=\"middle\">R V</forename><surname>Joze</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">E</forename><surname>Sommerlade</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Pirsiavash</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Gall</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">European Conference on Computer Vision</title>\n",
       " <imprint>\n",
       " <publisher>Springer</publisher>\n",
       " <biblScope unit=\"page\">2022</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b44\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Attention is all you need</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Vaswani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Shazeer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Parmar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Uszkoreit</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Jones</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><forename type=\"middle\">N</forename><surname>Gomez</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Å</forename><surname>Kaiser</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><surname>Polosukhin</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in neural information processing systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " <biblScope unit=\"volume\">30</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b45\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Is space-time attention all you need for video understanding?</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Bertasius</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Torresani</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">\" in ICML</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021\"/>\n",
       " <biblScope unit=\"volume\">2</biblScope>\n",
       " <biblScope unit=\"page\">4</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b46\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Language model beats diffusion-tokenizer is key to visual generation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Lezama</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><forename type=\"middle\">B</forename><surname>Gundavarapu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Versari</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Sohn</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Minnen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Cheng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Gupta</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Gu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><forename type=\"middle\">G</forename><surname>Hauptmann</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2310.05737</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b47\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Fast transformer decoding: One write-head is all you need</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Shazeer</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b48\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Gqa: Training generalized multi-query transformer models from multi-head checkpoints</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Ainslie</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Lee-Thorp</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Jong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Zemlyanskiy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>LebrÃ³n</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Sanghai</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2305.13245</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b49\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Mamba: Linear-time sequence modeling with selective state spaces</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Gu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Dao</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2312.00752</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b50\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Deep unsupervised learning using nonequilibrium thermodynamics</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Sohl-Dickstein</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">E</forename><forename type=\"middle\">A</forename><surname>Weiss</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Maheswaranathan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Ganguli</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1503.03585</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b51\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Denoising diffusion probabilistic models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Ho</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Jain</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Abbeel</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in Neural Information Processing Systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " <biblScope unit=\"volume\">33</biblScope>\n",
       " <biblScope from=\"6840\" to=\"6851\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b52\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Score-based generative modeling through stochastic differential equations</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Song</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Sohl-Dickstein</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><forename type=\"middle\">P</forename><surname>Kingma</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Kumar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Ermon</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Poole</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2011.13456</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b53\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">All are worth words: A vit backbone for diffusion models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Bao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Nie</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Xue</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Cao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Su</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Zhu</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>\n",
       " <meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>\n",
       " <imprint>\n",
       " <biblScope unit=\"page\">2023</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b54\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Masked diffusion transformer is a strong image synthesizer</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M.-M</forename><surname>Cheng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Yan</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2303.14389</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b55\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Diffit: Diffusion vision transformers for image generation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Hatamizadeh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Song</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Kautz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Vahdat</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2312.02139</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b56\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Classifier-free diffusion guidance</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Ho</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Salimans</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2207.12598</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b57\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Progressive distillation for fast sampling of diffusion models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Salimans</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Ho</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2202.00512</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b58\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Cascaded diffusion models for high fidelity image generation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Ho</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Saharia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Chan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><forename type=\"middle\">J</forename><surname>Fleet</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Norouzi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Salimans</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">The Journal of Machine Learning Research</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">23</biblScope>\n",
       " <biblScope unit=\"issue\">1</biblScope>\n",
       " <biblScope from=\"2249\" to=\"2281\" unit=\"page\"/>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b59\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">High-resolution image synthesis with latent diffusion models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Rombach</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Blattmann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Lorenz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Esser</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Ommer</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b60\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Sdxl: Improving latent diffusion models for high-resolution image synthesis</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Podell</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>English</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Lacey</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Blattmann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Dockhorn</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>MÃ¼ller</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Penna</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Rombach</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2307.01952</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b61\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Language models are few-shot learners</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><forename type=\"middle\">B</forename><surname>Brown</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Mann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Ryder</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Subbiah</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Kaplan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Dhariwal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Neelakantan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Shyam</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Sastry</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Askell</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b62\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Conditional prompt learning for vision-language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><forename type=\"middle\">C</forename><surname>Loy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>\n",
       " <meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " <biblScope from=\"16816\" to=\"16825\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b63\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Multitask prompted training enables zero-shot task generalization</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><surname>Sanh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Webson</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Raffel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><forename type=\"middle\">H</forename><surname>Bach</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Sutawika</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Alyafeai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Chaffin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Stiegler</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><forename type=\"middle\">L</forename><surname>Scao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Raja</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2110.08207</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b64\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Finetuned language models are zero-shot learners</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Wei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Bosma</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><forename type=\"middle\">Y</forename><surname>Zhao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Guu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><forename type=\"middle\">W</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Lester</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Du</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><forename type=\"middle\">M</forename><surname>Dai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Q</forename><forename type=\"middle\">V</forename><surname>Le</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2109.01652</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b65\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Training language models to follow instructions with human feedback</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Ouyang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Jiang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Almeida</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Wainwright</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Mishkin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Agarwal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Slama</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Ray</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in Neural Information Processing Systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " <biblScope unit=\"volume\">35</biblScope>\n",
       " <biblScope from=\"27730\" to=\"27744\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b66\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Scaling up visual and vision-language representation learning with noisy text supervision</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Jia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Xia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y.-T</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Parekh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Pham</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Q</forename><surname>Le</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y.-H</forename><surname>Sung</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Duerig</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">International conference on machine learning</title>\n",
       " <imprint>\n",
       " <publisher>PMLR</publisher>\n",
       " <biblScope unit=\"page\">2021</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b67\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Coca: Contrastive captioners are image-text foundation models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><surname>Vasudevan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Yeung</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Seyedhosseini</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2205.01917</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b68\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Video-text modeling with zero-shot transfer from contrastive captioners</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Yan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Zhu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Cao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Ghosh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2212.04979</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b69\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">mplug-2: A modularized multi-modal foundation model across text, image and video</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Q</forename><surname>Ye</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Yan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Shi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Ye</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Bi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Q</forename><surname>Qian</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2302.00402</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b70\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Git: A generative image-to-text transformer for vision and language</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Hu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Lin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Gan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2205.14100</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b71\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Zero-shot video question answering via frozen bidirectional language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Miech</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Sivic</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><surname>Laptev</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Schmid</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in Neural Information Processing Systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " <biblScope unit=\"volume\">35</biblScope>\n",
       " <biblScope from=\"124\" to=\"141\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b72\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A practical survey on zero-shot prompt design for in-context learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the Conference Recent Advances in Natural Language Processing -Large Language Models for Natural Language Processings</title>\n",
       " <meeting>the Conference Recent Advances in Natural Language Processing -Large Language Models for Natural Language Processings<address><addrLine>Shoumen, BULGARIA</addrLine></address></meeting>\n",
       " <imprint>\n",
       " <biblScope unit=\"page\">2023</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b73\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Unleashing the potential of prompt engineering in large language models: a comprehensive review</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>LangrenÃ©</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Zhu</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2310.14735</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b74\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Boosted prompt ensembles for large language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Pitis</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><forename type=\"middle\">R</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Ba</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b75\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Optimizing prompts for text-to-image generation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Hao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Chi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Dong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Wei</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b76\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Vop: Text-video co-operative prompt tuning for cross-modal retrieval</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Gong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Pan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Jiang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Lv</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b77\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Tune-avideo: One-shot tuning of image diffusion models for text-to-video generation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><forename type=\"middle\">Z</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Ge</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Lei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Gu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Shi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Hsu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Shan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Qie</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><forename type=\"middle\">Z</forename><surname>Shou</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b78\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Image segmentation using text and image prompts</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>LÃ¼ddecke</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Ecker</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>\n",
       " <meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022-06\"/>\n",
       " <biblScope from=\"7086\" to=\"7096\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b79\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Seine: Short-to-long video diffusion model for generative transition and prediction</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Zhuang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Ma</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Lin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Qiao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b80\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Videocrafter2: Overcoming data limitations for high-quality video diffusion models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Cun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Xia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Weng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Shan</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2024\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b81\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Video-to-video synthesis</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T.-C</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M.-Y</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J.-Y</forename><surname>Zhu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Tao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Kautz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Catanzaro</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b82\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Few-shot video-to-video synthesis</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T.-C</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M.-Y</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Tao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Kautz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Catanzaro</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b83\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Moonshot: Towards controllable video generation and editing with multimodal conditions</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><forename type=\"middle\">J</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Le</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><forename type=\"middle\">Z</forename><surname>Shou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Xiong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Sahoo</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2024\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b84\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Fast-vid2vid: Spatial-temporal compression for videoto-video synthesis</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Zhuo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b85\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Yuan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Fu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Jiang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Hayashi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Neubig</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b86\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">The power of scale for parameter-efficient prompt tuning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Lester</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Al-Rfou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Constant</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>\n",
       " <meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021\"/>\n",
       " <biblScope from=\"3045\" to=\"3059\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b87\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Visual prompt tuning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Jia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Tang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B.-C</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Cardie</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Belongie</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Hariharan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S.-N</forename><surname>Lim</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">European Conference on Computer Vision</title>\n",
       " <imprint>\n",
       " <publisher>Springer</publisher>\n",
       " <biblScope unit=\"page\">2022</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b88\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Introducing chatgpt</title>\n",
       " <author>\n",
       " <persName><surname>Openai</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b89\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Gpt-4v(ision) system card</title>\n",
       " <author>\n",
       " <persName><surname>Openai</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b90\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Harnessing the power of chatgpt in fake news: An in-depth exploration in generation, detection and explanation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Sun</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b91\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Can llm-generated misinformation be detected?</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Shu</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b92\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Deid-gpt: Zero-shot medical text de-identification by gpt-4</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Cao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Dai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Zhao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Shu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Zeng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Sun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Shen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Q</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Zhu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b93\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Value fulcra: Mapping large language models to the multidimensional spectrum of basic human values</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Yao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Yi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Gong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Xie</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b94\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Trustgpt: A benchmark for trustworthy and responsible large language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Q</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><forename type=\"middle\">S</forename></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Sun</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b95\">\n",
       " <monogr>\n",
       " <title/>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Sun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Q</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Lyu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Kailkhura</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Xiong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Xiao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">E</forename><surname>Xing</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Ji</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Yao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Kellis</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Zitnik</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Jiang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Bansal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Zou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Pei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Han</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Zhao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Tang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Mitchell</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Shu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K.-W</forename><surname>Chang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Backes</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><forename type=\"middle\">Z</forename><surname>Gong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><forename type=\"middle\">S</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P.-Y</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Q</forename><surname>Gu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Ying</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Ji</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Jana</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Xie</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Ye</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Cao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Zhao</surname></persName>\n",
       " </author>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " <note>Trustllm: Trustworthiness in large language models,\" 2024</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b96\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Harmbench: A standardized evaluation framework for automated red teaming and robust refusal</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Mazeika</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Phan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Yin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Zou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Mu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">E</forename><surname>Sakhaee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Basart</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Forsyth</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Hendrycks</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2024\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b97\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Do-not-answer: A dataset for evaluating safeguards in llms</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Han</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Nakov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Baldwin</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b98\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Decodingtrust: A comprehensive assessment of trustworthiness in gpt models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Pei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Xie</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Kang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Xiong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Dutta</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Schaeffer</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2306.11698</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b99\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Safetybench: Evaluating the safety of large language models with multiple choice questions</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Lei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Sun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Long</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Lei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Tang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b100\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Shen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Backes</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Shen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2308.03825</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b101\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Autodan: Generating stealthy jailbreak prompts on aligned large language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Xiao</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2310.04451</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b102\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Autodan: Interpretable gradient-based adversarial attacks on large language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Zhu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>An</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Barrow</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Nenkova</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Sun</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b103\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Robust prompt optimization for defending language models against jailbreaking attacks</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2401.17263</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2024\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b104\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Cold-attack: Jailbreaking llms with stealthiness and controllability</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Guo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Qin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Hu</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2024\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b105\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Jailbroken: How does llm safety training fail?</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Wei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Haghtalab</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Steinhardt</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2307.02483</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b106\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Jailbreaking attack against multimodal large language model</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Niu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Ren</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Hua</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Jin</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2024\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b107\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">A survey on hallucination in large vision-language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Xue</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Zhao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Hou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Peng</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2024\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b108\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Hallusionbench: An advanced diagnostic suite for entangled language hallucination &amp; visual illusion in large vision-language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Guan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Xian</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Yacoob</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Manocha</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b109\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Evaluating object hallucination in large vision-language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Du</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><forename type=\"middle\">X</forename><surname>Zhao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J.-R</forename><surname>Wen</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b110\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Metatool benchmark for large language models: Deciding whether to use tools and which to use</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Shi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Fan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Q</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Wan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><forename type=\"middle\">Z</forename><surname>Gong</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2310.03128</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b111\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Mitigating hallucination in large multi-modal models via robust instruction tuning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Lin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Yacoob</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b112\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Mitigating fine-grained hallucination by fine-tuning large vision-language models with caption rewrites</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">E.-P</forename><surname>Lim</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">International Conference on Multimedia Modeling</title>\n",
       " <imprint>\n",
       " <publisher>Springer</publisher>\n",
       " <date type=\"published\" when=\"2024\"/>\n",
       " <biblScope from=\"32\" to=\"45\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b113\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Analyzing and mitigating object hallucination in large vision-language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Cui</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Yoon</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Deng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Finn</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Bansal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Yao</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2310.00754</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b114\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Bias and fairness in large language models: A survey</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><forename type=\"middle\">O</forename><surname>Gallegos</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><forename type=\"middle\">A</forename><surname>Rossi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Barrow</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><forename type=\"middle\">M</forename><surname>Tanjim</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Kim</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Dernoncourt</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><forename type=\"middle\">K</forename><surname>Ahmed</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2309.00770</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b115\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Bao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Feng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2305.07609</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b116\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Beyond detection: Unveiling fairness vulnerabilities in abusive language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Liang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Cheng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Payani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Shu</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b117\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Fair diffusion: Instructing text-to-image generation models on fairness</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Friedrich</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Schramowski</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Brack</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Struppek</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Hintersdorf</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Luccioni</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Kersting</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2302.10893</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b118\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Mitigating political bias in language models through reinforced calibration</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Jia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Wei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Vosoughi</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the AAAI Conference on Artificial Intelligence</title>\n",
       " <meeting>the AAAI Conference on Artificial Intelligence</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2021-05\"/>\n",
       " <biblScope unit=\"volume\">35</biblScope>\n",
       " <biblScope from=\"14857\" to=\"14866\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b119\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">End-to-end bias mitigation by modelling biases in corpora</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><forename type=\"middle\">K</forename><surname>Mahabadi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Belinkov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Henderson</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b120\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Can llms keep a secret? testing privacy implications of language models via contextual integrity theory</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Mireshghallah</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Kim</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Tsvetkov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Sap</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Shokri</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Choi</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2310.17884</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b121\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">You are what you write: Preserving privacy in the era of large language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Plant</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><surname>Giuffrida</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Gkatzia</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2204.09391</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b122\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Privacy in large language models: Attacks, defenses and future directions</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Luo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Kang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Q</forename><surname>Hu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Chan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Song</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b123\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Bommasani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><forename type=\"middle\">A</forename><surname>Hudson</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">E</forename><surname>Adeli</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Altman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Arora</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Arx</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><forename type=\"middle\">S</forename><surname>Bernstein</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Bohg</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Bosselut</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">E</forename><surname>Brunskill</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">E</forename><surname>Brynjolfsson</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Buch</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Card</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Castellon</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Chatterji</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Creel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><forename type=\"middle\">Q</forename><surname>Davis</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Demszky</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Donahue</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Doumbouya</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">E</forename><surname>Durmus</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Ermon</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Etchemendy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Ethayarajh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Fei-Fei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Finn</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Gale</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Gillespie</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Goel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Goodman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Grossman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Guha</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Hashimoto</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Henderson</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Hewitt</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><forename type=\"middle\">E</forename><surname>Ho</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Hong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Hsu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Icard</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Jain</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Jurafsky</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Kalluri</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Karamcheti</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Keeling</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Khani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">O</forename><surname>Khattab</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><forename type=\"middle\">W</forename><surname>Koh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Krass</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Krishna</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Kuditipudi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Kumar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Ladhak</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Leskovec</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><surname>Levent</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><forename type=\"middle\">L</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Ma</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Malik</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><forename type=\"middle\">D</forename><surname>Manning</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Mirchandani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">E</forename><surname>Mitchell</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Munyikwa</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Nair</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Narayan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Narayanan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Newman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Nie</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><forename type=\"middle\">C</forename><surname>Niebles</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Nilforoshan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Nyarko</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Ogut</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Orr</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><surname>Papadimitriou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><forename type=\"middle\">S</forename><surname>Park</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Piech</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">E</forename><surname>Portelance</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Potts</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Raghunathan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Reich</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Ren</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Rong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Roohani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Ruiz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Ryan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>RÃ©</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Sadigh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Sagawa</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Santhanam</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Shih</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Srinivasan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Tamkin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Taori</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><forename type=\"middle\">W</forename><surname>Thomas</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>TramÃ¨r</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><forename type=\"middle\">E</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><forename type=\"middle\">M</forename><surname>Xie</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Yasunaga</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>You</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Zaharia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Zheng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Liang</surname></persName>\n",
       " </author>\n",
       " <title level=\"m\">On the opportunities and risks of foundation models</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b124\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Large language model alignment: A survey</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Shen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Jin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Dong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Guo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Xiong</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2309.15025</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b125\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Alignbench: Benchmarking chinese alignment of large language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Lei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Feng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Wen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Cheng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Ke</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><forename type=\"middle\">L</forename><surname>Tam</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Sun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Dong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Tang</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b126\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Deep reinforcement learning from human preferences</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Christiano</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Leike</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><forename type=\"middle\">B</forename><surname>Brown</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Martic</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Legg</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Amodei</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b127\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Yao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Han</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Cui</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Hu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H.-T</forename><surname>Zheng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Sun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T.-S</forename><surname>Chua</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b128\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A systematic review of hate speech automatic detection using natural language processing</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><forename type=\"middle\">S</forename><surname>Jahan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Oussalah</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Neurocomputing</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"page\">126232</biblScope>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b129\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Sora safety</title>\n",
       " <author>\n",
       " <persName><surname>Openai</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"https://openai.com/sora#safety\"/>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2024\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b130\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Lawbench: Benchmarking legal knowledge of large language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Fei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Shen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Zhu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Han</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Shen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Ge</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2309.16289</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b131\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">I think, therefore i am: Benchmarking awareness of large language models using awarebench</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Lin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Wan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Sun</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2024\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b132\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Moviefactory: Automatic movie creation from text using large generative models for language and images</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Zhu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Tuo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W.-H</forename><surname>Cheng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Song</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Fu</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2306.07257</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b133\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Mobilevidfactory: Automatic diffusion-based social media video generation for mobile devices from text</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Zhu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Tuo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W.-H</forename><surname>Cheng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Song</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Fu</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 31st ACM International Conference on Multimedia</title>\n",
       " <meeting>the 31st ACM International Conference on Multimedia</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " <biblScope from=\"9371\" to=\"9373\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b134\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Vlogger: Make your dream a vlog</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Zhuang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Qiao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2401.09414</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2024\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b135\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Ccedit: Creative and controllable video editing via diffusion models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Feng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Weng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Yuan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Bao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Luo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Guo</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2309.16496</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b136\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Makeyour-video: Customized video generation using textual and structural guidance</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Xing</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Xia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Cun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2306.00943</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b137\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Animatediff: Animate your personalized text-to-image diffusion models without specific tuning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Guo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Rao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Qiao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Lin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Dai</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2307.04725</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b138\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Animate-a-story: Storytelling with retrieval-augmented video generation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Xia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Cun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Gong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Xing</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Weng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Shan</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2307.06940</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b139\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Conditional image-to-video generation with latent flow diffusion models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Ni</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Shi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><forename type=\"middle\">X</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><forename type=\"middle\">R</forename><surname>Min</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>\n",
       " <meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " <biblScope from=\"18444\" to=\"18455\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b140\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Animate anyone: Consistent and controllable image-to-video synthesis for character animation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Hu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Sun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Bo</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2311.17117</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b141\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Make it move: controllable image-to-video generation with text descriptions</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Hu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Luo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>\n",
       " <meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " <biblScope from=\"18219\" to=\"18228\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b142\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Vidm: Video implicit diffusion models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Mei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><surname>Patel</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the AAAI Conference on Artificial Intelligence</title>\n",
       " <meeting>the AAAI Conference on Artificial Intelligence</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " <biblScope unit=\"volume\">37</biblScope>\n",
       " <biblScope from=\"9117\" to=\"9125\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b143\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Video probabilistic diffusion models in projected latent space</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Sohn</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Kim</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Shin</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>\n",
       " <meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " <biblScope from=\"18456\" to=\"18466\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b144\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Physics-driven diffusion models for impact sound synthesis from videos</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Su</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Qian</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">E</forename><surname>Shlizerman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Torralba</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Gan</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>\n",
       " <meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " <biblScope from=\"9749\" to=\"9759\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b145\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Dance-to-music generation with encoder-based textual inversion of diffusion models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Dong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Tang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Ma</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">O</forename><surname>Deussen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T.-Y.</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2401.17800</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2024\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b146\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Video diffusion models for the apoptosis forcasting</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Awasthi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Nizam</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Zare</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Ahmad</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><forename type=\"middle\">J</forename><surname>Montalvo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Varadarajan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Roysam</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><forename type=\"middle\">V</forename><surname>Nguyen</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">bioRxiv</title>\n",
       " <imprint>\n",
       " <biblScope from=\"2023\" to=\"2034\" unit=\"page\"/>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b147\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Dermosegdiff: A boundaryaware segmentation diffusion model for skin lesion delineation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Bozorgpour</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Sadegheih</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Kazerouni</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Azad</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Merhof</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">International Workshop on PRedictive Intelligence In MEdicine</title>\n",
       " <imprint>\n",
       " <publisher>Springer</publisher>\n",
       " <biblScope unit=\"page\">2023</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b148\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Multimodal motion conditioned diffusion model for skeleton-based video anomaly detection</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Flaborea</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Collorone</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><forename type=\"middle\">M D</forename><surname>Di Melendugno</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>D'arrigo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Prenkaj</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Galasso</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>\n",
       " <meeting>the IEEE/CVF International Conference on Computer Vision</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " <biblScope from=\"10318\" to=\"10329\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b149\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Medsegdiff-v2: Diffusion based medical image segmentation with transformer</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Fu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Fang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2301.11798</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b150\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Diffusion transformer u-net for medical image segmentation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><forename type=\"middle\">J</forename><surname>Chowdary</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Yin</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>\n",
       " <imprint>\n",
       " <publisher>Springer</publisher>\n",
       " <biblScope unit=\"page\">2023</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b151\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Dall-e-bot: Introducing web-scale diffusion models to robotics</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><surname>Kapelyukh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><surname>Vosylius</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">E</forename><surname>Johns</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">IEEE Robotics and Automation Letters</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b152\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Structdiffusion: Object-centric diffusion for semantic rearrangement of novel objects</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Hermans</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Chernova</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Paxton</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Workshop on Language and Robotics at CoRL 2022</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b153\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Planning with diffusion for flexible behavior synthesis</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Janner</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Du</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><forename type=\"middle\">B</forename><surname>Tenenbaum</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Levine</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2205.09991</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b154\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Is conditional generative modeling all you need for decision-making?</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Ajay</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Du</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Gupta</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Tenenbaum</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Jaakkola</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2211.15657</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b155\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Motion planning diffusion: Learning and planning of robot motions with diffusion models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Carvalho</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><forename type=\"middle\">T</forename><surname>Le</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Baierl</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Koert</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Peters</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>\n",
       " <imprint>\n",
       " <publisher>IEEE</publisher>\n",
       " <biblScope unit=\"page\">2023</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b156\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Seer: Language instructed video prediction with latent diffusion models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Gu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Wen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Song</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2303.14897</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b157\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Genaug: Retargeting behaviors to unseen situations via generative augmentation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Kiami</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Gupta</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><surname>Kumar</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2302.06671</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b158\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Cacti: A framework for scalable multi-task multi-scene visual imitation learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Mandi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Bharadhwaj</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><surname>Moens</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Song</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Rajeswaran</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><surname>Kumar</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2212.05711</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b159\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A generalist framework for panoptic segmentation of images and videos</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Saxena</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Hinton</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><forename type=\"middle\">J</forename><surname>Fleet</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>\n",
       " <meeting>the IEEE/CVF International Conference on Computer Vision</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " <biblScope from=\"909\" to=\"919\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b160\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Flexible diffusion modeling of long videos</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Harvey</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Naderiparizi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><surname>Masrani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Weilbach</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Wood</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in Neural Information Processing Systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " <biblScope unit=\"volume\">35</biblScope>\n",
       " <biblScope from=\"27953\" to=\"27965\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b161\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Maskvit: Masked visual pre-training for video prediction</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Gupta</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Tian</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>MartÃ­n-MartÃ­n</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Fei-Fei</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2206.11894</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b162\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Cogvideo: Large-scale pretraining for text-tovideo generation via transformers</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Hong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Ding</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Zheng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Tang</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2205.15868</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b163\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Make-a-video: Text-to-video generation without text-video data</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">U</forename><surname>Singer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Polyak</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Hayes</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Yin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>An</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Q</forename><surname>Hu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">O</forename><surname>Ashual</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">O</forename><surname>Gafni</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2209.14792</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b164\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Magicvideo: Efficient video generation with latent diffusion models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Yan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Lv</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Zhu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Feng</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2211.11018</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b165\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Long video generation with time-agnostic vqgan and time-sensitive transformer</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Ge</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Hayes</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Yin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Pang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Jacobs</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J.-B</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Parikh</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">European Conference on Computer Vision</title>\n",
       " <imprint>\n",
       " <publisher>Springer</publisher>\n",
       " <biblScope unit=\"page\">2022</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b166\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Phenaki: Variable length video generation from open domain textual description</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Villegas</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Babaeizadeh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P.-J</forename><surname>Kindermans</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Moraldo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><forename type=\"middle\">T</forename><surname>Saffar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Castro</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Kunze</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Erhan</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2210.02399</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b167\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Structure and content-guided video synthesis with diffusion models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Esser</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Chiu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Atighehchian</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Granskog</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Germanidis</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>\n",
       " <meeting>the IEEE/CVF International Conference on Computer Vision</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " <biblScope from=\"7346\" to=\"7356\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b168\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Text2video-zero: Text-to-image diffusion models are zero-shot video generators</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Khachatryan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Movsisyan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><surname>Tadevosyan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Henschel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Navasardyan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Shi</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2303.13439</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b169\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Videofusion: Decomposed diffusion models for high-quality video generation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Luo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Shen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Zhao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Tan</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>\n",
       " <meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " <biblScope from=\"10209\" to=\"10218\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b170\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Scalable adaptive computation for iterative generation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Jabri</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Fleet</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2212.11972</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2022\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b171\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Llm-grounded video diffusion models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Lian</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Shi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Yala</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Darrell</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2309.17444</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b172\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Dreamix: Video diffusion models are general video editors</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">E</forename><surname>Molad</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">E</forename><surname>Horwitz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Valevski</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><forename type=\"middle\">R</forename><surname>Acha</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Matias</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Pritch</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Leviathan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Hoshen</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2302.01329</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b173\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><forename type=\"middle\">H</forename><surname>Liew</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Yan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Feng</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2308.14749</idno>\n",
       " <title level=\"m\">Magicedit: High-fidelity and temporally coherent video editing</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b174\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Control-a-video: Controllable text-to-video generation with diffusion models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Xie</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Xia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Xiao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Lin</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2305.13840</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b175\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Stablevideo: Text-driven consistency-aware diffusion video editing</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Chai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Guo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Lu</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>\n",
       " <meeting>the IEEE/CVF International Conference on Computer Vision</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " <biblScope from=\"23040\" to=\"23050\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b176\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Rerender a video: Zero-shot text-guided video-to-video translation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><forename type=\"middle\">C</forename><surname>Loy</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2306.07954</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b177\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Pix2video: Video editing using image diffusion</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Ceylan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C.-H</forename><forename type=\"middle\">P</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><forename type=\"middle\">J</forename><surname>Mitra</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>\n",
       " <meeting>the IEEE/CVF International Conference on Computer Vision</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " <biblScope from=\"23206\" to=\"23217\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b178\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Instructvid2vid: Controllable video editing with natural language instructions</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Qin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Tang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T.-S</forename><surname>Chua</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Zhuang</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2305.12328</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b179\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Diffusion action segmentation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Q</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A.-D</forename><surname>Dinh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Jiang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Shah</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>\n",
       " <meeting>the IEEE/CVF International Conference on Computer Vision</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " <biblScope from=\"10139\" to=\"10149\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b180\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Diffpose: Spatiotemporal diffusion model for video-based human pose estimation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Feng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><forename type=\"middle\">H E</forename><surname>Tse</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Ma</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><forename type=\"middle\">J</forename><surname>Chang</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>\n",
       " <meeting>the IEEE/CVF International Conference on Computer Vision</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " <biblScope from=\"14861\" to=\"14872\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b181\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Magvit: Masked generative video transformer</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Cheng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Sohn</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Lezama</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Chang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><forename type=\"middle\">G</forename><surname>Hauptmann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M.-H</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Hao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><surname>Essa</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>\n",
       " <meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " <biblScope from=\"10459\" to=\"10469\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b182\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Generative image dynamics</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Tucker</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">N</forename><surname>Snavely</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Holynski</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2309.07906</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b183\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Zeroscope -ai text-to-video model</title>\n",
       " <author>\n",
       " <persName><surname>Easywithai</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"https://easywithai.com/ai-video-generators/zeroscope/,2023\"/>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b184\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Emu video: Factorizing text-to-video generation by explicit image conditioning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Girdhar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Singh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Brown</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Q</forename><surname>Duval</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Azadi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><forename type=\"middle\">S</forename><surname>Rambhatla</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Shah</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Yin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Parikh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><surname>Misra</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2311.10709</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b185\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Make pixels dance: High-dynamic video generation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Zeng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Wei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Zheng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Zou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Wei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2311.10982</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b186\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Photorealistic video generation with diffusion models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Gupta</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Sohn</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Gu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Hahn</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Fei-Fei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><surname>Essa</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Jiang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Lezama</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2312.06662</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b187\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Fairy: Fast parallelized instruction-guided video-to-video synthesis</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C.-Y</forename><surname>Chuang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Jia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Krishnakumar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Xiao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Liang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Vajda</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2312.13834</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b188\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Videopoet: A large language model for zero-shot video generation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Kondratyuk</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Gu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Lezama</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Hornung</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Adam</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Akbari</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Alon</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><surname>Birodkar</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2312.14125</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b189\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Towards language-driven video inpainting via multimodal large language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Si</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Tong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2401.10226</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2024\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b190\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Lumiere: A space-time diffusion model for video generation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">O</forename><surname>Bar-Tal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Chefer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">O</forename><surname>Tov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Herrmann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Paiss</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Zada</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Ephrat</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Hur</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Michaeli</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2401.12945</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2024\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " </listBibl>\n",
       " </div>\n",
       " </back>\n",
       " </text>\n",
       " </TEI>,\n",
       " <?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       " <TEI xml:space=\"preserve\" xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd\">\n",
       " <teiHeader xml:lang=\"en\">\n",
       " <fileDesc>\n",
       " <titleStmt>\n",
       " <title level=\"a\" type=\"main\">Attention Is All You Need</title>\n",
       " </titleStmt>\n",
       " <publicationStmt>\n",
       " <publisher/>\n",
       " <availability status=\"unknown\"><licence/></availability>\n",
       " <date type=\"published\" when=\"2023-08-02\">2 Aug 2023</date>\n",
       " </publicationStmt>\n",
       " <sourceDesc>\n",
       " <biblStruct>\n",
       " <analytic>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ashish</forename><surname>Vaswani</surname></persName>\n",
       " <email>avaswani@google.com</email>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noam</forename><surname>Shazeer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Google</forename><surname>Brain</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Niki</forename><surname>Parmar</surname></persName>\n",
       " <email>nikip@google.com</email>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jakob</forename><surname>Uszkoreit</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Llion</forename><surname>Jones</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Aidan</forename><forename type=\"middle\">N</forename><surname>Gomez</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Åukasz</forename><surname>Kaiser</surname></persName>\n",
       " <email>lukaszkaiser@google.com</email>\n",
       " </author>\n",
       " <author>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"department\">Google Brain</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <affiliation key=\"aff1\">\n",
       " <orgName type=\"department\">Google Research</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <affiliation key=\"aff2\">\n",
       " <orgName type=\"department\">Google Research</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <affiliation key=\"aff3\">\n",
       " <orgName type=\"department\">Google Research</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <affiliation key=\"aff4\">\n",
       " <orgName type=\"institution\">University of Toronto</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <affiliation key=\"aff5\">\n",
       " <orgName type=\"department\">Google Brain</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <affiliation key=\"aff6\">\n",
       " <orgName type=\"laboratory\">31st Conference on Neural Information Processing Systems (NIPS 2017)</orgName>\n",
       " <address>\n",
       " <settlement>Long Beach</settlement>\n",
       " <region>CA</region>\n",
       " <country key=\"US\">USA</country>\n",
       " </address>\n",
       " </affiliation>\n",
       " </author>\n",
       " <title level=\"a\" type=\"main\">Attention Is All You Need</title>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2023-08-02\">2 Aug 2023</date>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <idno type=\"arXiv\">arXiv:1706.03762v7[cs.CL]</idno>\n",
       " </biblStruct>\n",
       " </sourceDesc>\n",
       " </fileDesc>\n",
       " <encodingDesc>\n",
       " <appInfo>\n",
       " <application ident=\"GROBID\" version=\"0.7.0-SNAPSHOT\" when=\"2024-04-27T16:15+0000\">\n",
       " <desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>\n",
       " <ref target=\"https://github.com/kermitt2/grobid\"/>\n",
       " </application>\n",
       " </appInfo>\n",
       " </encodingDesc>\n",
       " <profileDesc>\n",
       " <abstract>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.</p><p>â€  Work performed while at Google Brain. â€¡ Work performed while at Google Research.</p></div>\n",
       " </abstract>\n",
       " </profileDesc>\n",
       " </teiHeader>\n",
       " <text xml:lang=\"en\">\n",
       " <body>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>Recurrent neural networks, long short-term memory <ref target=\"#b12\" type=\"bibr\">[13]</ref> and gated recurrent <ref target=\"#b6\" type=\"bibr\">[7]</ref> neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation <ref target=\"#b34\" type=\"bibr\">[35,</ref><ref target=\"#b1\" type=\"bibr\">2,</ref><ref target=\"#b4\" type=\"bibr\">5]</ref>. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures <ref target=\"#b37\" type=\"bibr\">[38,</ref><ref target=\"#b23\" type=\"bibr\">24,</ref><ref target=\"#b14\" type=\"bibr\">15]</ref>.</p><p>Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h t , as a function of the previous hidden state h tâˆ’1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks <ref target=\"#b20\" type=\"bibr\">[21]</ref> and conditional computation <ref target=\"#b31\" type=\"bibr\">[32]</ref>, while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.</p><p>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences <ref target=\"#b1\" type=\"bibr\">[2,</ref><ref target=\"#b18\" type=\"bibr\">19]</ref>. In all but a few cases <ref target=\"#b26\" type=\"bibr\">[27]</ref>, however, such attention mechanisms are used in conjunction with a recurrent network.</p><p>In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">Background</head><p>The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU <ref target=\"#b15\" type=\"bibr\">[16]</ref>, ByteNet <ref target=\"#b17\" type=\"bibr\">[18]</ref> and ConvS2S <ref target=\"#b8\" type=\"bibr\">[9]</ref>, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions <ref target=\"#b11\" type=\"bibr\">[12]</ref>. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.</p><p>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations <ref target=\"#b3\" type=\"bibr\">[4,</ref><ref target=\"#b26\" type=\"bibr\">27,</ref><ref target=\"#b27\" type=\"bibr\">28,</ref><ref target=\"#b21\" type=\"bibr\">22]</ref>. End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks <ref target=\"#b33\" type=\"bibr\">[34]</ref>.</p><p>To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as <ref target=\"#b16\" type=\"bibr\">[17,</ref><ref target=\"#b17\" type=\"bibr\">18]</ref> and <ref target=\"#b8\" type=\"bibr\">[9]</ref>.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3\">Model Architecture</head><p>Most competitive neural sequence transduction models have an encoder-decoder structure <ref target=\"#b4\" type=\"bibr\">[5,</ref><ref target=\"#b1\" type=\"bibr\">2,</ref><ref target=\"#b34\" type=\"bibr\">35]</ref>. Here, the encoder maps an input sequence of symbol representations (x 1 , ..., x n ) to a sequence of continuous representations z = (z 1 , ..., z n ). Given z, the decoder then generates an output sequence (y 1 , ..., y m ) of symbols one element at a time. At each step the model is auto-regressive <ref target=\"#b9\" type=\"bibr\">[10]</ref>, consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure <ref target=\"#fig_0\" type=\"figure\">1</ref>, respectively.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1\">Encoder and Decoder Stacks</head><p>Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection <ref target=\"#b10\" type=\"bibr\">[11]</ref> around each of the two sub-layers, followed by layer normalization <ref target=\"#b0\" type=\"bibr\">[1]</ref>. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d model = 512.</p><p>Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Attention</head><p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum Scaled Dot-Product Attention Multi-Head Attention of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2.1\">Scaled Dot-Product Attention</head><p>We call our particular attention \"Scaled Dot-Product Attention\" (Figure <ref target=\"#fig_1\" type=\"figure\">2</ref>). The input consists of queries and keys of dimension d k , and values of dimension d v . We compute the dot products of the query with all keys, divide each by âˆš d k , and apply a softmax function to obtain the weights on the values.</p><p>In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:</p><formula xml:id=\"formula_0\">Attention(Q, K, V ) = softmax( QK T âˆš d k )V<label>(1)</label></formula><p>The two most commonly used attention functions are additive attention <ref target=\"#b1\" type=\"bibr\">[2]</ref>, and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1</p><formula xml:id=\"formula_1\">âˆš d k .</formula><p>Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.</p><p>While for small values of d k the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d k <ref target=\"#b2\" type=\"bibr\">[3]</ref>. We suspect that for large values of d k , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4 . To counteract this effect, we scale the dot products by 1</p><formula xml:id=\"formula_2\">âˆš d k .</formula></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2.2\">Multi-Head Attention</head><p>Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k , d k and d v dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d v -dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure <ref target=\"#fig_1\" type=\"figure\">2</ref>.</p><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p><formula xml:id=\"formula_3\">MultiHead(Q, K, V ) = Concat(head 1 , ..., head h )W O</formula><p>where</p><formula xml:id=\"formula_4\">head i = Attention(QW Q i , KW K i , V W V i )</formula><p>Where the projections are parameter matrices</p><formula xml:id=\"formula_5\">W Q i âˆˆ R dmodelÃ—d k , W K i âˆˆ R dmodelÃ—d k , W V i âˆˆ R dmodelÃ—dv and W O âˆˆ R hdvÃ—dmodel .</formula><p>In this work we employ h = 8 parallel attention layers, or heads. For each of these we use</p><formula xml:id=\"formula_6\">d k = d v = d model /h = 64.</formula><p>Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2.3\">Applications of Attention in our Model</head><p>The Transformer uses multi-head attention in three different ways:</p><p>â€¢ In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as <ref target=\"#b37\" type=\"bibr\">[38,</ref><ref target=\"#b1\" type=\"bibr\">2,</ref><ref target=\"#b8\" type=\"bibr\">9]</ref>.</p><p>â€¢ The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.</p><p>â€¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to âˆ’âˆž) all values in the input of the softmax which correspond to illegal connections. See Figure <ref target=\"#fig_1\" type=\"figure\">2</ref>.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.3\">Position-wise Feed-Forward Networks</head><p>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.</p><formula xml:id=\"formula_7\">FFN(x) = max(0, xW 1 + b 1 )W 2 + b 2 (2)</formula><p>While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1.</p><p>The dimensionality of input and output is d model = 512, and the inner-layer has dimensionality d f f = 2048.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.4\">Embeddings and Softmax</head><p>Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model . We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to <ref target=\"#b29\" type=\"bibr\">[30]</ref>. In the embedding layers, we multiply those weights by âˆš d model .  <ref target=\"#formula_0\" type=\"formula\">1</ref>) O(n/r)</p><formula xml:id=\"formula_8\">O(n 2 â€¢ d) O(1) O(1) Recurrent O(n â€¢ d 2 ) O(n) O(n) Convolutional O(k â€¢ n â€¢ d 2 ) O(1) O(log k (n)) Self-Attention (restricted) O(r â€¢ n â€¢ d) O(</formula></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.5\">Positional Encoding</head><p>Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d model as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed <ref target=\"#b8\" type=\"bibr\">[9]</ref>.</p><p>In this work, we use sine and cosine functions of different frequencies:</p><formula xml:id=\"formula_9\">P E (pos,2i) = sin(pos/10000 2i/dmodel ) P E (pos,2i+1) = cos(pos/10000 2i/dmodel )</formula><p>where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2Ï€ to 10000 â€¢ 2Ï€. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, P E pos+k can be represented as a linear function of P E pos .</p><p>We also experimented with using learned positional embeddings <ref target=\"#b8\" type=\"bibr\">[9]</ref> instead, and found that the two versions produced nearly identical results (see Table <ref target=\"#tab_3\" type=\"table\">3</ref> row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4\">Why Self-Attention</head><p>In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1 , ..., x n ) to another sequence of equal length (z 1 , ..., z n ), with x i , z i âˆˆ R d , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.</p><p>One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.</p><p>The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies <ref target=\"#b11\" type=\"bibr\">[12]</ref>. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.</p><p>As noted in Table <ref target=\"#tab_0\" type=\"table\">1</ref>, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece <ref target=\"#b37\" type=\"bibr\">[38]</ref> and byte-pair <ref target=\"#b30\" type=\"bibr\">[31]</ref> representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.</p><p>A single convolutional layer with kernel width k &lt; n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(log k (n)) in the case of dilated convolutions <ref target=\"#b17\" type=\"bibr\">[18]</ref>, increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions <ref target=\"#b5\" type=\"bibr\">[6]</ref>, however, decrease the complexity considerably, to O(k</p><formula xml:id=\"formula_10\">â€¢ n â€¢ d + n â€¢ d 2 )</formula><p>. Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.</p><p>As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5\">Training</head><p>This section describes the training regime for our models.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">Training Data and Batching</head><p>We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding <ref target=\"#b2\" type=\"bibr\">[3]</ref>, which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary <ref target=\"#b37\" type=\"bibr\">[38]</ref>. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.2\">Hardware and Schedule</head><p>We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table <ref target=\"#tab_3\" type=\"table\">3</ref>), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.3\">Optimizer</head><p>We used the Adam optimizer <ref target=\"#b19\" type=\"bibr\">[20]</ref> with Î² 1 = 0.9, Î² 2 = 0.98 and Ïµ = 10 âˆ’9 . We varied the learning rate over the course of training, according to the formula:</p><formula xml:id=\"formula_11\">lrate = d âˆ’0.5 model â€¢ min(step_num âˆ’0.5 , step_num â€¢ warmup_steps âˆ’1.5 )<label>(3)</label></formula><p>This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.4\">Regularization</head><p>We employ three types of regularization during training: Residual Dropout We apply dropout <ref target=\"#b32\" type=\"bibr\">[33]</ref> to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P drop = 0.1.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Label Smoothing</head><p>During training, we employed label smoothing of value Ïµ ls = 0.1 <ref target=\"#b35\" type=\"bibr\">[36]</ref>. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.</p><p>6 Results</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6.1\">Machine Translation</head><p>On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table <ref target=\"#tab_1\" type=\"table\">2</ref>) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table <ref target=\"#tab_3\" type=\"table\">3</ref>. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.</p><p>On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate P drop = 0.1, instead of 0.3.</p><p>For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty Î± = 0.6 <ref target=\"#b37\" type=\"bibr\">[38]</ref>. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible <ref target=\"#b37\" type=\"bibr\">[38]</ref>. </p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6.2\">Model Variations</head><p>To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the . We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table <ref target=\"#tab_3\" type=\"table\">3</ref>.</p><p>In Table <ref target=\"#tab_3\" type=\"table\">3</ref> rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.</p><p>In Table <ref target=\"#tab_3\" type=\"table\">3</ref> rows (B), we observe that reducing the attention key size d k hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings <ref target=\"#b8\" type=\"bibr\">[9]</ref>, and observe nearly identical results to the base model.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6.3\">English Constituency Parsing</head><p>To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes <ref target=\"#b36\" type=\"bibr\">[37]</ref>.</p><p>We trained a 4-layer transformer with d model = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank <ref target=\"#b24\" type=\"bibr\">[25]</ref>, about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences <ref target=\"#b36\" type=\"bibr\">[37]</ref>. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.</p><p>We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we increased the maximum output length to input length + 300. We used a beam size of 21 and Î± = 0.3 for both WSJ only and the semi-supervised setting.</p><p>Our results in Table <ref target=\"#tab_4\" type=\"table\">4</ref> show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar <ref target=\"#b7\" type=\"bibr\">[8]</ref>.</p><p>In contrast to RNN sequence-to-sequence models <ref target=\"#b36\" type=\"bibr\">[37]</ref>, the Transformer outperforms the Berkeley-Parser <ref target=\"#b28\" type=\"bibr\">[29]</ref> even when training only on the WSJ training set of 40K sentences.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"7\">Conclusion</head><p>In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.</p><p>For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.</p><p>We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.</p><p>The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Attention Visualizations</head><p>Input-Input Layer5  </p></div><figure xml:id=\"fig_0\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The Transformer -model architecture.</figDesc></figure>\n",
       " <figure xml:id=\"fig_1\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.</figDesc></figure>\n",
       " <figure xml:id=\"fig_2\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult'. Attentions here shown only for the word 'making'. Different colors represent different heads. Best viewed in color.</figDesc></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_0\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 1 :</head><label>1</label><figDesc>Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.</figDesc><table><row><cell>Layer Type</cell><cell>Complexity per Layer Sequential Maximum Path Length</cell></row><row><cell/><cell>Operations</cell></row><row><cell>Self-Attention</cell><cell/></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_1\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 2 :</head><label>2</label><figDesc>The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.</figDesc><table><row><cell>Model</cell><cell cols=\"2\">BLEU EN-DE EN-FR</cell><cell>Training Cost (FLOPs) EN-DE EN-FR</cell></row><row><cell>ByteNet [18]</cell><cell>23.75</cell><cell/><cell/></row><row><cell>Deep-Att + PosUnk [39]</cell><cell/><cell>39.2</cell><cell>1.0 â€¢ 10 20</cell></row><row><cell>GNMT + RL [38]</cell><cell>24.6</cell><cell>39.92</cell><cell>2.3 â€¢ 10 19 1.4 â€¢ 10 20</cell></row><row><cell>ConvS2S [9]</cell><cell>25.16</cell><cell>40.46</cell><cell>9.6 â€¢ 10 18 1.5 â€¢ 10 20</cell></row><row><cell>MoE [32]</cell><cell>26.03</cell><cell>40.56</cell><cell>2.0 â€¢ 10 19 1.2 â€¢ 10 20</cell></row><row><cell>Deep-Att + PosUnk Ensemble [39]</cell><cell/><cell>40.4</cell><cell>8.0 â€¢ 10 20</cell></row><row><cell>GNMT + RL Ensemble [38]</cell><cell>26.30</cell><cell>41.16</cell><cell>1.8 â€¢ 10 20 1.1 â€¢ 10 21</cell></row><row><cell>ConvS2S Ensemble [9]</cell><cell>26.36</cell><cell>41.29</cell><cell>7.7 â€¢ 10 19 1.2 â€¢ 10 21</cell></row><row><cell>Transformer (base model)</cell><cell>27.3</cell><cell>38.1</cell><cell>3.3 â€¢ 10 18</cell></row><row><cell>Transformer (big)</cell><cell>28.4</cell><cell>41.8</cell><cell>2.3 â€¢ 10 19</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_2\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 2</head><label>2</label><figDesc>summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5 .</figDesc><table/></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_3\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 3 :</head><label>3</label><figDesc>Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.</figDesc><table><row><cell/><cell cols=\"2\">N d model</cell><cell>d ff</cell><cell>h</cell><cell>d k</cell><cell>d v</cell><cell cols=\"2\">P drop Ïµ ls</cell><cell>train steps (dev) (dev) PPL BLEU params Ã—10 6</cell></row><row><cell cols=\"2\">base 6</cell><cell>512</cell><cell cols=\"2\">2048 8</cell><cell>64</cell><cell>64</cell><cell>0.1</cell><cell cols=\"2\">0.1 100K 4.92</cell><cell>25.8</cell><cell>65</cell></row><row><cell/><cell/><cell/><cell/><cell cols=\"3\">1 512 512</cell><cell/><cell/><cell>5.29</cell><cell>24.9</cell></row><row><cell>(A)</cell><cell/><cell/><cell/><cell cols=\"3\">4 128 128 16 32 32</cell><cell/><cell/><cell>5.00 4.91</cell><cell>25.5 25.8</cell></row><row><cell/><cell/><cell/><cell/><cell cols=\"2\">32 16</cell><cell>16</cell><cell/><cell/><cell>5.01</cell><cell>25.4</cell></row><row><cell>(B)</cell><cell/><cell/><cell/><cell/><cell>16 32</cell><cell/><cell/><cell/><cell>5.16 5.01</cell><cell>25.1 25.4</cell><cell>58 60</cell></row><row><cell/><cell>2</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell>6.11</cell><cell>23.7</cell><cell>36</cell></row><row><cell/><cell>4</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell>5.19</cell><cell>25.3</cell><cell>50</cell></row><row><cell/><cell>8</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell>4.88</cell><cell>25.5</cell><cell>80</cell></row><row><cell>(C)</cell><cell/><cell>256</cell><cell/><cell/><cell>32</cell><cell>32</cell><cell/><cell/><cell>5.75</cell><cell>24.5</cell><cell>28</cell></row><row><cell/><cell/><cell>1024</cell><cell/><cell/><cell cols=\"2\">128 128</cell><cell/><cell/><cell>4.66</cell><cell>26.0</cell><cell>168</cell></row><row><cell/><cell/><cell/><cell>1024</cell><cell/><cell/><cell/><cell/><cell/><cell>5.12</cell><cell>25.4</cell><cell>53</cell></row><row><cell/><cell/><cell/><cell>4096</cell><cell/><cell/><cell/><cell/><cell/><cell>4.75</cell><cell>26.2</cell><cell>90</cell></row><row><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell>0.0</cell><cell/><cell>5.77</cell><cell>24.6</cell></row><row><cell>(D)</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell>0.2</cell><cell>0.0</cell><cell>4.95 4.67</cell><cell>25.5 25.3</cell></row><row><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell>0.2</cell><cell>5.47</cell><cell>25.7</cell></row><row><cell>(E)</cell><cell/><cell cols=\"7\">positional embedding instead of sinusoids</cell><cell>4.92</cell><cell>25.7</cell></row><row><cell>big</cell><cell>6</cell><cell cols=\"3\">1024 4096 16</cell><cell/><cell/><cell>0.3</cell><cell/><cell>300K 4.33</cell><cell>26.4</cell><cell>213</cell></row><row><cell cols=\"4\">development set, newstest2013</cell><cell/><cell/><cell/><cell/><cell/></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_4\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 4 :</head><label>4</label><figDesc>The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)</figDesc><table><row><cell>Parser</cell><cell>Training</cell><cell>WSJ 23 F1</cell></row><row><cell cols=\"2\">Vinyals &amp; Kaiser el al. (2014) [37] WSJ only, discriminative</cell><cell>88.3</cell></row><row><cell>Petrov et al. (2006) [29]</cell><cell>WSJ only, discriminative</cell><cell>90.4</cell></row><row><cell>Zhu et al. (2013) [40]</cell><cell>WSJ only, discriminative</cell><cell>90.4</cell></row><row><cell>Dyer et al. (2016) [8]</cell><cell>WSJ only, discriminative</cell><cell>91.7</cell></row><row><cell>Transformer (4 layers)</cell><cell>WSJ only, discriminative</cell><cell>91.3</cell></row><row><cell>Zhu et al. (2013) [40]</cell><cell>semi-supervised</cell><cell>91.3</cell></row><row><cell>Huang &amp; Harper (2009) [14]</cell><cell>semi-supervised</cell><cell>91.3</cell></row><row><cell>McClosky et al. (2006) [26]</cell><cell>semi-supervised</cell><cell>92.1</cell></row><row><cell>Vinyals &amp; Kaiser el al. (2014) [37]</cell><cell>semi-supervised</cell><cell>92.1</cell></row><row><cell>Transformer (4 layers)</cell><cell>semi-supervised</cell><cell>92.7</cell></row><row><cell>Luong et al. (2015) [23]</cell><cell>multi-task</cell><cell>93.0</cell></row><row><cell>Dyer et al. (2016) [8]</cell><cell>generative</cell><cell>93.3</cell></row></table></figure>\n",
       " <note n=\"4\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q â€¢ k = d k i=1 qiki, has mean 0 and variance d k .</note>\n",
       " <note n=\"5\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.</note>\n",
       " </body>\n",
       " <back>\n",
       " <div type=\"acknowledgement\">\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.</p></div>\n",
       " </div>\n",
       " <div type=\"annex\">\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Input-Input Layer5</head><p>The  </p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Input-Input Layer5</head><p>The  </p></div> </div>\n",
       " <div type=\"references\">\n",
       " <listBibl>\n",
       " <biblStruct xml:id=\"b0\">\n",
       " <monogr>\n",
       " <title/>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jimmy</forename><forename type=\"middle\">Lei</forename><surname>Ba</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jamie</forename><forename type=\"middle\">Ryan</forename><surname>Kiros</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Geoffrey</forename><forename type=\"middle\">E</forename><surname>Hinton</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1607.06450</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">Layer normalization. arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b1\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Neural machine translation by jointly learning to align and translate</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dzmitry</forename><surname>Bahdanau</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kyunghyun</forename><surname>Cho</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yoshua</forename><surname>Bengio</surname></persName>\n",
       " </author>\n",
       " <idno>abs/1409.0473</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " <publisher>CoRR</publisher>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b2\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Massive exploration of neural machine translation architectures</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Denny</forename><surname>Britz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Anna</forename><surname>Goldie</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Minh-Thang</forename><surname>Luong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Quoc</forename><forename type=\"middle\">V</forename><surname>Le</surname></persName>\n",
       " </author>\n",
       " <idno>abs/1703.03906</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b3\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Long short-term memory-networks for machine reading</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jianpeng</forename><surname>Cheng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Li</forename><surname>Dong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mirella</forename><surname>Lapata</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1601.06733</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b4\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kyunghyun</forename><surname>Cho</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bart</forename><surname>Van Merrienboer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Caglar</forename><surname>Gulcehre</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Fethi</forename><surname>Bougares</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Holger</forename><surname>Schwenk</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yoshua</forename><surname>Bengio</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"CoRR,abs/1406.1078\"/>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b5\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Francois</forename><surname>Chollet</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1610.02357</idno>\n",
       " <title level=\"m\">Xception: Deep learning with depthwise separable convolutions</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b6\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Junyoung</forename><surname>Chung</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ã‡aglar</forename><surname>GÃ¼lÃ§ehre</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kyunghyun</forename><surname>Cho</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yoshua</forename><surname>Bengio</surname></persName>\n",
       " </author>\n",
       " <idno>abs/1412.3555</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b7\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Recurrent neural network grammars</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chris</forename><surname>Dyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Adhiguna</forename><surname>Kuncoro</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Miguel</forename><surname>Ballesteros</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noah</forename><forename type=\"middle\">A</forename><surname>Smith</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proc. of NAACL</title>\n",
       " <meeting>of NAACL</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b8\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Convolutional sequence to sequence learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jonas</forename><surname>Gehring</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Michael</forename><surname>Auli</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">David</forename><surname>Grangier</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Denis</forename><surname>Yarats</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yann</forename><forename type=\"middle\">N</forename><surname>Dauphin</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1705.03122v2</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b9\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Graves</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1308.0850</idno>\n",
       " <title level=\"m\">Generating sequences with recurrent neural networks</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2013\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b10\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Deep residual learning for image recognition</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kaiming</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xiangyu</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Shaoqing</forename><surname>Ren</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jian</forename><surname>Sun</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>\n",
       " <meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " <biblScope from=\"770\" to=\"778\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b11\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sepp</forename><surname>Hochreiter</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yoshua</forename><surname>Bengio</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Paolo</forename><surname>Frasconi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2001\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b12\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Long short-term memory</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sepp</forename><surname>Hochreiter</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Neural computation</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">9</biblScope>\n",
       " <biblScope unit=\"issue\">8</biblScope>\n",
       " <biblScope from=\"1735\" to=\"1780\" unit=\"page\"/>\n",
       " <date type=\"published\" when=\"1997\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b13\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Self-training PCFG grammars with latent annotations across languages</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhongqiang</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mary</forename><surname>Harper</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>\n",
       " <meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2009-08\"/>\n",
       " <biblScope from=\"832\" to=\"841\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b14\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Exploring the limits of language modeling</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rafal</forename><surname>Jozefowicz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oriol</forename><surname>Vinyals</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mike</forename><surname>Schuster</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noam</forename><surname>Shazeer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yonghui</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1602.02410</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b15\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Can active memory replace attention?</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Åukasz</forename><surname>Kaiser</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samy</forename><surname>Bengio</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in Neural Information Processing Systems, (NIPS)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b16\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Neural GPUs learn algorithms</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Åukasz</forename><surname>Kaiser</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ilya</forename><surname>Sutskever</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">International Conference on Learning Representations (ICLR)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b17\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nal</forename><surname>Kalchbrenner</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lasse</forename><surname>Espeholt</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Karen</forename><surname>Simonyan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Aaron</forename><surname>Van Den Oord</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Graves</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Koray</forename><surname>Kavukcuoglu</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1610.10099v2</idno>\n",
       " <title level=\"m\">Neural machine translation in linear time</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b18\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Structured attention networks</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yoon</forename><surname>Kim</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Carl</forename><surname>Denton</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luong</forename><surname>Hoang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alexander</forename><forename type=\"middle\">M</forename><surname>Rush</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">International Conference on Learning Representations</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b19\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Adam: A method for stochastic optimization</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Diederik</forename><surname>Kingma</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jimmy</forename><surname>Ba</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ICLR</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b20\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oleksii</forename><surname>Kuchaiev</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Boris</forename><surname>Ginsburg</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1703.10722</idno>\n",
       " <title level=\"m\">Factorization tricks for LSTM networks</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b21\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">A structured self-attentive sentence embedding</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhouhan</forename><surname>Lin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Minwei</forename><surname>Feng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Cicero</forename><surname>Nogueira</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mo</forename><surname>Santos</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bing</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bowen</forename><surname>Xiang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yoshua</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Bengio</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1703.03130</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b22\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Multi-task sequence to sequence learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Minh-Thang</forename><surname>Luong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Quoc</forename><forename type=\"middle\">V</forename><surname>Le</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ilya</forename><surname>Sutskever</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oriol</forename><surname>Vinyals</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lukasz</forename><surname>Kaiser</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1511.06114</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b23\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Effective approaches to attentionbased neural machine translation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Minh-Thang</forename><surname>Luong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hieu</forename><surname>Pham</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher D</forename><surname>Manning</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1508.04025</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b24\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Building a large annotated corpus of english: The penn treebank</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Mitchell</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mary</forename><forename type=\"middle\">Ann</forename><surname>Marcus</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Beatrice</forename><surname>Marcinkiewicz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Santorini</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Computational linguistics</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">19</biblScope>\n",
       " <biblScope unit=\"issue\">2</biblScope>\n",
       " <biblScope from=\"313\" to=\"330\" unit=\"page\"/>\n",
       " <date type=\"published\" when=\"1993\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b25\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Effective self-training for parsing</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">David</forename><surname>Mcclosky</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Eugene</forename><surname>Charniak</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mark</forename><surname>Johnson</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the Human Language Technology Conference of the NAACL, Main Conference</title>\n",
       " <meeting>the Human Language Technology Conference of the NAACL, Main Conference</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2006-06\"/>\n",
       " <biblScope from=\"152\" to=\"159\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b26\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A decomposable attention model</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ankur</forename><surname>Parikh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oscar</forename><surname>TÃ¤ckstrÃ¶m</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dipanjan</forename><surname>Das</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jakob</forename><surname>Uszkoreit</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Empirical Methods in Natural Language Processing</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b27\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">A deep reinforced model for abstractive summarization</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Romain</forename><surname>Paulus</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Caiming</forename><surname>Xiong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Richard</forename><surname>Socher</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1705.04304</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b28\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Learning accurate, compact, and interpretable tree annotation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Slav</forename><surname>Petrov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Leon</forename><surname>Barrett</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Romain</forename><surname>Thibaux</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dan</forename><surname>Klein</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL</title>\n",
       " <meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2006-07\"/>\n",
       " <biblScope from=\"433\" to=\"440\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b29\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Using the output embedding to improve language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ofir</forename><surname>Press</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lior</forename><surname>Wolf</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1608.05859</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b30\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rico</forename><surname>Sennrich</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Barry</forename><surname>Haddow</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alexandra</forename><surname>Birch</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1508.07909</idno>\n",
       " <title level=\"m\">Neural machine translation of rare words with subword units</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b31\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noam</forename><surname>Shazeer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Azalia</forename><surname>Mirhoseini</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Krzysztof</forename><surname>Maziarz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andy</forename><surname>Davis</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Quoc</forename><surname>Le</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Geoffrey</forename><surname>Hinton</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeff</forename><surname>Dean</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1701.06538</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b32\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Dropout: a simple way to prevent neural networks from overfitting</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nitish</forename><surname>Srivastava</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Geoffrey</forename><forename type=\"middle\">E</forename><surname>Hinton</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Krizhevsky</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ilya</forename><surname>Sutskever</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ruslan</forename><surname>Salakhutdinov</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Journal of Machine Learning Research</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">15</biblScope>\n",
       " <biblScope unit=\"issue\">1</biblScope>\n",
       " <biblScope from=\"1929\" to=\"1958\" unit=\"page\"/>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b33\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">End-to-end memory networks</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sainbayar</forename><surname>Sukhbaatar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Arthur</forename><surname>Szlam</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jason</forename><surname>Weston</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rob</forename><surname>Fergus</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in Neural Information Processing Systems</title>\n",
       " <editor>\n",
       " <persName><forename type=\"first\">C</forename><surname>Cortes</surname></persName>\n",
       " <persName><forename type=\"first\">N</forename><forename type=\"middle\">D</forename><surname>Lawrence</surname></persName>\n",
       " <persName><forename type=\"first\">D</forename><forename type=\"middle\">D</forename><surname>Lee</surname></persName>\n",
       " <persName><forename type=\"first\">M</forename><surname>Sugiyama</surname></persName>\n",
       " <persName><forename type=\"first\">R</forename><surname>Garnett</surname></persName>\n",
       " </editor>\n",
       " <imprint>\n",
       " <publisher>Curran Associates, Inc</publisher>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " <biblScope unit=\"volume\">28</biblScope>\n",
       " <biblScope from=\"2440\" to=\"2448\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b34\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Sequence to sequence learning with neural networks</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ilya</forename><surname>Sutskever</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oriol</forename><surname>Vinyals</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Quoc Vv</forename><surname>Le</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in Neural Information Processing Systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " <biblScope from=\"3104\" to=\"3112\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b35\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Rethinking the inception architecture for computer vision</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christian</forename><surname>Szegedy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Vincent</forename><surname>Vanhoucke</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sergey</forename><surname>Ioffe</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jonathon</forename><surname>Shlens</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zbigniew</forename><surname>Wojna</surname></persName>\n",
       " </author>\n",
       " <idno>abs/1512.00567</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b36\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Grammar as a foreign language</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">&amp;</forename><surname>Vinyals</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Koo</forename><surname>Kaiser</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sutskever</forename><surname>Petrov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Hinton</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in Neural Information Processing Systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b37\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Google's neural machine translation system: Bridging the gap between human and machine translation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yonghui</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mike</forename><surname>Schuster</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhifeng</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><surname>Quoc</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mohammad</forename><surname>Le</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wolfgang</forename><surname>Norouzi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Maxim</forename><surname>Macherey</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yuan</forename><surname>Krikun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Qin</forename><surname>Cao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Klaus</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Macherey</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1609.08144</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b38\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Deep recurrent models with fast-forward connections for neural machine translation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jie</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ying</forename><surname>Cao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xuguang</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Peng</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wei</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <idno>abs/1606.04199</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b39\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Fast and accurate shift-reduce constituent parsing</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Muhua</forename><surname>Zhu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yue</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wenliang</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Min</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jingbo</forename><surname>Zhu</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 51st Annual Meeting of the ACL</title>\n",
       " <meeting>the 51st Annual Meeting of the ACL</meeting>\n",
       " <imprint>\n",
       " <publisher>ACL</publisher>\n",
       " <date type=\"published\" when=\"2013-08\"/>\n",
       " <biblScope unit=\"volume\">1</biblScope>\n",
       " <biblScope from=\"434\" to=\"443\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " </listBibl>\n",
       " </div>\n",
       " </back>\n",
       " </text>\n",
       " </TEI>,\n",
       " <?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       " <TEI xml:space=\"preserve\" xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd\">\n",
       " <teiHeader xml:lang=\"en\">\n",
       " <fileDesc>\n",
       " <titleStmt>\n",
       " <title level=\"a\" type=\"main\">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>\n",
       " </titleStmt>\n",
       " <publicationStmt>\n",
       " <publisher/>\n",
       " <availability status=\"unknown\"><licence/></availability>\n",
       " </publicationStmt>\n",
       " <sourceDesc>\n",
       " <biblStruct>\n",
       " <analytic>\n",
       " <author role=\"corresp\">\n",
       " <persName><forename type=\"first\">Victor</forename><surname>Sanh</surname></persName>\n",
       " <email>victor@huggingface.co</email>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"department\">Lysandre DEBUT</orgName>\n",
       " <orgName key=\"instit1\" type=\"institution\">Julien CHAUMOND</orgName>\n",
       " <orgName key=\"instit2\" type=\"institution\">Thomas WOLF Hugging Face</orgName>\n",
       " </affiliation>\n",
       " </author>\n",
       " <title level=\"a\" type=\"main\">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <imprint>\n",
       " <date/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " </sourceDesc>\n",
       " </fileDesc>\n",
       " <encodingDesc>\n",
       " <appInfo>\n",
       " <application ident=\"GROBID\" version=\"0.7.0-SNAPSHOT\" when=\"2024-04-27T16:15+0000\">\n",
       " <desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>\n",
       " <ref target=\"https://github.com/kermitt2/grobid\"/>\n",
       " </application>\n",
       " </appInfo>\n",
       " </encodingDesc>\n",
       " <profileDesc>\n",
       " <abstract>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</p></div>\n",
       " </abstract>\n",
       " </profileDesc>\n",
       " </teiHeader>\n",
       " <text xml:lang=\"en\">\n",
       " <body>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>The last two years have seen the rise of Transfer Learning approaches in Natural Language Processing (NLP) with large-scale pre-trained language models becoming a basic tool in many NLP tasks <ref target=\"#b0\" type=\"bibr\">[Devlin et al., 2018</ref><ref target=\"#b1\" type=\"bibr\">, Radford et al., 2019</ref>. While these models lead to significant improvement, they often have several hundred million parameters and current research 1 on pre-trained models indicates that training even larger models still leads to better performances on downstream tasks.</p><p>The trend toward bigger models raises several concerns. First is the environmental cost of exponentially scaling these models' computational requirements as mentioned in <ref type=\"bibr\">Schwartz et al. [2019]</ref>, <ref target=\"#b3\" type=\"bibr\">Strubell et al. [2019]</ref>. Second, while operating these models on-device in real-time has the potential to enable novel and interesting language processing applications, the growing computational and memory requirements of these models may hamper wide adoption.</p><p>In this paper, we show that it is possible to reach similar performances on many downstream-tasks using much smaller language models pre-trained with knowledge distillation, resulting in models that are lighter and faster at inference time, while also requiring a smaller computational training budget. Our general-purpose pre-trained models can be fine-tuned with good performances on several downstream tasks, keeping the flexibility of larger models. We also show that our compressed models are small enough to run on the edge, e.g. on mobile devices.</p><p>Using a triple loss, we show that a 40% smaller Transformer <ref target=\"#b4\" type=\"bibr\">(Vaswani et al. [2017]</ref>) pre-trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time. Further ablation studies indicate that all the components of the triple loss are important for best performances.</p><p>We have made the trained weights available along with the training code in the Transformers 2 library from HuggingFace <ref target=\"#b5\" type=\"bibr\">[Wolf et al., 2019]</ref>.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">Knowledge distillation</head><p>Knowledge distillation <ref target=\"#b6\" type=\"bibr\">[Bucila et al., 2006</ref><ref target=\"#b7\" type=\"bibr\">, Hinton et al., 2015</ref>] is a compression technique in which a compact model -the student -is trained to reproduce the behaviour of a larger model -the teacheror an ensemble of models.</p><p>In supervised learning, a classification model is generally trained to predict an instance class by maximizing the estimated probability of gold labels. A standard training objective thus involves minimizing the cross-entropy between the model's predicted distribution and the one-hot empirical distribution of training labels. A model performing well on the training set will predict an output distribution with high probability on the correct class and with near-zero probabilities on other classes. But some of these \"near-zero\" probabilities are larger than others and reflect, in part, the generalization capabilities of the model and how well it will perform on the test set <ref type=\"bibr\">3</ref> .</p><p>Training loss The student is trained with a distillation loss over the soft target probabilities of the teacher: L ce = i t i * log(s i ) where t i (resp. s i ) is a probability estimated by the teacher (resp. the student). This objective results in a rich training signal by leveraging the full teacher distribution. Following <ref target=\"#b7\" type=\"bibr\">Hinton et al. [2015]</ref> we used a softmax-temperature:</p><formula xml:id=\"formula_0\">p i = exp(zi/T ) j exp(zj /T )</formula><p>where T controls the smoothness of the output distribution and z i is the model score for the class i. The same temperature T is applied to the student and the teacher at training time, while at inference, T is set to 1 to recover a standard softmax.</p><p>The final training objective is a linear combination of the distillation loss L ce with the supervised training loss, in our case the masked language modeling loss L mlm <ref target=\"#b0\" type=\"bibr\">[Devlin et al., 2018]</ref>. We found it beneficial to add a cosine embedding loss (L cos ) which will tend to align the directions of the student and teacher hidden states vectors.</p><p>3 DistilBERT: a distilled version of BERT Student architecture In the present work, the student -DistilBERT -has the same general architecture as BERT. The token-type embeddings and the pooler are removed while the number of layers is reduced by a factor of 2. Most of the operations used in the Transformer architecture (linear layer and layer normalisation) are highly optimized in modern linear algebra frameworks and our investigations showed that variations on the last dimension of the tensor (hidden size dimension) have a smaller impact on computation efficiency (for a fixed parameters budget) than variations on other factors like the number of layers. Thus we focus on reducing the number of layers.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Student initialization</head><p>In addition to the previously described optimization and architectural choices, an important element in our training procedure is to find the right initialization for the sub-network to converge. Taking advantage of the common dimensionality between teacher and student networks, we initialize the student from the teacher by taking one layer out of two. </p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4\">Experiments</head><p>General Language Understanding We assess the language understanding and generalization capabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark <ref target=\"#b9\" type=\"bibr\">[Wang et al., 2018]</ref>, a collection of 9 datasets for evaluating natural language understanding systems. We report scores on the development sets for each task by fine-tuning DistilBERT without the use of ensembling or multi-tasking scheme for fine-tuning (which are mostly orthogonal to the present work). We compare the results to the baseline provided by the authors of GLUE: an ELMo <ref target=\"#b10\" type=\"bibr\">(Peters et al. [2018]</ref>) encoder followed by two BiLSTMs. 4</p><p>The results on each of the 9 tasks are showed on Table <ref target=\"#tab_0\" type=\"table\">1</ref> along with the macro-score (average of individual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo baseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to BERT, retaining 97% of the performance with 40% fewer parameters.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1\">Downstream task benchmark</head><p>Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - <ref target=\"#b12\" type=\"bibr\">Maas et al. [2011]</ref>) and a question answering task <ref type=\"bibr\">(SQuAD v1.1 -Rajpurkar et al. [2016]</ref>).</p><p>As shown in Table <ref type=\"table\">2</ref>, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb benchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT.</p><p>We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a </p><formula xml:id=\"formula_1\">âˆ… -L cos -L mlm -2.96 L ce -âˆ… -L mlm -1.46 L ce -L cos -âˆ… -0.</formula><p>31 Triple loss + random weights initialization -3.69</p><p>teacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase.</p><p>In this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and 70.4 EM, i.e. within 3 points of the full model.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Size and inference speed</head><p>To further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table <ref type=\"table\">3</ref>) the number of parameters of each model along with the inference time needed to do a full pass on the STS-B development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1. DistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT.</p><p>On device computation We studied whether DistilBERT could be used for on-the-edge applications by building a mobile application for question answering. We compare the average inference time on a recent smartphone (iPhone 7 Plus) against our previously trained question answering model based on BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole model weighs 207 MB (which could be further reduced with quantization). Our code is available 5 .</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2\">Ablation study</head><p>In this section, we investigate the influence of various components of the triple loss and the student initialization on the performances of the distilled model. We report the macro-score on GLUE. Table <ref target=\"#tab_1\" type=\"table\">4</ref> presents the deltas with the full triple loss: removing the Masked Language Modeling loss has little impact while the two distillation losses account for a large portion of the performance.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5\">Related work</head><p>Task-specific distillation Most of the prior works focus on building task-specific distillation setups. <ref target=\"#b14\" type=\"bibr\">Tang et al. [2019]</ref> transfer fine-tune classification model BERT to an LSTM-based classifier. <ref target=\"#b15\" type=\"bibr\">Chatterjee [2019]</ref> distill BERT model fine-tuned on SQuAD in a smaller Transformer model previously initialized from BERT. In the present work, we found it beneficial to use a general-purpose pre-training distillation rather than a task-specific distillation. <ref type=\"bibr\">Turc et al. [2019]</ref> use the original pretraining objective to train smaller student, then fine-tuned via distillation. As shown in the ablation study, we found it beneficial to leverage the teacher's knowledge to pre-train with additional distillation signal.</p><p>Multi-distillation <ref target=\"#b16\" type=\"bibr\">Yang et al. [2019]</ref> combine the knowledge of an ensemble of teachers using multi-task learning to regularize the distillation. The authors apply Multi-Task Knowledge Distillation to learn a compact question answering model from a set of large question answering models. An application of multi-distillation is multi-linguality: <ref target=\"#b17\" type=\"bibr\">Tsai et al. [2019]</ref> adopts a similar approach to us by pre-training a multilingual model from scratch solely through distillation. However, as shown in the ablation study, leveraging the teacher's knowledge with initialization and additional losses leads to substantial gains.</p><p>Other compression techniques have been studied to compress large models. Recent developments in weights pruning reveal that it is possible to remove some heads in the self-attention at test time without significantly degrading the performance <ref target=\"#b18\" type=\"bibr\">Michel et al. [2019]</ref>. Some layers can be reduced to one head. A separate line of study leverages quantization to derive smaller models <ref target=\"#b19\" type=\"bibr\">(Gupta et al. [2015]</ref>). Pruning and quantization are orthogonal to the present work.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6\">Conclusion and future work</head><p>We introduced DistilBERT, a general-purpose pre-trained version of BERT, 40% smaller, 60% faster, that retains 97% of the language understanding capabilities. We showed that a general-purpose language model can be successfully trained with distillation and analyzed the various components with an ablation study. We further demonstrated that DistilBERT is a compelling option for edge applications.</p></div><figure xml:id=\"fig_0\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Parameter counts of several recently released pretrained language models.</figDesc></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_0\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 1 :</head><label>1</label><figDesc>DistilBERT retains 97% of BERT performance. Comparison on the dev sets of the GLUE benchmark. ELMo results as reported by the authors. BERT and DistilBERT results are the medians of 5 runs with different seeds.ModelScore CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B WNLI</figDesc><table><row><cell>ELMo</cell><cell>68.7</cell><cell>44.1</cell><cell>68.6</cell><cell>76.6</cell><cell>71.1</cell><cell>86.2 53.4</cell><cell>91.5</cell><cell>70.4</cell><cell>56.3</cell></row><row><cell>BERT-base</cell><cell>79.5</cell><cell>56.3</cell><cell>86.7</cell><cell>88.6</cell><cell>91.8</cell><cell>89.6 69.3</cell><cell>92.7</cell><cell>89.0</cell><cell>53.5</cell></row><row><cell>DistilBERT</cell><cell>77.0</cell><cell>51.3</cell><cell>82.2</cell><cell>87.5</cell><cell>89.2</cell><cell>88.5 59.9</cell><cell>91.3</cell><cell>86.9</cell><cell>56.3</cell></row><row><cell cols=\"5\">Table 2: DistilBERT yields to comparable performance on downstream tasks. Com-parison on downstream tasks: IMDb (test ac-curacy) and SQuAD 1.1 (EM/F1 on dev set). D: with a second step of distillation during fine-tuning.</cell><cell cols=\"5\">Table 3: DistilBERT is significantly smaller while being constantly faster. Inference time of a full pass of GLUE task STS-B (sen-timent analysis) on CPU with a batch size of 1.</cell></row><row><cell>Model</cell><cell/><cell cols=\"2\">IMDb SQuAD (acc.) (EM/F1)</cell><cell/><cell/><cell>Model</cell><cell cols=\"3\"># param. (Millions) (seconds) Inf. time</cell></row><row><cell cols=\"2\">BERT-base DistilBERT DistilBERT (D)</cell><cell cols=\"2\">93.46 81.2/88.5 92.82 77.7/85.8 -79.1/86.9</cell><cell/><cell/><cell>ELMo BERT-base DistilBERT</cell><cell>180 110 66</cell><cell/><cell>895 668 410</cell></row></table><note>Distillation We applied best practices for training BERT model recently proposed in. As such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K examples per batch) using dynamic masking and without the next sentence prediction objective.Data and compute power We train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus<ref target=\"#b8\" type=\"bibr\">[Zhu et al., 2015]</ref>. DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the RoBERTa model required 1 day of training on 1024 32GB V100.</note></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_1\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 4 :</head><label>4</label><figDesc>Ablation study. Variations are relative to the model trained with triple loss and teacher weights initialization.</figDesc><table><row><cell>Ablation</cell><cell>Variation on GLUE macro-score</cell></row></table></figure>\n",
       " <note n=\"1\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">See for instance the recently released MegatronLM (https://nv-adlr.github.io/MegatronLM) EMC^2: 5th Edition Co-located with NeurIPS'19 arXiv:1910.01108v4 [cs.CL] 1 Mar 2020</note>\n",
       " <note n=\"2\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">https://github.com/huggingface/transformers 3 E.g. BERT-base's predictions for a masked token in \"I think this is the beginning of a beautiful [MASK]\" comprise two high probability tokens (day and life) and a long tail of valid predictions (future, story, world. . . ).</note>\n",
       " <note n=\"4\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">We use jiant<ref target=\"#b11\" type=\"bibr\">[Wang et al., 2019]</ref> to compute the baseline.</note>\n",
       " <note n=\"5\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">https://github.com/huggingface/swift-coreml-transformers</note>\n",
       " </body>\n",
       " <back>\n",
       " <div type=\"references\">\n",
       " <listBibl>\n",
       " <biblStruct xml:id=\"b0\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Bert: Pre-training of deep bidirectional transformers for language understanding</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jacob</forename><surname>Devlin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ming-Wei</forename><surname>Chang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kristina</forename><surname>Toutanova</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NAACL-HLT</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b1\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Language models are unsupervised multitask learners</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alec</forename><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeffrey</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rewon</forename><surname>Child</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">David</forename><surname>Luan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dario</forename><surname>Amodei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ilya</forename><surname>Sutskever</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b2\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Roberta: A robustly optimized bert pretraining approach. ArXiv, abs</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yinhan</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Myle</forename><surname>Ott</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Naman</forename><surname>Goyal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jingfei</forename><surname>Du</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mandar</forename><forename type=\"middle\">S</forename><surname>Joshi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Danqi</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Omer</forename><surname>Levy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mike</forename><surname>Lewis</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luke</forename><forename type=\"middle\">S</forename><surname>Zettlemoyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Veselin</forename><surname>Stoyanov</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"1907\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b3\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Energy and policy considerations for deep learning in nlp</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Emma</forename><surname>Strubell</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ananya</forename><surname>Ganesh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrew</forename><surname>Mccallum</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b4\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Attention is all you need</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ashish</forename><surname>Vaswani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noam</forename><surname>Shazeer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Niki</forename><surname>Parmar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jakob</forename><surname>Uszkoreit</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Llion</forename><surname>Jones</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Aidan</forename><forename type=\"middle\">N</forename><surname>Gomez</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lukasz</forename><surname>Kaiser</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Illia</forename><surname>Polosukhin</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NIPS</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b5\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Transformers: State-of-the-art natural language processing</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Thomas</forename><surname>Wolf</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lysandre</forename><surname>Debut</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Victor</forename><surname>Sanh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Julien</forename><surname>Chaumond</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Clement</forename><surname>Delangue</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Anthony</forename><surname>Moi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pierric</forename><surname>Cistac</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tim</forename><surname>Rault</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">RÃ©mi</forename><surname>Louf</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Morgan</forename><surname>Funtowicz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jamie</forename><surname>Brew</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b6\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Model compression</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Cristian</forename><surname>Bucila</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rich</forename><surname>Caruana</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">KDD</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2006\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b7\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Distilling the knowledge in a neural network</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Geoffrey</forename><forename type=\"middle\">E</forename><surname>Hinton</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oriol</forename><surname>Vinyals</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeffrey</forename><surname>Dean</surname></persName>\n",
       " </author>\n",
       " <idno>abs/1503.02531</idno>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">ArXiv</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b8\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yukun</forename><surname>Zhu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ryan</forename><surname>Kiros</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Richard</forename><forename type=\"middle\">S</forename><surname>Zemel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ruslan</forename><surname>Salakhutdinov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Raquel</forename><surname>Urtasun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Antonio</forename><surname>Torralba</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sanja</forename><surname>Fidler</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">IEEE International Conference on Computer Vision (ICCV)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " <biblScope from=\"19\" to=\"27\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b9\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Amanpreet</forename><surname>Singh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Julian</forename><surname>Michael</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Felix</forename><surname>Hill</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Omer</forename><surname>Levy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samuel</forename><forename type=\"middle\">R</forename><surname>Bowman</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ICLR</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b10\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Deep contextualized word representations</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matthew</forename><forename type=\"middle\">E</forename><surname>Peters</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mark</forename><surname>Neumann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mohit</forename><surname>Iyyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matt</forename><surname>Gardner</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><surname>Clark</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NAACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b11\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ian</forename><forename type=\"middle\">F</forename><surname>Tenney</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yada</forename><surname>Pruksachatkun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Katherin</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jan</forename><surname>Hula</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Patrick</forename><surname>Xia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Raghu</forename><surname>Pappagari</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><forename type=\"middle\">Thomas</forename><surname>Shuning Jin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Roma</forename><surname>Mccoy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yinghui</forename><surname>Patel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jason</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Edouard</forename><surname>Phang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Najoung</forename><surname>Grave</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Kim</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Phu Mon</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Htut</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Berlin</forename><surname>Thibault F'evry</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nikita</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Haokun</forename><surname>Nangia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <ptr target=\"http://jiant.info/\"/>\n",
       " <title level=\"m\">Anhad Mohananey, Shikha Bordia, Nicolas Patry, Ellie Pavlick, and Samuel R. Bowman. jiant 1.1: A software toolkit for research on general-purpose text understanding models</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b12\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Learning word vectors for sentiment analysis</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrew</forename><forename type=\"middle\">L</forename><surname>Maas</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Raymond</forename><forename type=\"middle\">E</forename><surname>Daly</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Peter</forename><forename type=\"middle\">T</forename><surname>Pham</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dan</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrew</forename><forename type=\"middle\">Y</forename><surname>Ng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><surname>Potts</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2011\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b13\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Squad: 100, 000+ questions for machine comprehension of text</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pranav</forename><surname>Rajpurkar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jian</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Konstantin</forename><surname>Lopyrev</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Percy</forename><surname>Liang</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">EMNLP</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b14\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Distilling task-specific knowledge from bert into simple neural networks. ArXiv, abs</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Raphael</forename><surname>Tang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yao</forename><surname>Lu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Linqing</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lili</forename><surname>Mou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Olga</forename><surname>Vechtomova</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jimmy</forename><surname>Lin</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"1903\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b15\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Well-read students learn better: The impact of student initialization on knowledge distillation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Debajyoti</forename><surname>Chatterjee</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">ArXiv</title>\n",
       " <editor>Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova</editor>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"1904\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>Making neural machine reading comprehension faster. ArXiv, abs/</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b16\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Model compression with multi-task knowledge distillation for web-scale question answering system. ArXiv, abs</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ze</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Linjun</forename><surname>Shou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ming</forename><surname>Gong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wutao</forename><surname>Lin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Daxin</forename><surname>Jiang</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"1904\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b17\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Small and practical bert models for sequence labeling</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Henry</forename><surname>Tsai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jason</forename><surname>Riesa</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Melvin</forename><surname>Johnson</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Naveen</forename><surname>Arivazhagan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xin</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Amelia</forename><surname>Archer</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">EMNLP-IJCNLP</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b18\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Are sixteen heads really better than one</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Paul</forename><surname>Michel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Omer</forename><surname>Levy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Graham</forename><surname>Neubig</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NeurIPS</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b19\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Deep learning with limited numerical precision</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Suyog</forename><surname>Gupta</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ankur</forename><surname>Agrawal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kailash</forename><surname>Gopalakrishnan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pritish</forename><surname>Narayanan</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ICML</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " </listBibl>\n",
       " </div>\n",
       " </back>\n",
       " </text>\n",
       " </TEI>,\n",
       " <?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       " <TEI xml:space=\"preserve\" xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd\">\n",
       " <teiHeader xml:lang=\"en\">\n",
       " <fileDesc>\n",
       " <titleStmt>\n",
       " <title level=\"a\" type=\"main\">Exploring teachersâ€™ confidence in addressing mental health issues in learners with Profound and Multiple Learning Difficulties (PMLD) pre and post training</title>\n",
       " </titleStmt>\n",
       " <publicationStmt>\n",
       " <publisher>University of Wales Press/Gwasg Prifysgol Cymru</publisher>\n",
       " <availability status=\"unknown\"><p>Copyright University of Wales Press/Gwasg Prifysgol Cymru</p>\n",
       " </availability>\n",
       " <date type=\"published\" when=\"2019-05-24\">24 May 2019</date>\n",
       " </publicationStmt>\n",
       " <sourceDesc>\n",
       " <biblStruct>\n",
       " <analytic>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jacob</forename><surname>Devlin</surname></persName>\n",
       " <email>jacobdevlin@google.com</email>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ming-Wei</forename><surname>Chang</surname></persName>\n",
       " <email>mingweichang@google.com</email>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n",
       " <email>kentonl@google.com</email>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kristina</forename><surname>Toutanova</surname></persName>\n",
       " </author>\n",
       " <title level=\"a\" type=\"main\">Exploring teachersâ€™ confidence in addressing mental health issues in learners with Profound and Multiple Learning Difficulties (PMLD) pre and post training</title>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\" type=\"main\">Cylchgrawn Addysg Cymru / Wales Journal of Education</title>\n",
       " <title level=\"j\" type=\"abbrev\">WJE</title>\n",
       " <idno type=\"ISSN\">2059-3708</idno>\n",
       " <idno type=\"eISSN\">2059-3716</idno>\n",
       " <imprint>\n",
       " <publisher>University of Wales Press/Gwasg Prifysgol Cymru</publisher>\n",
       " <date type=\"published\" when=\"2019-05-24\">24 May 2019</date>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <idno type=\"DOI\">10.16922/wje.p5</idno>\n",
       " <idno type=\"arXiv\">arXiv:1810.04805v2[cs.CL]</idno>\n",
       " </biblStruct>\n",
       " </sourceDesc>\n",
       " </fileDesc>\n",
       " <encodingDesc>\n",
       " <appInfo>\n",
       " <application ident=\"GROBID\" version=\"0.7.0-SNAPSHOT\" when=\"2024-04-27T16:15+0000\">\n",
       " <desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>\n",
       " <ref target=\"https://github.com/kermitt2/grobid\"/>\n",
       " </application>\n",
       " </appInfo>\n",
       " </encodingDesc>\n",
       " <profileDesc>\n",
       " <abstract>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p><p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p></div>\n",
       " </abstract>\n",
       " </profileDesc>\n",
       " </teiHeader>\n",
       " <text xml:lang=\"en\">\n",
       " <body>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and <ref target=\"#b14\" type=\"bibr\">Le, 2015;</ref><ref target=\"#b35\" type=\"bibr\">Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018;</ref><ref target=\"#b20\" type=\"bibr\">Howard and Ruder, 2018)</ref>. These include sentence-level tasks such as natural language inference <ref target=\"#b5\" type=\"bibr\">(Bowman et al., 2015;</ref><ref target=\"#b50\" type=\"bibr\">Williams et al., 2018)</ref> and paraphrasing <ref target=\"#b16\" type=\"bibr\">(Dolan and Brockett, 2005)</ref>, which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim <ref target=\"#b43\" type=\"bibr\">Sang and De Meulder, 2003;</ref><ref target=\"#b38\" type=\"bibr\">Rajpurkar et al., 2016)</ref>.</p><p>There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a)</ref>, uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) <ref target=\"#b37\" type=\"bibr\">(Radford et al., 2018)</ref>, introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.</p><p>We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer <ref target=\"#b45\" type=\"bibr\">(Vaswani et al., 2017)</ref>. Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.</p><p>In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task <ref target=\"#b42\" type=\"bibr\">(Taylor, 1953)</ref>. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations. The contributions of our paper are as follows:</p><p>â€¢ We demonstrate the importance of bidirectional pre-training for language representations. Unlike <ref target=\"#b37\" type=\"bibr\">Radford et al. (2018)</ref>, which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to <ref target=\"#b35\" type=\"bibr\">Peters et al. (2018a)</ref>, which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.</p><p>â€¢ We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.</p><p>â€¢ BERT advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at https://github.com/ google-research/bert.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">Related Work</head><p>There is a long history of pre-training general language representations, and we briefly review the most widely-used approaches in this section.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.1\">Unsupervised Feature-based Approaches</head><p>Learning widely applicable representations of words has been an active area of research for decades, including non-neural <ref target=\"#b6\" type=\"bibr\">(Brown et al., 1992;</ref><ref target=\"#b2\" type=\"bibr\">Ando and Zhang, 2005;</ref><ref target=\"#b4\" type=\"bibr\">Blitzer et al., 2006)</ref> and neural <ref target=\"#b33\" type=\"bibr\">Pennington et al., 2014)</ref> methods. Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch <ref target=\"#b44\" type=\"bibr\">(Turian et al., 2010)</ref>. To pretrain word embedding vectors, left-to-right language modeling objectives have been used <ref target=\"#b31\" type=\"bibr\">(Mnih and Hinton, 2009)</ref>, as well as objectives to discriminate correct from incorrect words in left and right context .</p><p>These approaches have been generalized to coarser granularities, such as sentence embeddings <ref target=\"#b27\" type=\"bibr\">Logeswaran and Lee, 2018)</ref> or paragraph embeddings <ref target=\"#b25\" type=\"bibr\">(Le and Mikolov, 2014)</ref>. To train sentence representations, prior work has used objectives to rank candidate next sentences <ref target=\"#b22\" type=\"bibr\">(Jernite et al., 2017;</ref><ref target=\"#b27\" type=\"bibr\">Logeswaran and Lee, 2018)</ref>, left-to-right generation of next sentence words given a representation of the previous sentence , or denoising autoencoder derived objectives <ref target=\"#b19\" type=\"bibr\">(Hill et al., 2016)</ref>.</p><p>ELMo and its predecessor <ref target=\"#b34\" type=\"bibr\">(Peters et al., 2017</ref><ref target=\"#b35\" type=\"bibr\">(Peters et al., , 2018a</ref> generalize traditional word embedding research along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a)</ref> including question answering <ref target=\"#b38\" type=\"bibr\">(Rajpurkar et al., 2016)</ref>, sentiment analysis <ref target=\"#b40\" type=\"bibr\">(Socher et al., 2013)</ref>, and named entity recognition (Tjong Kim <ref target=\"#b43\" type=\"bibr\">Sang and De Meulder, 2003)</ref>. <ref target=\"#b29\" type=\"bibr\">Melamud et al. (2016)</ref> proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. <ref target=\"#b17\" type=\"bibr\">Fedus et al. (2018)</ref> shows that the cloze task can be used to improve the robustness of text generation models.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Unsupervised Fine-tuning Approaches</head><p>As with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text <ref target=\"#b12\" type=\"bibr\">(Collobert and Weston, 2008)</ref>.</p><p>More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task (Dai and <ref target=\"#b14\" type=\"bibr\">Le, 2015;</ref><ref target=\"#b20\" type=\"bibr\">Howard and Ruder, 2018;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>. The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT <ref target=\"#b37\" type=\"bibr\">(Radford et al., 2018)</ref> achieved previously state-of-the-art results on many sentencelevel tasks from the GLUE benchmark <ref target=\"#b47\" type=\"bibr\">(Wang et al., 2018a)</ref>. Left-to-right language model-BERT BERT</p><formula xml:id=\"formula_0\">E [CLS] E 1 E [SEP] ... E N E 1 ' ... E M ' C T 1 T [SEP]</formula><p>... ... ing and auto-encoder objectives have been used for pre-training such models <ref target=\"#b20\" type=\"bibr\">(Howard and Ruder, 2018;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018;</ref><ref target=\"#b14\" type=\"bibr\">Dai and Le, 2015)</ref>.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.3\">Transfer Learning from Supervised Data</head><p>There has also been work showing effective transfer from supervised tasks with large datasets, such as natural language inference <ref target=\"#b13\" type=\"bibr\">(Conneau et al., 2017)</ref> and machine translation <ref target=\"#b28\" type=\"bibr\">(McCann et al., 2017)</ref>. Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models, where an effective recipe is to fine-tune models pre-trained with Ima-geNet <ref target=\"#b15\" type=\"bibr\">(Deng et al., 2009;</ref><ref target=\"#b52\" type=\"bibr\">Yosinski et al., 2014)</ref>.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3\">BERT</head><p>We introduce BERT and its detailed implementation in this section. There are two steps in our framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure <ref type=\"figure\">1</ref> will serve as a running example for this section.</p><p>A distinctive feature of BERT is its unified architecture across different tasks. There is mini-mal difference between the pre-trained architecture and the final downstream architecture.</p><p>Model Architecture BERT's model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in <ref target=\"#b45\" type=\"bibr\">Vaswani et al. (2017)</ref> and released in the tensor2tensor library. 1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to <ref target=\"#b45\" type=\"bibr\">Vaswani et al. (2017)</ref> as well as excellent guides such as \"The Annotated Transformer.\" <ref type=\"bibr\">2</ref> In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A. <ref type=\"bibr\">3</ref> We primarily report results on two model sizes: BERT BASE (L=12, H=768, A=12, Total Param-eters=110M) and BERT LARGE (L=24, H=1024, A=16, Total Parameters=340M).</p><p>BERT BASE was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left. <ref type=\"bibr\">4</ref> Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., Question, Answer ) in one token sequence. Throughout this work, a \"sentence\" can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A \"sequence\" refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.</p><p>We use WordPiece embeddings <ref target=\"#b51\" type=\"bibr\">(Wu et al., 2016)</ref> with a 30,000 token vocabulary. The first token of every sequence is always a special classification token <ref type=\"bibr\">([CLS]</ref>). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token <ref type=\"bibr\">([SEP]</ref>). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure <ref type=\"figure\">1</ref>, we denote input embedding as E, the final hidden vector of the special [CLS] token as C âˆˆ R H , and the final hidden vector for the i th input token as</p><formula xml:id=\"formula_1\">T i âˆˆ R H .</formula><p>For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure <ref target=\"#fig_0\" type=\"figure\">2</ref>.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1\">Pre-training BERT</head><p>Unlike <ref target=\"#b35\" type=\"bibr\">Peters et al. (2018a)</ref> and <ref target=\"#b37\" type=\"bibr\">Radford et al. (2018)</ref>, we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure <ref type=\"figure\">1</ref>.</p><p>Task #1: Masked LM Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-toright and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly \"see itself\", and the model could trivially predict the target word in a multi-layered context.</p><p>former is often referred to as a \"Transformer encoder\" while the left-context-only version is referred to as a \"Transformer decoder\" since it can be used for text generation.</p><p>In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a \"masked LM\" (MLM), although it is often referred to as a Cloze task in the literature <ref target=\"#b42\" type=\"bibr\">(Taylor, 1953)</ref>. In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders <ref target=\"#b46\" type=\"bibr\">(Vincent et al., 2008)</ref>, we only predict the masked words rather than reconstructing the entire input.</p><p>Although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning. To mitigate this, we do not always replace \"masked\" words with the actual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, T i will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2.</p><p>Task #2: Next Sentence Prediction (NSP) Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure <ref type=\"figure\">1</ref>, C is used for next sentence prediction (NSP). 5 Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI. 6  The NSP task is closely related to representationlearning objectives used in <ref target=\"#b22\" type=\"bibr\">Jernite et al. (2017)</ref> and <ref target=\"#b27\" type=\"bibr\">Logeswaran and Lee (2018)</ref>. However, in prior work, only sentence embeddings are transferred to down-stream tasks, where BERT transfers all parameters to initialize end-task model parameters.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Pre-training data</head><p>The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words)  and English Wikipedia (2,500M words).</p><p>For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark <ref target=\"#b8\" type=\"bibr\">(Chelba et al., 2013)</ref> in order to extract long contiguous sequences.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Fine-tuning BERT</head><p>Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs-by swapping out the appropriate inputs and outputs.</p><p>For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as Parikh et al. ( <ref type=\"formula\">2016</ref>); <ref target=\"#b39\" type=\"bibr\">Seo et al. (2017)</ref>. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.</p><p>For each task, we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end-to-end. At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate text-âˆ… pair in text classification or sequence tagging. At the output, the token representations are fed into an output layer for tokenlevel tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis.</p><p>Compared to pre-training, fine-tuning is relatively inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model. <ref type=\"bibr\">7</ref> We describe the task-specific details in the corresponding subsections of Section 4. More details can be found in Appendix A.5.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4\">Experiments</head><p>In this section, we present BERT fine-tuning results on 11 NLP tasks.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1\">GLUE</head><p>The General Language Understanding Evaluation (GLUE) benchmark <ref target=\"#b47\" type=\"bibr\">(Wang et al., 2018a</ref>) is a collection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1.</p><p>To fine-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the final hidden vector C âˆˆ R H corresponding to the first input token ([CLS]) as the aggregate representation. The only new parameters introduced during fine-tuning are classification layer weights W âˆˆ R KÃ—H , where K is the number of labels. We compute a standard classification loss with C and W , i.e., log(softmax(CW T )).  We use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERT LARGE we found that finetuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but perform different fine-tuning data shuffling and classifier layer initialization. <ref type=\"bibr\">9</ref> Results are presented in Table <ref target=\"#tab_4\" type=\"table\">1</ref>. Both BERT BASE and BERT LARGE outperform all systems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy improvement over the prior state of the art. Note that BERT BASE and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the official GLUE leaderboard 10 , BERT LARGE obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing.</p><p>We find that BERT LARGE significantly outperforms BERT BASE across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2\">SQuAD v1.1</head><p>The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowdsourced question/answer pairs <ref target=\"#b38\" type=\"bibr\">(Rajpurkar et al., 2016)</ref>. Given a question and a passage from <ref type=\"bibr\">9</ref> The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE.</p><p>10 https://gluebenchmark.com/leaderboard</p><p>Wikipedia containing the answer, the task is to predict the answer text span in the passage.</p><p>As shown in Figure <ref type=\"figure\">1</ref>, in the question answering task, we represent the input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding. We only introduce a start vector S âˆˆ R H and an end vector E âˆˆ R H during fine-tuning. The probability of word i being the start of the answer span is computed as a dot product between T i and S followed by a softmax over all of the words in the paragraph: P i = e Sâ€¢T i j e Sâ€¢T j . The analogous formula is used for the end of the answer span. The score of a candidate span from position i to position j is defined as Sâ€¢T i + Eâ€¢T j , and the maximum scoring span where j â‰¥ i is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.</p><p>Table <ref target=\"#tab_6\" type=\"table\">2</ref> shows top leaderboard entries as well as results from top published systems <ref target=\"#b39\" type=\"bibr\">(Seo et al., 2017;</ref><ref target=\"#b10\" type=\"bibr\">Clark and Gardner, 2018;</ref><ref target=\"#b35\" type=\"bibr\">Peters et al., 2018a;</ref><ref target=\"#b21\" type=\"bibr\">Hu et al., 2018)</ref>. The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available, 11 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA <ref target=\"#b23\" type=\"bibr\">(Joshi et al., 2017)</ref> befor fine-tuning on SQuAD.</p><p>Our best performing system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system. In fact, our single BERT model outperforms the top ensemble system in terms of F1 score.   tuning data, we only lose 0.1-0.4 F1, still outperforming all existing systems by a wide margin. 12 4.3 SQuAD v2.0</p><p>The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided paragraph, making the problem more realistic.</p><p>We use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat questions that do not have an answer as having an answer span with start and end at the [CLS] token. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, we compare the score of the no-answer span: s null = Sâ€¢C + Eâ€¢C to the score of the best non-null span <ref type=\"bibr\">12</ref> The TriviaQA data we used consists of paragraphs from TriviaQA-Wiki formed of the first 400 tokens in documents, that contain at least one of the provided possible answers.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>System</head><p>Dev Test  s i,j = max jâ‰¥i Sâ€¢T i + Eâ€¢T j . We predict a non-null answer whenÅ i,j &gt; s null + Ï„ , where the threshold Ï„ is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.</p><p>The results compared to prior leaderboard entries and top published work <ref target=\"#b41\" type=\"bibr\">(Sun et al., 2018;</ref><ref target=\"#b48\" type=\"bibr\">Wang et al., 2018b</ref>) are shown in Table <ref target=\"#tab_7\" type=\"table\">3</ref>, excluding systems that use BERT as one of their components. We observe a +5.1 F1 improvement over the previous best system.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.4\">SWAG</head><p>The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference <ref target=\"#b54\" type=\"bibr\">(Zellers et al., 2018)</ref>. Given a sentence, the task is to choose the most plausible continuation among four choices.</p><p>When fine-tuning on the SWAG dataset, we construct four input sequences, each containing the concatenation of the given sentence (sentence A) and a possible continuation (sentence B). The only task-specific parameters introduced is a vector whose dot product with the [CLS] token representation C denotes a score for each choice which is normalized with a softmax layer.</p><p>We fine-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16. Results are presented in Table <ref target=\"#tab_9\" type=\"table\">4</ref>. BERT LARGE outperforms the authors' baseline ESIM+ELMo system by +27.1% and OpenAI GPT by 8.3%.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5\">Ablation Studies</head><p>In this section, we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance. Additional  ablation studies can be found in Appendix C.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">Effect of Pre-training Tasks</head><p>We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data, fine-tuning scheme, and hyperparameters as BERT BASE :</p><p>No NSP: A bidirectional model which is trained using the \"masked LM\" (MLM) but without the \"next sentence prediction\" (NSP) task.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>LTR &amp; No NSP:</head><p>A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input representation, and our fine-tuning scheme.</p><p>We first examine the impact brought by the NSP task. In Table <ref target=\"#tab_11\" type=\"table\">5</ref>, we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by comparing \"No NSP\" to \"LTR &amp; No NSP\". The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.</p><p>For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no rightside context. In order to make a good faith attempt at strengthening the LTR system, we added a randomly initialized BiLSTM on top. This does significantly improve results on SQuAD, but the results are still far worse than those of the pretrained bidirectional models. The BiLSTM hurts performance on the GLUE tasks.</p><p>We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models, as ELMo does. However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.2\">Effect of Model Size</head><p>In this section, we explore the effect of model size on fine-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training procedure as described previously.</p><p>Results on selected GLUE tasks are shown in Table <ref target=\"#tab_13\" type=\"table\">6</ref>. In this table, we report the average Dev Set accuracy from 5 random restarts of fine-tuning. We can see that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks. It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature. For example, the largest Transformer explored in <ref target=\"#b45\" type=\"bibr\">Vaswani et al. (2017)</ref> is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters <ref target=\"#b1\" type=\"bibr\">(Al-Rfou et al., 2018)</ref>. By contrast, BERT BASE contains 110M parameters and BERT LARGE contains 340M parameters.</p><p>It has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table <ref target=\"#tab_13\" type=\"table\">6</ref>. However, we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained. <ref target=\"#b36\" type=\"bibr\">Peters et al. (2018b)</ref> presented mixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to four layers and <ref target=\"#b29\" type=\"bibr\">Melamud et al. (2016)</ref> mentioned in passing that increasing hidden dimension size from 200 to 600 helped, but increasing further to 1,000 did not bring further improvements. Both of these prior works used a featurebased approach -we hypothesize that when the model is fine-tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters, the taskspecific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.3\">Feature-based Approach with BERT</head><p>All of the BERT results presented so far have used the fine-tuning approach, where a simple classification layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a downstream task. However, the feature-based approach, where fixed features are extracted from the pretrained model, has certain advantages. First, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.</p><p>In this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim <ref target=\"#b43\" type=\"bibr\">Sang and De Meulder, 2003)</ref>. In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we formulate this as a tagging task but do not use a CRF   layer in the output. We use the representation of the first sub-token as the input to the token-level classifier over the NER label set.</p><p>To ablate the fine-tuning approach, we apply the feature-based approach by extracting the activations from one or more layers without fine-tuning any parameters of BERT. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer.</p><p>Results are presented in Table <ref target=\"#tab_14\" type=\"table\">7</ref>. BERT LARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both finetuning and feature-based approaches.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6\">Conclusion</head><p>Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks. A Additional Details for BERT</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A.1 Illustration of the Pre-training Tasks</head><p>We provide examples of the pre-training tasks in the following.</p><p>Masked LM and the Masking Procedure Assuming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further illustrated by</p><p>â€¢ 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy â†’ my dog is [MASK]</p><p>â€¢ 10% of the time: Replace the word with a random word, e.g., my dog is hairy â†’ my dog is apple</p><p>â€¢ 10% of the time: Keep the word unchanged, e.g., my dog is hairy â†’ my dog is hairy. The purpose of this is to bias the representation towards the actual observed word.</p><p>The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words, so it is forced to keep a distributional contextual representation of every input token. Additionally, because random replacement only occurs for 1.5% of all tokens (i.e., 10% of 15%), this does not seem to harm the model's language understanding capability. In Section C.2, we evaluate the impact this procedure.</p><p>Compared to standard langauge model training, the masked LM only make predictions on 15% of tokens in each batch, which suggests that more pre-training steps may be required for the model </p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A.2 Pre-training Procedure</head><p>To generate each training input sequence, we sample two spans of text from the corpus, which we refer to as \"sentences\" even though they are typically much longer than single sentences (but can be shorter also). The first sentence receives the A embedding and the second receives the B embedding. 50% of the time B is the actual next sentence that follows A and 50% of the time it is a random sentence, which is done for the \"next sentence prediction\" task. They are sampled such that the combined length is â‰¤ 512 tokens. The LM masking is applied after WordPiece tokenization with a uniform masking rate of 15%, and no special consideration given to partial word pieces. We train with batch size of 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40 epochs over the 3.3 billion word corpus. We use Adam with learning rate of 1e-4, Î² 1 = 0.9, Î² 2 = 0.999, L2 weight decay of 0.01, learning rate warmup over the first 10,000 steps, and linear decay of the learning rate. We use a dropout probability of 0.1 on all layers. We use a gelu activation <ref target=\"#b18\" type=\"bibr\">(Hendrycks and Gimpel, 2016)</ref> rather than the standard relu, following OpenAI GPT. The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.</p><p>Training of BERT BASE was performed on 4 Cloud TPUs in Pod configuration (16 TPU chips total). <ref type=\"bibr\">13</ref> Training of BERT LARGE was performed on 16 Cloud TPUs (64 TPU chips total). Each pretraining took 4 days to complete.</p><p>Longer sequences are disproportionately expensive because attention is quadratic to the sequence length. To speed up pretraing in our experiments, we pre-train the model with sequence length of 128 for 90% of the steps. Then, we train the rest 10% of the steps of sequence of 512 to learn the positional embeddings.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A.3 Fine-tuning Procedure</head><p>For fine-tuning, most model hyperparameters are the same as in pre-training, with the exception of the batch size, learning rate, and number of training epochs. The dropout probability was always kept at 0.1. The optimal hyperparameter values are task-specific, but we found the following range of possible values to work well across all tasks:</p><p>â€¢ Batch size: 16, 32 We also observed that large data sets (e.g., 100k+ labeled training examples) were far less sensitive to hyperparameter choice than small data sets. Fine-tuning is typically very fast, so it is reasonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set.</p><p>A.4 Comparison of <ref type=\"bibr\">BERT, ELMo ,and</ref> OpenAI GPT</p><p>Here we studies the differences in recent popular representation learning models including ELMo, OpenAI GPT and BERT. The comparisons between the model architectures are shown visually in Figure <ref type=\"figure\">3</ref>. Note that in addition to the architecture differences, BERT and OpenAI GPT are finetuning approaches, while ELMo is a feature-based approach.</p><p>The most comparable existing pre-training method to BERT is OpenAI GPT, which trains a left-to-right Transformer LM on a large text corpus. In fact, many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared. The core argument of this work is that the bi-directionality and the two pretraining tasks presented in Section 3.1 account for the majority of the empirical improvements, but we do note that there are several other differences between how BERT and GPT were trained:</p><p>â€¢ GPT is trained on the BooksCorpus (800M words); BERT is trained on the BooksCorpus (800M words) and Wikipedia (2,500M words).</p><p>â€¢ GPT uses a sentence separator ( â€¢ GPT was trained for 1M steps with a batch size of 32,000 words; BERT was trained for 1M steps with a batch size of 128,000 words.</p><p>â€¢ GPT used the same learning rate of 5e-5 for all fine-tuning experiments; BERT chooses a task-specific fine-tuning learning rate which performs the best on the development set.</p><p>To isolate the effect of these differences, we perform ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A.5 Illustrations of Fine-tuning on Different Tasks</head><p>The illustration of fine-tuning BERT on different tasks can be seen in Figure <ref type=\"figure\">4</ref>. Our task-specific models are formed by incorporating BERT with one additional output layer, so a minimal number of parameters need to be learned from scratch. Among the tasks, Our GLUE results in Table1 are obtained from https://gluebenchmark.com/ leaderboard and https://blog. openai.com/language-unsupervised. The GLUE benchmark includes the following datasets, the descriptions of which were originally summarized in <ref target=\"#b47\" type=\"bibr\">Wang et al. (2018a)</ref>:</p><p>MNLI Multi-Genre Natural Language Inference is a large-scale, crowdsourced entailment classification task <ref target=\"#b50\" type=\"bibr\">(Williams et al., 2018)</ref>. Given a pair of sentences, the goal is to predict whether the second sentence is an entailment, contradiction, or neutral with respect to the first one.</p><p>QQP Quora Question Pairs is a binary classification task where the goal is to determine if two questions asked on Quora are semantically equivalent . QNLI Question Natural Language Inference is a version of the Stanford Question Answering Dataset <ref target=\"#b38\" type=\"bibr\">(Rajpurkar et al., 2016)</ref> which has been converted to a binary classification task <ref target=\"#b47\" type=\"bibr\">(Wang et al., 2018a</ref> ... ... </p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>SST-2</head><p>The Stanford Sentiment Treebank is a binary single-sentence classification task consisting of sentences extracted from movie reviews with human annotations of their sentiment <ref target=\"#b40\" type=\"bibr\">(Socher et al., 2013)</ref>.</p><p>CoLA The Corpus of Linguistic Acceptability is a binary single-sentence classification task, where the goal is to predict whether an English sentence is linguistically \"acceptable\" or not <ref target=\"#b49\" type=\"bibr\">(Warstadt et al., 2018)</ref>.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>STS-B</head><p>The Semantic Textual Similarity Benchmark is a collection of sentence pairs drawn from news headlines and other sources <ref target=\"#b7\" type=\"bibr\">(Cer et al., 2017)</ref>. They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning.</p><p>MRPC Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent <ref target=\"#b16\" type=\"bibr\">(Dolan and Brockett, 2005)</ref>.</p><p>RTE Recognizing Textual Entailment is a binary entailment task similar to MNLI, but with much less training data <ref target=\"#b3\" type=\"bibr\">(Bentivogli et al., 2009</ref>). 14 WNLI Winograd NLI is a small natural language inference dataset <ref target=\"#b26\" type=\"bibr\">(Levesque et al., 2011)</ref>.</p><p>The GLUE webpage notes that there are issues with the construction of this dataset, 15 and every trained system that's been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class. We therefore exclude this set to be fair to OpenAI GPT. For our GLUE submission, we always predicted the ma-jority class. </p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>C.2 Ablation for Different Masking Procedures</head><p>In Section 3.1, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective. The following is an ablation study to evaluate the effect of different masking strategies. Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and fine-tuning, as the [MASK] symbol never appears during the fine-tuning stage. We report the Dev results for both MNLI and NER. For NER, we report both fine-tuning and feature-based approaches, as we expect the mismatch will be amplified for the feature-based approach as the model will not have the chance to adjust the representations.  The results are presented in Table <ref target=\"#tab_21\" type=\"table\">8</ref>. In the table, MASK means that we replace the target token with the [MASK] symbol for MLM; SAME means that we keep the target token as is; RND means that we replace the target token with another random token.</p><p>The numbers in the left part of the table represent the probabilities of the specific strategies used during MLM pre-training (BERT uses 80%, 10%, 10%). The right part of the paper represents the Dev set results. For the feature-based approach, we concatenate the last 4 layers of BERT as the features, which was shown to be the best approach in Section 5.3.</p><p>From the table it can be seen that fine-tuning is surprisingly robust to different masking strategies. However, as expected, using only the MASK strategy was problematic when applying the featurebased approach to NER. Interestingly, using only the RND strategy performs much worse than our strategy as well.</p></div><figure xml:id=\"fig_0\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings.</figDesc></figure>\n",
       " <figure xml:id=\"fig_1\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>-</head><label/><figDesc>Effect of Number of Training Steps; and -Ablation for Different Masking Procedures.</figDesc></figure>\n",
       " <figure xml:id=\"fig_2\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>13 https://cloudplatform.googleblog.com/2018/06/Cloud-TPU-now-offers-preemptible-pricing-and-globalavailability.html â€¢ Learning rate (Adam): 5e-5, 3e-5, 2e-5 â€¢ Number of epochs: 2, 3, 4</figDesc></figure>\n",
       " <figure xml:id=\"fig_3\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>[SEP]) and classifier token ([CLS]) which are only introduced at fine-tuning time; BERT learns [SEP], [CLS] and sentence A/B embeddings during pre-training.</figDesc></figure>\n",
       " <figure xml:id=\"fig_4\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>(a) and (b) are sequence-level tasks while (c) and (d) are token-level tasks. In the figure, E represents the input embedding, T i represents the contextual representation of token i, [CLS] is the special symbol for classification output, and [SEP] is the special symbol to separate non-consecutive token sequences. B Detailed Experimental Setup B.1 Detailed Descriptions for the GLUE Benchmark Experiments.</figDesc></figure>\n",
       " <figure xml:id=\"fig_5\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Ablation over number of training steps. This shows the MNLI accuracy after fine-tuning, starting from model parameters that have been pre-trained for k steps. The x-axis is the value of k.</figDesc></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_1\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architectures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During fine-tuning, all parameters are fine-tuned.[CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating questions/answers).</figDesc><table><row><cell>NSP</cell><cell/><cell>Mask LM</cell><cell/><cell/><cell cols=\"2\">Mask LM</cell><cell>MNLI</cell><cell>NER</cell><cell>SQuAD</cell></row><row><cell/><cell/><cell/><cell>T N</cell><cell/><cell>T 1 '</cell><cell>...</cell><cell>T M '</cell></row><row><cell>[CLS]</cell><cell>Tok 1</cell><cell>...</cell><cell>Tok N</cell><cell>[SEP]</cell><cell>Tok 1</cell><cell>...</cell><cell>TokM</cell></row><row><cell cols=\"4\">Masked Sentence A</cell><cell/><cell cols=\"3\">Masked Sentence B</cell></row><row><cell/><cell/><cell cols=\"5\">Unlabeled Sentence A and B Pair</cell><cell/><cell>Question Answer Pair</cell></row><row><cell/><cell/><cell cols=\"4\">Pre-training</cell><cell/><cell/><cell>Fine-Tuning</cell></row><row><cell>Figure 1:</cell><cell/><cell/><cell/><cell/><cell/><cell/><cell/></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_4\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 1 :</head><label>1</label><figDesc>GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard). The number below each task denotes the number of training examples. The \"Average\" column is slightly different than the official GLUE score, since we exclude the problematic WNLI set.8  BERT and OpenAI GPT are singlemodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.</figDesc><table/></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_6\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 2 :</head><label>2</label><figDesc>SQuAD 1.1 results. The BERT ensemble is 7x systems which use different pre-training checkpoints and fine-tuning seeds.</figDesc><table><row><cell>System</cell><cell>Dev</cell><cell>Test</cell></row><row><cell/><cell cols=\"2\">EM F1 EM F1</cell></row><row><cell cols=\"3\">Top Leaderboard Systems (Dec 10th, 2018)</cell></row><row><cell>Human</cell><cell cols=\"2\">86.3 89.0 86.9 89.5</cell></row><row><cell cols=\"2\">#1 Single -MIR-MRC (F-Net) -</cell><cell>-74.8 78.0</cell></row><row><cell>#2 Single -nlnet</cell><cell>-</cell><cell>-74.2 77.1</cell></row><row><cell cols=\"2\">Published</cell><cell/></row><row><cell>unet (Ensemble)</cell><cell>-</cell><cell>-71.4 74.9</cell></row><row><cell>SLQA+ (Single)</cell><cell>-</cell><cell>71.4 74.4</cell></row><row><cell>Ours</cell><cell/><cell/></row><row><cell>BERTLARGE (Single)</cell><cell cols=\"2\">78.7 81.9 80.0 83.1</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_7\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 3 :</head><label>3</label><figDesc/><table/><note>SQuAD 2.0 results. We exclude entries that use BERT as one of their components.</note></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_9\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 4 :</head><label>4</label><figDesc>SWAG Dev and Test accuracies. â€  Human performance is measured with 100 samples, as reported in the SWAG paper.</figDesc><table/></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_11\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 5</head><label>5</label><figDesc/><table><row><cell>: Ablation over the pre-training tasks using the</cell></row><row><cell>BERT BASE architecture. \"No NSP\" is trained without</cell></row><row><cell>the next sentence prediction task. \"LTR &amp; No NSP\" is</cell></row><row><cell>trained as a left-to-right LM without the next sentence</cell></row><row><cell>prediction, like OpenAI GPT. \"+ BiLSTM\" adds a ran-</cell></row><row><cell>domly initialized BiLSTM on top of the \"LTR + No</cell></row><row><cell>NSP\" model during fine-tuning.</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_13\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 6</head><label>6</label><figDesc/><table><row><cell>: Ablation over BERT model size. #L = the</cell></row><row><cell>number of layers; #H = hidden size; #A = number of at-</cell></row><row><cell>tention heads. \"LM (ppl)\" is the masked LM perplexity</cell></row><row><cell>of held-out training data.</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_14\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 7</head><label>7</label><figDesc/><table><row><cell>: CoNLL-2003 Named Entity Recognition re-</cell></row><row><cell>sults. Hyperparameters were selected using the Dev</cell></row><row><cell>set. The reported Dev and Test scores are averaged over</cell></row><row><cell>5 random restarts using those hyperparameters.</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_16\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>). The positive examples are (question, sentence) pairs which do contain the correct answer, and the negative examples are (question, sentence) from the same paragraph which do not contain the answer.</figDesc><table><row><cell>C</cell><cell>T 1</cell><cell/><cell/><cell>T [SEP]</cell><cell/><cell/><cell/></row><row><cell/><cell/><cell/><cell cols=\"2\">BERT</cell><cell/><cell/><cell/></row><row><cell>E [CLS]</cell><cell>E 1</cell><cell>...</cell><cell>E N</cell><cell>E [SEP]</cell><cell>E 1 '</cell><cell>...</cell><cell>E M '</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_19\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>C Additional Ablation Studies C.1 Effect of Number of Training Steps Figure 5 presents MNLI Dev accuracy after finetuning from a checkpoint that has been pre-trained for k steps. This allows us to answer the following The MLM model does converge slightly slower than the LTR model. However, in terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately.</figDesc><table><row><cell>questions:</cell></row><row><cell>1. Question: Does BERT really need such</cell></row><row><cell>a large amount of pre-training (128,000</cell></row><row><cell>words/batch * 1,000,000 steps) to achieve</cell></row><row><cell>high fine-tuning accuracy?</cell></row><row><cell>Answer: Yes, BERT BASE achieves almost</cell></row><row><cell>1.0% additional accuracy on MNLI when</cell></row><row><cell>trained on 1M steps compared to 500k steps.</cell></row><row><cell>2. Question: Does MLM pre-training converge</cell></row><row><cell>slower than LTR pre-training, since only 15%</cell></row><row><cell>of words are predicted in each batch rather</cell></row><row><cell>than every word?</cell></row><row><cell>Answer:</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_21\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 8 :</head><label>8</label><figDesc>Ablation over different masking strategies.</figDesc><table/></figure>\n",
       " <note n=\"1\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">https://github.com/tensorflow/tensor2tensor 2 http://nlp.seas.harvard.edu/2018/04/03/attention.html 3 In all cases we set the feed-forward/filter size to be 4H, i.e., 3072 for the H = 768 and 4096 for the H = 1024.</note>\n",
       " <note n=\"5\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">The final model achieves 97%-98% accuracy on NSP.6  The vector C is not a meaningful sentence representation without fine-tuning, since it was trained with NSP.</note>\n",
       " <note n=\"7\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">For example, the BERT SQuAD model can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%.8 See (10) in https://gluebenchmark.com/faq.</note>\n",
       " <note n=\"11\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">QANet is described in<ref target=\"#b53\" type=\"bibr\">Yu et al. (2018)</ref>, but the system has improved substantially after publication.</note>\n",
       " <note n=\"14\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">Note that we only report single-task fine-tuning results in this paper. A multitask fine-tuning approach could potentially push the performance even further. For example, we did observe substantial improvements on RTE from multitask training with MNLI.15 https://gluebenchmark.com/faq</note>\n",
       " </body>\n",
       " <back>\n",
       " <div type=\"annex\">\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Appendix for \"BERT: Pre-training of Deep Bidirectional Transformers for</head><p>Language Understanding\"</p><p>We organize the appendix into three sections:</p><p>â€¢ Additional implementation details for BERT are presented in Appendix A;</p><p>â€¢ Additional details for our experiments are presented in Appendix B; and</p><p>â€¢ Additional ablation studies are presented in Appendix C.</p><p>We present additional ablation studies for BERT including:</p></div> </div>\n",
       " <div type=\"references\">\n",
       " <listBibl>\n",
       " <biblStruct xml:id=\"b0\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Contextual string embeddings for sequence labeling</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alan</forename><surname>Akbik</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Duncan</forename><surname>Blythe</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Roland</forename><surname>Vollgraf</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 27th International Conference on Computational Linguistics</title>\n",
       " <meeting>the 27th International Conference on Computational Linguistics</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " <biblScope from=\"1638\" to=\"1649\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b1\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Character-level language modeling with deeper self-attention</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rami</forename><surname>Al-Rfou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dokook</forename><surname>Choe</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noah</forename><surname>Constant</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mandy</forename><surname>Guo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Llion</forename><surname>Jones</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1808.04444</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b2\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A framework for learning predictive structures from multiple tasks and unlabeled data</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rie</forename><surname>Kubota</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ando</forename></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tong</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Journal of Machine Learning Research</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">6</biblScope>\n",
       " <biblScope from=\"1817\" to=\"1853\" unit=\"page\"/>\n",
       " <date type=\"published\" when=\"2005-11\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b3\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">The fifth PASCAL recognizing textual entailment challenge</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luisa</forename><surname>Bentivogli</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bernardo</forename><surname>Magnini</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ido</forename><surname>Dagan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hoa</forename><forename type=\"middle\">Trang</forename><surname>Dang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Danilo</forename><surname>Giampiccolo</surname></persName>\n",
       " </author>\n",
       " <editor>TAC. NIST</editor>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2009\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b4\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Domain adaptation with structural correspondence learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">John</forename><surname>Blitzer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ryan</forename><surname>Mcdonald</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Fernando</forename><surname>Pereira</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 2006 conference on empirical methods in natural language processing</title>\n",
       " <meeting>the 2006 conference on empirical methods in natural language processing</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2006\"/>\n",
       " <biblScope from=\"120\" to=\"128\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>Association for Computational Linguistics</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b5\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A large annotated corpus for learning natural language inference</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Samuel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Gabor</forename><surname>Bowman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><surname>Angeli</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><forename type=\"middle\">D</forename><surname>Potts</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Manning</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">EMNLP. Association for Computational Linguistics</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b6\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Class-based n-gram models of natural language</title>\n",
       " <author>\n",
       " <persName><surname>Peter F Brown</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><surname>Peter</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Desouza</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Robert</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Vincent J Della</forename><surname>Mercer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jenifer C</forename><surname>Pietra</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Lai</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Computational linguistics</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">18</biblScope>\n",
       " <biblScope unit=\"issue\">4</biblScope>\n",
       " <biblScope from=\"467\" to=\"479\" unit=\"page\"/>\n",
       " <date type=\"published\" when=\"1992\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b7\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Daniel</forename><surname>Cer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mona</forename><surname>Diab</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Eneko</forename><surname>Agirre</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Inigo</forename><surname>Lopez-Gazpio</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lucia</forename><surname>Specia</surname></persName>\n",
       " </author>\n",
       " <idno type=\"DOI\">10.18653/v1/S17-2001</idno>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>\n",
       " <meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)</meeting>\n",
       " <imprint>\n",
       " <publisher>Vancouver</publisher>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " <biblScope from=\"1\" to=\"14\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b8\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">One billion word benchmark for measuring progress in statistical language modeling</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ciprian</forename><surname>Chelba</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tomas</forename><surname>Mikolov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mike</forename><surname>Schuster</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Qi</forename><surname>Ge</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1312.3005</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2013\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " <note>Thorsten Brants, Phillipp Koehn, and Tony Robinson</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b9\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Quora question pairs</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Z</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">X</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Zhao</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b10\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Simple and effective multi-paragraph reading comprehension</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><surname>Clark</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matt</forename><surname>Gardner</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b11\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Semi-supervised sequence modeling with cross-view training</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kevin</forename><surname>Clark</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Minh-Thang</forename><surname>Luong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><forename type=\"middle\">D</forename><surname>Manning</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Quoc</forename><surname>Le</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>\n",
       " <meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " <biblScope from=\"1914\" to=\"1925\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b12\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ronan</forename><surname>Collobert</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jason</forename><surname>Weston</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 25th international conference on Machine learning</title>\n",
       " <meeting>the 25th international conference on Machine learning</meeting>\n",
       " <imprint>\n",
       " <publisher>ACM</publisher>\n",
       " <date type=\"published\" when=\"2008\"/>\n",
       " <biblScope from=\"160\" to=\"167\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b13\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Supervised learning of universal sentence representations from natural language inference data</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alexis</forename><surname>Conneau</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Douwe</forename><surname>Kiela</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Holger</forename><surname>Schwenk</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">LoÃ¯c</forename><surname>Barrault</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Antoine</forename><surname>Bordes</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>\n",
       " <meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>\n",
       " <imprint>\n",
       " <publisher>Association for Computational Linguistics</publisher>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " <biblScope from=\"670\" to=\"680\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b14\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Semi-supervised sequence learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Andrew</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Quoc V</forename><surname>Dai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Le</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in neural information processing systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " <biblScope from=\"3079\" to=\"3087\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b15\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">ImageNet: A Large-Scale Hierarchical Image Database</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Deng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><surname>Dong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Socher</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L.-J</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Fei-Fei</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">CVPR09</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2009\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b16\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Automatically constructing a corpus of sentential paraphrases</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">B</forename><surname>William</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chris</forename><surname>Dolan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Brockett</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</title>\n",
       " <meeting>the Third International Workshop on Paraphrasing (IWP2005)</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2005\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b17\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">William</forename><surname>Fedus</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ian</forename><surname>Goodfellow</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrew M</forename><surname>Dai</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1801.07736</idno>\n",
       " <title level=\"m\">Maskgan: Better text generation via filling in the</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b18\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Bridging nonlinearities and stochastic regularizers with gaussian error linear units</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dan</forename><surname>Hendrycks</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kevin</forename><surname>Gimpel</surname></persName>\n",
       " </author>\n",
       " <idno>abs/1606.08415</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b19\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Learning distributed representations of sentences from unlabelled data</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Felix</forename><surname>Hill</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kyunghyun</forename><surname>Cho</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Anna</forename><surname>Korhonen</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>\n",
       " <meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b20\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Universal language model fine-tuning for text classification</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeremy</forename><surname>Howard</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sebastian</forename><surname>Ruder</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ACL</title>\n",
       " <imprint>\n",
       " <publisher>Association for Computational Linguistics</publisher>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b21\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Reinforced mnemonic reader for machine reading comprehension</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Minghao</forename><surname>Hu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yuxing</forename><surname>Peng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhen</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xipeng</forename><surname>Qiu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Furu</forename><surname>Wei</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ming</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">IJCAI</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b22\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Discourse-based objectives for fast unsupervised sentence representation learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yacine</forename><surname>Jernite</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samuel</forename><forename type=\"middle\">R</forename><surname>Bowman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">David</forename><surname>Sontag</surname></persName>\n",
       " </author>\n",
       " <idno>abs/1705.00557</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b23\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mandar</forename><surname>Joshi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Eunsol</forename><surname>Choi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Daniel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luke</forename><surname>Weld</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Zettlemoyer</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b24\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Skip-thought vectors</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ryan</forename><surname>Kiros</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yukun</forename><surname>Zhu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Ruslan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Richard</forename><surname>Salakhutdinov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Raquel</forename><surname>Zemel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Antonio</forename><surname>Urtasun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sanja</forename><surname>Torralba</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Fidler</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in neural information processing systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " <biblScope from=\"3294\" to=\"3302\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b25\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Distributed representations of sentences and documents</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Quoc</forename><surname>Le</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tomas</forename><surname>Mikolov</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">International Conference on Machine Learning</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " <biblScope from=\"1188\" to=\"1196\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b26\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">The winograd schema challenge</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Hector</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ernest</forename><surname>Levesque</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Leora</forename><surname>Davis</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Morgenstern</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Aaai spring symposium: Logical formalizations of commonsense reasoning</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2011\"/>\n",
       " <biblScope unit=\"volume\">46</biblScope>\n",
       " <biblScope unit=\"page\">47</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b27\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">An efficient framework for learning sentence representations</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lajanugen</forename><surname>Logeswaran</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Honglak</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">International Conference on Learning Representations</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b28\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Learned in translation: Contextualized word vectors</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bryan</forename><surname>Mccann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">James</forename><surname>Bradbury</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Caiming</forename><surname>Xiong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Richard</forename><surname>Socher</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NIPS</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b29\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">context2vec: Learning generic context embedding with bidirectional LSTM</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oren</forename><surname>Melamud</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jacob</forename><surname>Goldberger</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ido</forename><surname>Dagan</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>In CoNLL</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b30\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Distributed representations of words and phrases and their compositionality</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tomas</forename><surname>Mikolov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ilya</forename><surname>Sutskever</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kai</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Greg</forename><forename type=\"middle\">S</forename><surname>Corrado</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeff</forename><surname>Dean</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in Neural Information Processing Systems</title>\n",
       " <imprint>\n",
       " <publisher>Curran Associates, Inc</publisher>\n",
       " <date type=\"published\" when=\"2013\"/>\n",
       " <biblScope unit=\"volume\">26</biblScope>\n",
       " <biblScope from=\"3111\" to=\"3119\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b31\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A scalable hierarchical distributed language model</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andriy</forename><surname>Mnih</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Geoffrey</forename><forename type=\"middle\">E</forename><surname>Hinton</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in Neural Information Processing Systems</title>\n",
       " <editor>\n",
       " <persName><forename type=\"first\">D</forename><surname>Koller</surname></persName>\n",
       " <persName><forename type=\"first\">D</forename><surname>Schuurmans</surname></persName>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Bengio</surname></persName>\n",
       " <persName><forename type=\"first\">L</forename><surname>Bottou</surname></persName>\n",
       " </editor>\n",
       " <imprint>\n",
       " <publisher>Curran Associates, Inc</publisher>\n",
       " <date type=\"published\" when=\"2009\"/>\n",
       " <biblScope unit=\"volume\">21</biblScope>\n",
       " <biblScope from=\"1081\" to=\"1088\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b32\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A decomposable attention model for natural language inference</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Ankur</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oscar</forename><surname>Parikh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dipanjan</forename><surname>TÃ¤ckstrÃ¶m</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jakob</forename><surname>Das</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Uszkoreit</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">EMNLP</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b33\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Glove: Global vectors for word representation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeffrey</forename><surname>Pennington</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Richard</forename><surname>Socher</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><forename type=\"middle\">D</forename><surname>Manning</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Empirical Methods in Natural Language Processing (EMNLP)</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " <biblScope from=\"1532\" to=\"1543\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b34\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Semi-supervised sequence tagging with bidirectional language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matthew</forename><surname>Peters</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Waleed</forename><surname>Ammar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chandra</forename><surname>Bhagavatula</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Russell</forename><surname>Power</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b35\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Deep contextualized word representations</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matthew</forename><surname>Peters</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mark</forename><surname>Neumann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mohit</forename><surname>Iyyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matt</forename><surname>Gardner</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><surname>Clark</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NAACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b36\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Dissecting contextual word embeddings: Architecture and representation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matthew</forename><surname>Peters</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mark</forename><surname>Neumann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wen-Tau</forename><surname>Yih</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>\n",
       " <meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " <biblScope from=\"1499\" to=\"1509\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b37\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Improving language understanding with unsupervised learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alec</forename><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Karthik</forename><surname>Narasimhan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tim</forename><surname>Salimans</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ilya</forename><surname>Sutskever</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " <pubPlace>OpenAI</pubPlace>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">Technical report</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b38\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Squad: 100,000+ questions for machine comprehension of text</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pranav</forename><surname>Rajpurkar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jian</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Konstantin</forename><surname>Lopyrev</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Percy</forename><surname>Liang</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>\n",
       " <meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " <biblScope from=\"2383\" to=\"2392\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b39\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Bidirectional attention flow for machine comprehension</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Minjoon</forename><surname>Seo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Aniruddha</forename><surname>Kembhavi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ali</forename><surname>Farhadi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hannaneh</forename><surname>Hajishirzi</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>In ICLR</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b40\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Recursive deep models for semantic compositionality over a sentiment treebank</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Richard</forename><surname>Socher</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Perelygin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jean</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jason</forename><surname>Chuang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Christopher</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrew</forename><surname>Manning</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><surname>Ng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Potts</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 2013 conference on empirical methods in natural language processing</title>\n",
       " <meeting>the 2013 conference on empirical methods in natural language processing</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2013\"/>\n",
       " <biblScope from=\"1631\" to=\"1642\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b41\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Fu</forename><surname>Sun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Linyang</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xipeng</forename><surname>Qiu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yang</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1810.06638</idno>\n",
       " <title level=\"m\">Machine reading comprehension with unanswerable questions</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b42\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Cloze procedure: A new tool for measuring readability</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Wilson</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Taylor</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Journalism Bulletin</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">30</biblScope>\n",
       " <biblScope unit=\"issue\">4</biblScope>\n",
       " <biblScope from=\"415\" to=\"433\" unit=\"page\"/>\n",
       " <date type=\"published\" when=\"1953\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b43\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Erik F Tjong Kim</forename><surname>Sang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Fien</forename><surname>De Meulder</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2003\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>In CoNLL</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b44\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Word representations: A simple and general method for semi-supervised learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Joseph</forename><surname>Turian</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lev</forename><surname>Ratinov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yoshua</forename><surname>Bengio</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL '10</title>\n",
       " <meeting>the 48th Annual Meeting of the Association for Computational Linguistics, ACL '10</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2010\"/>\n",
       " <biblScope from=\"384\" to=\"394\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b45\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Attention is all you need</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ashish</forename><surname>Vaswani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noam</forename><surname>Shazeer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Niki</forename><surname>Parmar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jakob</forename><surname>Uszkoreit</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Llion</forename><surname>Jones</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Aidan</forename><forename type=\"middle\">N</forename><surname>Gomez</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lukasz</forename><surname>Kaiser</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Illia</forename><surname>Polosukhin</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in Neural Information Processing Systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " <biblScope from=\"6000\" to=\"6010\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b46\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Extracting and composing robust features with denoising autoencoders</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pascal</forename><surname>Vincent</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hugo</forename><surname>Larochelle</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yoshua</forename><surname>Bengio</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pierre-Antoine</forename><surname>Manzagol</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 25th international conference on Machine learning</title>\n",
       " <meeting>the 25th international conference on Machine learning</meeting>\n",
       " <imprint>\n",
       " <publisher>ACM</publisher>\n",
       " <date type=\"published\" when=\"2008\"/>\n",
       " <biblScope from=\"1096\" to=\"1103\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b47\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Amanpreet</forename><surname>Singh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Julian</forename><surname>Michael</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Felix</forename><surname>Hill</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Omer</forename><surname>Levy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samuel</forename><surname>Bowman</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>\n",
       " <meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " <biblScope from=\"353\" to=\"355\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b48\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Multigranularity hierarchical attention fusion networks for reading comprehension and question answering</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wei</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ming</forename><surname>Yan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chen</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>\n",
       " <meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " <biblScope unit=\"volume\">1</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b49\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Warstadt</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Amanpreet</forename><surname>Singh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samuel R</forename><surname>Bowman</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1805.12471</idno>\n",
       " <title level=\"m\">Neural network acceptability judgments</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b50\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A broad-coverage challenge corpus for sentence understanding through inference</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Adina</forename><surname>Williams</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nikita</forename><surname>Nangia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samuel R</forename><surname>Bowman</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NAACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b51\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Google's neural machine translation system: Bridging the gap between human and machine translation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yonghui</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mike</forename><surname>Schuster</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhifeng</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><surname>Quoc</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mohammad</forename><surname>Le</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wolfgang</forename><surname>Norouzi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Maxim</forename><surname>Macherey</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yuan</forename><surname>Krikun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Qin</forename><surname>Cao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Klaus</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Macherey</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1609.08144</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">arXiv preprint</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b52\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">How transferable are features in deep neural networks?</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jason</forename><surname>Yosinski</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeff</forename><surname>Clune</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yoshua</forename><surname>Bengio</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hod</forename><surname>Lipson</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in neural information processing systems</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " <biblScope from=\"3320\" to=\"3328\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b53\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">QANet: Combining local convolution with global self-attention for reading comprehension</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Adams</forename><forename type=\"middle\">Wei</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">David</forename><surname>Dohan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Minh-Thang</forename><surname>Luong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rui</forename><surname>Zhao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kai</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mohammad</forename><surname>Norouzi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Quoc V</forename><surname>Le</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ICLR</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b54\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Swag: A large-scale adversarial dataset for grounded commonsense inference</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rowan</forename><surname>Zellers</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yonatan</forename><surname>Bisk</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Roy</forename><surname>Schwartz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yejin</forename><surname>Choi</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>\n",
       " <meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>\n",
       " <imprint>\n",
       " <publisher>EMNLP</publisher>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b55\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yukun</forename><surname>Zhu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ryan</forename><surname>Kiros</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rich</forename><surname>Zemel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ruslan</forename><surname>Salakhutdinov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Raquel</forename><surname>Urtasun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Antonio</forename><surname>Torralba</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sanja</forename><surname>Fidler</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the IEEE international conference on computer vision</title>\n",
       " <meeting>the IEEE international conference on computer vision</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " <biblScope from=\"19\" to=\"27\" unit=\"page\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " </listBibl>\n",
       " </div>\n",
       " </back>\n",
       " </text>\n",
       " </TEI>,\n",
       " <?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       " <TEI xml:space=\"preserve\" xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd\">\n",
       " <teiHeader xml:lang=\"en\">\n",
       " <fileDesc>\n",
       " <titleStmt>\n",
       " <title level=\"a\" type=\"main\">Efficient Estimation of Word Representations in Vector Space</title>\n",
       " </titleStmt>\n",
       " <publicationStmt>\n",
       " <publisher/>\n",
       " <availability status=\"unknown\"><licence/></availability>\n",
       " <date type=\"published\" when=\"2013-09-07\">7 Sep 2013</date>\n",
       " </publicationStmt>\n",
       " <sourceDesc>\n",
       " <biblStruct>\n",
       " <analytic>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tomas</forename><surname>Mikolov</surname></persName>\n",
       " <email>tmikolov@google.com</email>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Google Inc</orgName>\n",
       " <address>\n",
       " <settlement>Mountain View</settlement>\n",
       " <region>CA</region>\n",
       " </address>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kai</forename><surname>Chen</surname></persName>\n",
       " <email>kaichen@google.com</email>\n",
       " <affiliation key=\"aff1\">\n",
       " <orgName type=\"institution\">Google Inc</orgName>\n",
       " <address>\n",
       " <settlement>Mountain View</settlement>\n",
       " <region>CA</region>\n",
       " </address>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Greg</forename><surname>Corrado</surname></persName>\n",
       " <email>gcorrado@google.com</email>\n",
       " <affiliation key=\"aff2\">\n",
       " <orgName type=\"institution\">Google Inc</orgName>\n",
       " <address>\n",
       " <settlement>Mountain View</settlement>\n",
       " <region>CA</region>\n",
       " </address>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeffrey</forename><surname>Dean</surname></persName>\n",
       " <affiliation key=\"aff3\">\n",
       " <orgName type=\"institution\">Google Inc</orgName>\n",
       " <address>\n",
       " <settlement>Mountain View</settlement>\n",
       " <region>CA</region>\n",
       " </address>\n",
       " </affiliation>\n",
       " </author>\n",
       " <title level=\"a\" type=\"main\">Efficient Estimation of Word Representations in Vector Space</title>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2013-09-07\">7 Sep 2013</date>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <idno type=\"arXiv\">arXiv:1301.3781v3[cs.CL]</idno>\n",
       " </biblStruct>\n",
       " </sourceDesc>\n",
       " </fileDesc>\n",
       " <encodingDesc>\n",
       " <appInfo>\n",
       " <application ident=\"GROBID\" version=\"0.7.0-SNAPSHOT\" when=\"2024-04-27T16:15+0000\">\n",
       " <desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>\n",
       " <ref target=\"https://github.com/kermitt2/grobid\"/>\n",
       " </application>\n",
       " </appInfo>\n",
       " </encodingDesc>\n",
       " <profileDesc>\n",
       " <abstract>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p></div>\n",
       " </abstract>\n",
       " </profileDesc>\n",
       " </teiHeader>\n",
       " <text xml:lang=\"en\">\n",
       " <body>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>Many current NLP systems and techniques treat words as atomic units -there is no notion of similarity between words, as these are represented as indices in a vocabulary. This choice has several good reasons -simplicity, robustness and the observation that simple models trained on huge amounts of data outperform complex systems trained on less data. An example is the popular N-gram model used for statistical language modeling -today, it is possible to train N-grams on virtually all available data (trillions of words <ref target=\"#b2\" type=\"bibr\">[3]</ref>).</p><p>However, the simple techniques are at their limits in many tasks. For example, the amount of relevant in-domain data for automatic speech recognition is limited -the performance is usually dominated by the size of high quality transcribed speech data (often just millions of words). In machine translation, the existing corpora for many languages contain only a few billions of words or less. Thus, there are situations where simple scaling up of the basic techniques will not result in any significant progress, and we have to focus on more advanced techniques.</p><p>With progress of machine learning techniques in recent years, it has become possible to train more complex models on much larger data set, and they typically outperform the simple models. Probably the most successful concept is to use distributed representations of words <ref target=\"#b9\" type=\"bibr\">[10]</ref>. For example, neural network based language models significantly outperform N-gram models <ref target=\"#b0\" type=\"bibr\">[1,</ref><ref target=\"#b26\" type=\"bibr\">27,</ref><ref target=\"#b16\" type=\"bibr\">17]</ref>.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1.1\">Goals of the Paper</head><p>The main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary. As far as we know, none of the previously proposed architectures has been successfully trained on more than a few hundred of millions of words, with a modest dimensionality of the word vectors between 50 -100.</p><p>We use recently proposed techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity <ref target=\"#b19\" type=\"bibr\">[20]</ref>. This has been observed earlier in the context of inflectional languages -for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings <ref target=\"#b12\" type=\"bibr\">[13,</ref><ref target=\"#b13\" type=\"bibr\">14]</ref>. Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector(\"King\") -vector(\"Man\") + vector(\"Woman\") results in a vector that is closest to the vector representation of the word Queen <ref target=\"#b19\" type=\"bibr\">[20]</ref>.</p><p>In this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words. We design a new comprehensive test set for measuring both syntactic and semantic regularities <ref target=\"#b0\" type=\"bibr\">1</ref> , and show that many such regularities can be learned with high accuracy. Moreover, we discuss how training time and accuracy depends on the dimensionality of the word vectors and on the amount of the training data.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1.2\">Previous Work</head><p>Representation of words as continuous vectors has a long history <ref target=\"#b9\" type=\"bibr\">[10,</ref><ref target=\"#b25\" type=\"bibr\">26,</ref><ref target=\"#b7\" type=\"bibr\">8]</ref>. A very popular model architecture for estimating neural network language model (NNLM) was proposed in <ref target=\"#b0\" type=\"bibr\">[1]</ref>, where a feedforward neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the word vector representation and a statistical language model. This work has been followed by many others.</p><p>Another interesting architecture of NNLM was presented in <ref target=\"#b12\" type=\"bibr\">[13,</ref><ref target=\"#b13\" type=\"bibr\">14]</ref>, where the word vectors are first learned using neural network with a single hidden layer. The word vectors are then used to train the NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this work, we directly extend this architecture, and focus just on the first step where the word vectors are learned using a simple model.</p><p>It was later shown that the word vectors can be used to significantly improve and simplify many NLP applications <ref target=\"#b3\" type=\"bibr\">[4,</ref><ref target=\"#b4\" type=\"bibr\">5,</ref><ref target=\"#b28\" type=\"bibr\">29]</ref>. Estimation of the word vectors itself was performed using different model architectures and trained on various corpora <ref target=\"#b3\" type=\"bibr\">[4,</ref><ref target=\"#b28\" type=\"bibr\">29,</ref><ref target=\"#b22\" type=\"bibr\">23,</ref><ref target=\"#b18\" type=\"bibr\">19,</ref><ref target=\"#b8\" type=\"bibr\">9]</ref>, and some of the resulting word vectors were made available for future research and comparison 2 . However, as far as we know, these architectures were significantly more computationally expensive for training than the one proposed in <ref target=\"#b12\" type=\"bibr\">[13]</ref>, with the exception of certain version of log-bilinear model where diagonal weight matrices are used <ref target=\"#b22\" type=\"bibr\">[23]</ref>.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">Model Architectures</head><p>Many different types of models were proposed for estimating continuous representations of words, including the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). In this paper, we focus on distributed representations of words learned by neural networks, as it was previously shown that they perform significantly better than LSA for preserving linear regularities among words <ref target=\"#b19\" type=\"bibr\">[20,</ref><ref target=\"#b30\" type=\"bibr\">31]</ref>; LDA moreover becomes computationally very expensive on large data sets.</p><p>Similar to <ref target=\"#b17\" type=\"bibr\">[18]</ref>, to compare different model architectures we define first the computational complexity of a model as the number of parameters that need to be accessed to fully train the model. Next, we will try to maximize the accuracy, while minimizing the computational complexity.</p><p>For all the following models, the training complexity is proportional to</p><formula xml:id=\"formula_0\">O = E Ã— T Ã— Q,<label>(1)</label></formula><p>where E is number of the training epochs, T is the number of the words in the training set and Q is defined further for each model architecture. Common choice is E = 3 âˆ’ 50 and T up to one billion. All models are trained using stochastic gradient descent and backpropagation <ref target=\"#b25\" type=\"bibr\">[26]</ref>.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.1\">Feedforward Neural Net Language Model (NNLM)</head><p>The probabilistic feedforward neural network language model has been proposed in <ref target=\"#b0\" type=\"bibr\">[1]</ref>. It consists of input, projection, hidden and output layers. At the input layer, N previous words are encoded using 1-of-V coding, where V is size of the vocabulary. The input layer is then projected to a projection layer P that has dimensionality N Ã— D, using a shared projection matrix. As only N inputs are active at any given time, composition of the projection layer is a relatively cheap operation.</p><p>The NNLM architecture becomes complex for computation between the projection and the hidden layer, as values in the projection layer are dense. For a common choice of N = 10, the size of the projection layer (P ) might be 500 to 2000, while the hidden layer size H is typically 500 to 1000 units. Moreover, the hidden layer is used to compute probability distribution over all the words in the vocabulary, resulting in an output layer with dimensionality V . Thus, the computational complexity per each training example is</p><formula xml:id=\"formula_1\">Q = N Ã— D + N Ã— D Ã— H + H Ã— V,<label>(2)</label></formula><p>where the dominating term is H Ã— V . However, several practical solutions were proposed for avoiding it; either using hierarchical versions of the softmax <ref target=\"#b24\" type=\"bibr\">[25,</ref><ref target=\"#b22\" type=\"bibr\">23,</ref><ref target=\"#b17\" type=\"bibr\">18]</ref>, or avoiding normalized models completely by using models that are not normalized during training <ref target=\"#b3\" type=\"bibr\">[4,</ref><ref target=\"#b8\" type=\"bibr\">9]</ref>. With binary tree representations of the vocabulary, the number of output units that need to be evaluated can go down to around log 2 (V ). Thus, most of the complexity is caused by the term N Ã— D Ã— H.</p><p>In our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary tree. This follows previous observations that the frequency of words works well for obtaining classes in neural net language models <ref target=\"#b15\" type=\"bibr\">[16]</ref>. Huffman trees assign short binary codes to frequent words, and this further reduces the number of output units that need to be evaluated: while balanced binary tree would require log 2 (V ) outputs to be evaluated, the Huffman tree based hierarchical softmax requires only about log 2 (U nigram perplexity(V )). For example when the vocabulary size is one million words, this results in about two times speedup in evaluation. While this is not crucial speedup for neural network LMs as the computational bottleneck is in the N Ã—D Ã—H term, we will later propose architectures that do not have hidden layers and thus depend heavily on the efficiency of the softmax normalization.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.2\">Recurrent Neural Net Language Model (RNNLM)</head><p>Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM, such as the need to specify the context length (the order of the model N ), and because theoretically RNNs can efficiently represent more complex patterns than the shallow neural networks <ref target=\"#b14\" type=\"bibr\">[15,</ref><ref target=\"#b1\" type=\"bibr\">2]</ref>. The RNN model does not have a projection layer; only input, hidden and output layer. What is special for this type of model is the recurrent matrix that connects hidden layer to itself, using time-delayed connections. This allows the recurrent model to form some kind of short term memory, as information from the past can be represented by the hidden layer state that gets updated based on the current input and the state of the hidden layer in the previous time step.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>The complexity per training example of the RNN model is</head><formula xml:id=\"formula_2\">Q = H Ã— H + H Ã— V,<label>(3)</label></formula><p>where the word representations D have the same dimensionality as the hidden layer H. Again, the term H Ã— V can be efficiently reduced to H Ã— log 2 (V ) by using hierarchical softmax. Most of the complexity then comes from H Ã— H.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2.3\">Parallel Training of Neural Networks</head><p>To train models on huge data sets, we have implemented several models on top of a large-scale distributed framework called DistBelief <ref target=\"#b5\" type=\"bibr\">[6]</ref>, including the feedforward NNLM and the new models proposed in this paper. The framework allows us to run multiple replicas of the same model in parallel, and each replica synchronizes its gradient updates through a centralized server that keeps all the parameters. For this parallel training, we use mini-batch asynchronous gradient descent with an adaptive learning rate procedure called Adagrad <ref target=\"#b6\" type=\"bibr\">[7]</ref>. Under this framework, it is common to use one hundred or more model replicas, each using many CPU cores at different machines in a data center.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3\">New Log-linear Models</head><p>In this section, we propose two new model architectures for learning distributed representations of words that try to minimize computational complexity. The main observation from the previous section was that most of the complexity is caused by the non-linear hidden layer in the model. While this is what makes neural networks so attractive, we decided to explore simpler models that might not be able to represent the data as precisely as neural networks, but can possibly be trained on much more data efficiently.</p><p>The new architectures directly follow those proposed in our earlier work <ref target=\"#b12\" type=\"bibr\">[13,</ref><ref target=\"#b13\" type=\"bibr\">14]</ref>, where it was found that neural network language model can be successfully trained in two steps: first, continuous word vectors are learned using simple model, and then the N-gram NNLM is trained on top of these distributed representations of words. While there has been later substantial amount of work that focuses on learning word vectors, we consider the approach proposed in <ref target=\"#b12\" type=\"bibr\">[13]</ref> to be the simplest one.</p><p>Note that related models have been proposed also much earlier <ref target=\"#b25\" type=\"bibr\">[26,</ref><ref target=\"#b7\" type=\"bibr\">8]</ref>.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1\">Continuous Bag-of-Words Model</head><p>The first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged). We call this architecture a bag-of-words model as the order of words in the history does not influence the projection. Furthermore, we also use words from the future; we have obtained the best performance on the task introduced in the next section by building a log-linear classifier with four future and four history words at the input, where the training criterion is to correctly classify the current (middle) word.</p><p>Training complexity is then</p><formula xml:id=\"formula_3\">Q = N Ã— D + D Ã— log 2 (V ).<label>(4)</label></formula><p>We denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous distributed representation of the context. The model architecture is shown at Figure <ref type=\"figure\">1</ref>. Note that the weight matrix between the input and the projection layer is shared for all word positions in the same way as in the NNLM.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2\">Continuous Skip-gram Model</head><p>The second architecture is similar to CBOW, but instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence. More precisely, we use each current word as an input to a log-linear classifier with continuous projection layer, and predict words within a certain range before and after the current word. We found that increasing the range improves quality of the resulting word vectors, but it also increases the computational complexity. Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples.</p><p>The training complexity of this architecture is proportional to</p><formula xml:id=\"formula_4\">Q = C Ã— (D + D Ã— log 2 (V )), (<label>5</label></formula><formula xml:id=\"formula_5\">)</formula><p>where C is the maximum distance of the words. Thus, if we choose C = 5, for each training word we will select randomly a number R in range &lt; 1; C &gt;, and then use R words from history and</p><formula xml:id=\"formula_6\">w(t-2) w(t+1) w(t-1) w(t+2) w(t) SUM INPUT PROJECTION OUTPUT w(t) INPUT PROJECTION OUTPUT w(t-2) w(t-1) w(t+1) w(t+2)</formula></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>CBOW</head></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Skip-gram</head><p>Figure <ref type=\"figure\">1</ref>: New model architectures. The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word.</p><p>R words from the future of the current word as correct labels. This will require us to do R Ã— 2 word classifications, with the current word as input, and each of the R + R words as output. In the following experiments, we use C = 10.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4\">Results</head><p>To compare the quality of different versions of word vectors, previous papers typically use a table</p><p>showing example words and their most similar words, and understand them intuitively. Although it is easy to show that word France is similar to Italy and perhaps some other countries, it is much more challenging when subjecting those vectors in a more complex similarity task, as follows. We follow previous observation that there can be many different types of similarities between words, for example, word big is similar to bigger in the same sense that small is similar to smaller. Example of another type of relationship can be word pairs big -biggest and small -smallest <ref target=\"#b19\" type=\"bibr\">[20]</ref>. We further denote two pairs of words with the same relationship as a question, as we can ask: \"What is the word that is similar to small in the same sense as biggest is similar to big?\"</p><p>Somewhat surprisingly, these questions can be answered by performing simple algebraic operations with the vector representation of words. To find a word that is similar to small in the same sense as biggest is similar to big, we can simply compute vector X = vector(\"biggest\") âˆ’ vector(\"big\") + vector(\"small\"). Then, we search in the vector space for the word closest to X measured by cosine distance, and use it as the answer to the question (we discard the input question words during this search). When the word vectors are well trained, it is possible to find the correct answer (word smallest) using this method.</p><p>Finally, we found that when we train high dimensional word vectors on a large amount of data, the resulting vectors can be used to answer very subtle semantic relationships between words, such as a city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors with such semantic relationships could be used to improve many existing NLP applications, such as machine translation, information retrieval and question answering systems, and may enable other future applications yet to be invented. </p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1\">Task Description</head><p>To measure quality of the word vectors, we define a comprehensive test set that contains five types of semantic questions, and nine types of syntactic questions. Two examples from each category are shown in Table <ref target=\"#tab_0\" type=\"table\">1</ref>. Overall, there are 8869 semantic and 10675 syntactic questions. The questions in each category were created in two steps: first, a list of similar word pairs was created manually.</p><p>Then, a large list of questions is formed by connecting two word pairs. For example, we made a list of 68 large American cities and the states they belong to, and formed about 2.5K questions by picking two word pairs at random. We have included in our test set only single token words, thus multi-word entities are not present (such as New York).</p><p>We evaluate the overall accuracy for all question types, and for each question type separately (semantic, syntactic). Question is assumed to be correctly answered only if the closest word to the vector computed using the above method is exactly the same as the correct word in the question; synonyms are thus counted as mistakes. This also means that reaching 100% accuracy is likely to be impossible, as the current models do not have any input information about word morphology. However, we believe that usefulness of the word vectors for certain applications should be positively correlated with this accuracy metric. Further progress can be achieved by incorporating information about structure of words, especially for the syntactic questions.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2\">Maximization of Accuracy</head><p>We have used a Google News corpus for training the word vectors. This corpus contains about 6B tokens. We have restricted the vocabulary size to 1 million most frequent words. Clearly, we are facing time constrained optimization problem, as it can be expected that both using more data and higher dimensional word vectors will improve the accuracy. To estimate the best choice of model architecture for obtaining as good as possible results quickly, we have first evaluated models trained on subsets of the training data, with vocabulary restricted to the most frequent 30k words.</p><p>The results using the CBOW architecture with different choice of word vector dimensionality and increasing amount of the training data are shown in Table <ref type=\"table\">2</ref>.</p><p>It can be seen that after some point, adding more dimensions or adding more training data provides diminishing improvements. So, we have to increase both vector dimensionality and the amount of the training data together. While this observation might seem trivial, it must be noted that it is currently popular to train word vectors on relatively large amounts of data, but with insufficient size (such as 50 -100). Given Equation <ref target=\"#formula_3\" type=\"formula\">4</ref>, increasing amount of training data twice results in about the same increase of computational complexity as increasing vector size twice.</p><p>For the experiments reported in Tables <ref target=\"#tab_2\" type=\"table\">2 and 4</ref>, we used three training epochs with stochastic gradient descent and backpropagation. We chose starting learning rate 0.025 and decreased it linearly, so that it approaches zero at the end of the last training epoch.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.3\">Comparison of Model Architectures</head><p>First we compare different model architectures for deriving the word vectors using the same training data and using the same dimensionality of 640 of the word vectors. In the further experiments, we use full set of questions in the new Semantic-Syntactic Word Relationship test set, i.e. unrestricted to the 30k vocabulary. We also include results on a test set introduced in <ref target=\"#b19\" type=\"bibr\">[20]</ref> that focuses on syntactic similarity between words 3 .</p><p>The training data consists of several LDC corpora and is described in detail in <ref target=\"#b17\" type=\"bibr\">[18]</ref> (320M words, 82K vocabulary). We used these data to provide a comparison to a previously trained recurrent neural network language model that took about 8 weeks to train on a single CPU. We trained a feedforward NNLM with the same number of 640 hidden units using the DistBelief parallel training <ref target=\"#b5\" type=\"bibr\">[6]</ref>, using a history of 8 previous words (thus, the NNLM has more parameters than the RNNLM, as the projection layer has size 640 Ã— 8).</p><p>In Table <ref type=\"table\">3</ref>, it can be seen that the word vectors from the RNN (as used in <ref target=\"#b19\" type=\"bibr\">[20]</ref>) perform well mostly on the syntactic questions. The NNLM vectors perform significantly better than the RNN -this is not surprising, as the word vectors in the RNNLM are directly connected to a non-linear hidden layer. The CBOW architecture works better than the NNLM on the syntactic tasks, and about the same on the semantic one. Finally, the Skip-gram architecture works slightly worse on the syntactic task than the CBOW model (but still better than the NNLM), and much better on the semantic part of the test than all the other models.</p><p>Next, we evaluated our models trained using one CPU only and compared the results against publicly available word vectors. The comparison is given in Table <ref target=\"#tab_2\" type=\"table\">4</ref>. The CBOW model was trained on subset  For experiments reported further, we used just one training epoch (again, we decrease the learning rate linearly so that it approaches zero at the end of training). Training a model on twice as much data using one epoch gives comparable or better results than iterating over the same data for three epochs, as is shown in Table <ref target=\"#tab_3\" type=\"table\">5</ref>, and provides additional small speedup.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.4\">Large Scale Parallel Training of Models</head><p>As mentioned earlier, we have implemented various models in a distributed framework called Dis-tBelief. Below we report the results of several models trained on the Google News 6B data set, with mini-batch asynchronous gradient descent and the adaptive learning rate procedure called Adagrad <ref target=\"#b6\" type=\"bibr\">[7]</ref>. We used 50 to 100 model replicas during the training. The number of CPU cores is an  <ref target=\"#b31\" type=\"bibr\">[32]</ref> 39 Average LSA similarity <ref target=\"#b31\" type=\"bibr\">[32]</ref> 49 Log-bilinear model <ref target=\"#b23\" type=\"bibr\">[24]</ref> 54.8 RNNLMs <ref target=\"#b18\" type=\"bibr\">[19]</ref> 55.4 Skip-gram 48.0 Skip-gram + RNNLMs 58.9</p><p>estimate since the data center machines are shared with other production tasks, and the usage can fluctuate quite a bit. Note that due to the overhead of the distributed framework, the CPU usage of the CBOW model and the Skip-gram model are much closer to each other than their single-machine implementations. The result are reported in Table <ref type=\"table\">6</ref>.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.5\">Microsoft Research Sentence Completion Challenge</head><p>The Microsoft Sentence Completion Challenge has been recently introduced as a task for advancing language modeling and other NLP techniques <ref target=\"#b31\" type=\"bibr\">[32]</ref>. This task consists of 1040 sentences, where one word is missing in each sentence and the goal is to select word that is the most coherent with the rest of the sentence, given a list of five reasonable choices. Performance of several techniques has been already reported on this set, including N-gram models, LSA-based model <ref target=\"#b31\" type=\"bibr\">[32]</ref>, log-bilinear model <ref target=\"#b23\" type=\"bibr\">[24]</ref> and a combination of recurrent neural networks that currently holds the state of the art performance of 55.4% accuracy on this benchmark <ref target=\"#b18\" type=\"bibr\">[19]</ref>.</p><p>We have explored the performance of Skip-gram architecture on this task. First, we train the 640dimensional model on 50M words provided in <ref target=\"#b31\" type=\"bibr\">[32]</ref>. Then, we compute score of each sentence in the test set by using the unknown word at the input, and predict all surrounding words in a sentence. The final sentence score is then the sum of these individual predictions. Using the sentence scores, we choose the most likely sentence.</p><p>A short summary of some previous results together with the new results is presented in Table <ref type=\"table\">7</ref>.</p><p>While the Skip-gram model itself does not perform on this task better than LSA similarity, the scores from this model are complementary to scores obtained with RNNLMs, and a weighted combination leads to a new state of the art result 58.9% accuracy (59.2% on the development part of the set and 58.7% on the test part of the set).</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5\">Examples of the Learned Relationships</head><p>Table <ref target=\"#tab_5\" type=\"table\">8</ref> shows words that follow various relationships. We follow the approach described above: the relationship is defined by subtracting two word vectors, and the result is added to another word. Thus for example, Paris -France + Italy = Rome. As it can be seen, accuracy is quite good, although there is clearly a lot of room for further improvements (note that using our accuracy metric that  <ref target=\"#tab_5\" type=\"table\">8</ref> would score only about 60%). We believe that word vectors trained on even larger data sets with larger dimensionality will perform significantly better, and will enable the development of new innovative applications. Another way to improve accuracy is to provide more than one example of the relationship. By using ten examples instead of one to form the relationship vector (we average the individual vectors together), we have observed improvement of accuracy of our best models by about 10% absolutely on the semantic-syntactic test.</p><p>It is also possible to apply the vector operations to solve different tasks. For example, we have observed good accuracy for selecting out-of-the-list words, by computing average vector for a list of words, and finding the most distant word vector. This is a popular type of problems in certain human intelligence tests. Clearly, there is still a lot of discoveries to be made using these techniques.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6\">Conclusion</head><p>In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks. We observed that it is possible to train high quality word vectors using very simple model architectures, compared to the popular neural network models (both feedforward and recurrent). Because of the much lower computational complexity, it is possible to compute very accurate high dimensional word vectors from a much larger data set.</p><p>Using the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram models even on corpora with one trillion words, for basically unlimited size of the vocabulary. That is several orders of magnitude larger than the best previously published results for similar models.</p><p>An interesting task where the word vectors have recently been shown to significantly outperform the previous state of the art is the SemEval-2012 Task 2 <ref target=\"#b10\" type=\"bibr\">[11]</ref>. The publicly available RNN vectors were used together with other techniques to achieve over 50% increase in Spearman's rank correlation over the previous best result <ref target=\"#b30\" type=\"bibr\">[31]</ref>. The neural network based word vectors were previously applied to many other NLP tasks, for example sentiment analysis <ref target=\"#b11\" type=\"bibr\">[12]</ref> and paraphrase detection <ref target=\"#b27\" type=\"bibr\">[28]</ref>. It can be expected that these applications can benefit from the model architectures described in this paper.</p><p>Our ongoing work shows that the word vectors can be successfully applied to automatic extension of facts in Knowledge Bases, and also for verification of correctness of existing facts. Results from machine translation experiments also look very promising. In the future, it would be also interesting to compare our techniques to Latent Relational Analysis <ref target=\"#b29\" type=\"bibr\">[30]</ref> and others. We believe that our comprehensive test set will help the research community to improve the existing techniques for estimating the word vectors. We also expect that high quality word vectors will become an important building block for future NLP applications.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"7\">Follow-Up Work</head><p>After the initial version of this paper was written, we published single-machine multi-threaded C++ code for computing the word vectors, using both the continuous bag-of-words and skip-gram architectures <ref target=\"#b3\" type=\"bibr\">4</ref> . The training speed is significantly higher than reported earlier in this paper, i.e. it is in the order of billions of words per hour for typical hyperparameter choices. We also published more than 1.4 million vectors that represent named entities, trained on more than 100 billion words. Some of our follow-up work will be published in an upcoming NIPS 2013 paper <ref target=\"#b20\" type=\"bibr\">[21]</ref>.</p></div><figure type=\"table\" xml:id=\"tab_0\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 1 :</head><label>1</label><figDesc>Examples of five types of semantic and nine types of syntactic questions in the Semantic-Syntactic Word Relationship test set.</figDesc><table><row><cell>Type of relationship</cell><cell cols=\"2\">Word Pair 1</cell><cell cols=\"2\">Word Pair 2</cell></row><row><cell>Common capital city</cell><cell>Athens</cell><cell>Greece</cell><cell>Oslo</cell><cell>Norway</cell></row><row><cell>All capital cities</cell><cell>Astana</cell><cell>Kazakhstan</cell><cell>Harare</cell><cell>Zimbabwe</cell></row><row><cell>Currency</cell><cell>Angola</cell><cell>kwanza</cell><cell>Iran</cell><cell>rial</cell></row><row><cell>City-in-state</cell><cell>Chicago</cell><cell>Illinois</cell><cell>Stockton</cell><cell>California</cell></row><row><cell>Man-Woman</cell><cell>brother</cell><cell>sister</cell><cell cols=\"2\">grandson granddaughter</cell></row><row><cell>Adjective to adverb</cell><cell>apparent</cell><cell>apparently</cell><cell>rapid</cell><cell>rapidly</cell></row><row><cell>Opposite</cell><cell>possibly</cell><cell>impossibly</cell><cell>ethical</cell><cell>unethical</cell></row><row><cell>Comparative</cell><cell>great</cell><cell>greater</cell><cell>tough</cell><cell>tougher</cell></row><row><cell>Superlative</cell><cell>easy</cell><cell>easiest</cell><cell>lucky</cell><cell>luckiest</cell></row><row><cell>Present Participle</cell><cell>think</cell><cell>thinking</cell><cell>read</cell><cell>reading</cell></row><row><cell cols=\"2\">Nationality adjective Switzerland</cell><cell>Swiss</cell><cell>Cambodia</cell><cell>Cambodian</cell></row><row><cell>Past tense</cell><cell>walking</cell><cell>walked</cell><cell>swimming</cell><cell>swam</cell></row><row><cell>Plural nouns</cell><cell>mouse</cell><cell>mice</cell><cell>dollar</cell><cell>dollars</cell></row><row><cell>Plural verbs</cell><cell>work</cell><cell>works</cell><cell>speak</cell><cell>speaks</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_1\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 2 :Table 3 :</head><label>23</label><figDesc>Accuracy on subset of the Semantic-Syntactic Word Relationship test set, using word vectors from the CBOW architecture with limited vocabulary. Only questions containing words from the most frequent 30k words are used. Comparison of architectures using models trained on the same data, with 640-dimensional word vectors. The accuracies are reported on our Semantic-Syntactic Word Relationship test set, and on the syntactic relationship test set of<ref target=\"#b19\" type=\"bibr\">[20]</ref> </figDesc><table><row><cell cols=\"6\">Dimensionality / Training words 24M 49M 98M 196M 391M 783M</cell></row><row><cell/><cell>50</cell><cell>13.4 15.7 18.6</cell><cell>19.1</cell><cell>22.5</cell><cell>23.2</cell></row><row><cell/><cell>100</cell><cell>19.4 23.1 27.8</cell><cell>28.7</cell><cell>33.4</cell><cell>32.2</cell></row><row><cell/><cell>300</cell><cell>23.2 29.2 35.3</cell><cell>38.6</cell><cell>43.7</cell><cell>45.9</cell></row><row><cell/><cell>600</cell><cell>24.0 30.1 36.5</cell><cell>40.8</cell><cell>46.6</cell><cell>50.4</cell></row><row><cell>Model</cell><cell cols=\"2\">Semantic-Syntactic Word Relationship test set</cell><cell/><cell cols=\"2\">MSR Word Relatedness</cell></row><row><cell cols=\"4\">Architecture Semantic Accuracy [%] Syntactic Accuracy [%]</cell><cell cols=\"2\">Test Set [20]</cell></row><row><cell>RNNLM</cell><cell>9</cell><cell>36</cell><cell/><cell>35</cell></row><row><cell>NNLM</cell><cell>23</cell><cell>53</cell><cell/><cell>47</cell></row><row><cell>CBOW</cell><cell>24</cell><cell>64</cell><cell/><cell>61</cell></row><row><cell>Skip-gram</cell><cell>55</cell><cell>59</cell><cell/><cell>56</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_2\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 4 :</head><label>4</label><figDesc>Comparison of publicly available word vectors on the Semantic-Syntactic Word Relationship test set, and word vectors from our models. Full vocabularies are used.</figDesc><table><row><cell>Model</cell><cell>Vector</cell><cell>Training</cell><cell/><cell>Accuracy [%]</cell><cell/></row><row><cell/><cell>Dimensionality</cell><cell>words</cell><cell/><cell/><cell/></row><row><cell/><cell/><cell/><cell cols=\"3\">Semantic Syntactic Total</cell></row><row><cell>Collobert-Weston NNLM</cell><cell>50</cell><cell>660M</cell><cell>9.3</cell><cell>12.3</cell><cell>11.0</cell></row><row><cell>Turian NNLM</cell><cell>50</cell><cell>37M</cell><cell>1.4</cell><cell>2.6</cell><cell>2.1</cell></row><row><cell>Turian NNLM</cell><cell>200</cell><cell>37M</cell><cell>1.4</cell><cell>2.2</cell><cell>1.8</cell></row><row><cell>Mnih NNLM</cell><cell>50</cell><cell>37M</cell><cell>1.8</cell><cell>9.1</cell><cell>5.8</cell></row><row><cell>Mnih NNLM</cell><cell>100</cell><cell>37M</cell><cell>3.3</cell><cell>13.2</cell><cell>8.8</cell></row><row><cell>Mikolov RNNLM</cell><cell>80</cell><cell>320M</cell><cell>4.9</cell><cell>18.4</cell><cell>12.7</cell></row><row><cell>Mikolov RNNLM</cell><cell>640</cell><cell>320M</cell><cell>8.6</cell><cell>36.5</cell><cell>24.6</cell></row><row><cell>Huang NNLM</cell><cell>50</cell><cell>990M</cell><cell>13.3</cell><cell>11.6</cell><cell>12.3</cell></row><row><cell>Our NNLM</cell><cell>20</cell><cell>6B</cell><cell>12.9</cell><cell>26.4</cell><cell>20.3</cell></row><row><cell>Our NNLM</cell><cell>50</cell><cell>6B</cell><cell>27.9</cell><cell>55.8</cell><cell>43.2</cell></row><row><cell>Our NNLM</cell><cell>100</cell><cell>6B</cell><cell>34.2</cell><cell>64.5</cell><cell>50.8</cell></row><row><cell>CBOW</cell><cell>300</cell><cell>783M</cell><cell>15.5</cell><cell>53.1</cell><cell>36.1</cell></row><row><cell>Skip-gram</cell><cell>300</cell><cell>783M</cell><cell>50.0</cell><cell>55.9</cell><cell>53.3</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_3\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 5 :</head><label>5</label><figDesc>Comparison of models trained for three epochs on the same data and models trained for one epoch. Accuracy is reported on the full Semantic-Syntactic data set.</figDesc><table><row><cell>Model</cell><cell>Vector</cell><cell>Training</cell><cell/><cell>Accuracy [%]</cell><cell/><cell>Training time</cell></row><row><cell/><cell>Dimensionality</cell><cell>words</cell><cell/><cell/><cell/><cell>[days]</cell></row><row><cell/><cell/><cell/><cell cols=\"3\">Semantic Syntactic Total</cell><cell/></row><row><cell>3 epoch CBOW</cell><cell>300</cell><cell>783M</cell><cell>15.5</cell><cell>53.1</cell><cell>36.1</cell><cell>1</cell></row><row><cell>3 epoch Skip-gram</cell><cell>300</cell><cell>783M</cell><cell>50.0</cell><cell>55.9</cell><cell>53.3</cell><cell>3</cell></row><row><cell>1 epoch CBOW</cell><cell>300</cell><cell>783M</cell><cell>13.8</cell><cell>49.9</cell><cell>33.6</cell><cell>0.3</cell></row><row><cell>1 epoch CBOW</cell><cell>300</cell><cell>1.6B</cell><cell>16.1</cell><cell>52.6</cell><cell>36.1</cell><cell>0.6</cell></row><row><cell>1 epoch CBOW</cell><cell>600</cell><cell>783M</cell><cell>15.4</cell><cell>53.3</cell><cell>36.2</cell><cell>0.7</cell></row><row><cell>1 epoch Skip-gram</cell><cell>300</cell><cell>783M</cell><cell>45.6</cell><cell>52.2</cell><cell>49.2</cell><cell>1</cell></row><row><cell>1 epoch Skip-gram</cell><cell>300</cell><cell>1.6B</cell><cell>52.2</cell><cell>55.1</cell><cell>53.8</cell><cell>2</cell></row><row><cell>1 epoch Skip-gram</cell><cell>600</cell><cell>783M</cell><cell>56.7</cell><cell>54.5</cell><cell>55.5</cell><cell>2.5</cell></row><row><cell cols=\"7\">of the Google News data in about a day, while training time for the Skip-gram model was about three</cell></row><row><cell>days.</cell><cell/><cell/><cell/><cell/><cell/><cell/></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_4\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 6 :Table 7 :</head><label>67</label><figDesc>Comparison of models trained using the DistBelief distributed framework. Note that training of NNLM with 1000-dimensional vectors would take too long to complete. Comparison and combination of models on the Microsoft Sentence Completion Challenge.</figDesc><table><row><cell>Model</cell><cell>Vector</cell><cell>Training</cell><cell/><cell>Accuracy [%]</cell><cell/><cell>Training time</cell></row><row><cell/><cell>Dimensionality</cell><cell>words</cell><cell/><cell/><cell/><cell>[days x CPU cores]</cell></row><row><cell/><cell/><cell/><cell cols=\"3\">Semantic Syntactic Total</cell><cell/></row><row><cell>NNLM</cell><cell>100</cell><cell>6B</cell><cell>34.2</cell><cell>64.5</cell><cell>50.8</cell><cell>14 x 180</cell></row><row><cell>CBOW</cell><cell>1000</cell><cell>6B</cell><cell>57.3</cell><cell>68.9</cell><cell>63.7</cell><cell>2 x 140</cell></row><row><cell>Skip-gram</cell><cell>1000</cell><cell>6B</cell><cell>66.1</cell><cell>65.1</cell><cell>65.6</cell><cell>2.5 x 125</cell></row><row><cell/><cell cols=\"2\">Architecture</cell><cell/><cell cols=\"2\">Accuracy [%]</cell><cell/></row><row><cell/><cell cols=\"2\">4-gram</cell><cell/><cell/><cell/><cell/></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_5\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 8 :</head><label>8</label><figDesc>Examples of the word pair relationships, using the best word vectors from Table4(Skipgram model trained on 783M words with 300 dimensionality).</figDesc><table><row><cell>Relationship</cell><cell>Example 1</cell><cell>Example 2</cell><cell>Example 3</cell></row><row><cell>France -Paris</cell><cell>Italy: Rome</cell><cell>Japan: Tokyo</cell><cell>Florida: Tallahassee</cell></row><row><cell>big -bigger</cell><cell>small: larger</cell><cell>cold: colder</cell><cell>quick: quicker</cell></row><row><cell>Miami -Florida</cell><cell>Baltimore: Maryland</cell><cell>Dallas: Texas</cell><cell>Kona: Hawaii</cell></row><row><cell>Einstein -scientist</cell><cell>Messi: midfielder</cell><cell>Mozart: violinist</cell><cell>Picasso: painter</cell></row><row><cell>Sarkozy -France</cell><cell>Berlusconi: Italy</cell><cell>Merkel: Germany</cell><cell>Koizumi: Japan</cell></row><row><cell>copper -Cu</cell><cell>zinc: Zn</cell><cell>gold: Au</cell><cell>uranium: plutonium</cell></row><row><cell>Berlusconi -Silvio</cell><cell>Sarkozy: Nicolas</cell><cell>Putin: Medvedev</cell><cell>Obama: Barack</cell></row><row><cell>Microsoft -Windows</cell><cell>Google: Android</cell><cell>IBM: Linux</cell><cell>Apple: iPhone</cell></row><row><cell>Microsoft -Ballmer</cell><cell>Google: Yahoo</cell><cell>IBM: McNealy</cell><cell>Apple: Jobs</cell></row><row><cell>Japan -sushi</cell><cell>Germany: bratwurst</cell><cell>France: tapas</cell><cell>USA: pizza</cell></row><row><cell cols=\"2\">assumes exact match, the results in Table</cell><cell/><cell/></row></table></figure>\n",
       " <note n=\"1\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">The test set is available at www.fit.vutbr.cz/Ëœimikolov/rnnlm/word-test.v1.txt 2 http://ronan.collobert.com/senna/ http://metaoptimize.com/projects/wordreprs/ http://www.fit.vutbr.cz/Ëœimikolov/rnnlm/ http://ai.stanford.edu/Ëœehhuang/</note>\n",
       " <note n=\"3\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">We thank Geoff Zweig for providing us the test set.</note>\n",
       " <note n=\"4\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">The code is available at https://code.google.com/p/word2vec/</note>\n",
       " </body>\n",
       " <back>\n",
       " <div type=\"references\">\n",
       " <listBibl>\n",
       " <biblStruct xml:id=\"b0\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A neural probabilistic language model</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Bengio</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Ducharme</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Vincent</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Journal of Machine Learning Research</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">3</biblScope>\n",
       " <biblScope from=\"1137\" to=\"1155\" unit=\"page\"/>\n",
       " <date type=\"published\" when=\"2003\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b1\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Scaling learning algorithms towards AI</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Bengio</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Lecun</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Large-Scale Kernel Machines</title>\n",
       " <imprint>\n",
       " <publisher>MIT Press</publisher>\n",
       " <date type=\"published\" when=\"2007\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b2\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Large language models in machine translation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Brants</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><forename type=\"middle\">C</forename><surname>Popat</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><forename type=\"middle\">J</forename><surname>Och</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Dean</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning</title>\n",
       " <meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2007\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b3\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Collobert</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Weston</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">International Conference on Machine Learning, ICML</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2008\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b4\">\n",
       " <analytic>\n",
       " <title/>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Collobert</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Weston</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Bottou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Karlen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Kavukcuoglu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Kuksa</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">12</biblScope>\n",
       " <biblScope from=\"2493\" to=\"2537\" unit=\"page\"/>\n",
       " <date type=\"published\" when=\"2011\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b5\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Dean</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><forename type=\"middle\">S</forename><surname>Corrado</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Monga</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Devin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Q</forename><forename type=\"middle\">V</forename><surname>Le</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><forename type=\"middle\">Z</forename><surname>Mao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><forename type=\"middle\">A</forename><surname>Ranzato</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Senior</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Tucker</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><forename type=\"middle\">Y</forename><surname>Ng</surname></persName>\n",
       " </author>\n",
       " <title level=\"m\">Large Scale Distributed Deep Networks, NIPS</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2012\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b6\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Adaptive subgradient methods for online learning and stochastic optimization</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><forename type=\"middle\">C</forename><surname>Duchi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">E</forename><surname>Hazan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Singer</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Journal of Machine Learning Research</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2011\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b7\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Finding Structure in Time</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Elman</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Cognitive Science</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">14</biblScope>\n",
       " <biblScope from=\"179\" to=\"211\" unit=\"page\"/>\n",
       " <date type=\"published\" when=\"1990\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b8\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Improving Word Representations via Global Context and Multiple Word Prototypes</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Eric</forename><forename type=\"middle\">H</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Socher</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><forename type=\"middle\">D</forename><surname>Manning</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrew</forename><forename type=\"middle\">Y</forename><surname>Ng</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proc. Association for Computational Linguistics</title>\n",
       " <meeting>Association for Computational Linguistics</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2012\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b9\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Distributed representations. In: Parallel distributed processing: Explorations in the microstructure of cognition</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><forename type=\"middle\">E</forename><surname>Hinton</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><forename type=\"middle\">L</forename><surname>Mcclelland</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><forename type=\"middle\">E</forename><surname>Rumelhart</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Foundations</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">1</biblScope>\n",
       " <date type=\"published\" when=\"1986\"/>\n",
       " <publisher>MIT Press</publisher>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b10\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Semeval-2012 task 2: Measuring degrees of relational similarity</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><forename type=\"middle\">A</forename><surname>Jurgens</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><forename type=\"middle\">M</forename><surname>Mohammad</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><forename type=\"middle\">D</forename><surname>Turney</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><forename type=\"middle\">J</forename><surname>Holyoak</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 6th International Workshop on Semantic Evaluation</title>\n",
       " <meeting>the 6th International Workshop on Semantic Evaluation</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2012\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b11\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Learning word vectors for sentiment analysis</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><forename type=\"middle\">L</forename><surname>Maas</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><forename type=\"middle\">E</forename><surname>Daly</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><forename type=\"middle\">T</forename><surname>Pham</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><forename type=\"middle\">Y</forename><surname>Ng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Potts</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of ACL</title>\n",
       " <meeting>ACL</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2011\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b12\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Language Modeling for Speech Recognition in Czech, Masters thesis, Brno University of Technology</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Mikolov</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2007\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b13\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Neural network based language models for higly inflective languages</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Mikolov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>KopeckÃ½</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Burget</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">O</forename><surname>Glembek</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>ÄŒernockÃ½</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proc. ICASSP</title>\n",
       " <meeting>ICASSP</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2009\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b14\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Recurrent neural network based language model</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Mikolov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>KarafiÃ¡t</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Burget</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>ÄŒernockÃ½</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Khudanpur</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of Interspeech</title>\n",
       " <meeting>Interspeech</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2010\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b15\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Extensions of recurrent neural network language model</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Mikolov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Kombrink</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Burget</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>ÄŒernockÃ½</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Khudanpur</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of ICASSP</title>\n",
       " <meeting>ICASSP</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2011\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b16\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Empirical Evaluation and Combination of Advanced Language Modeling Techniques</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Mikolov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Deoras</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">S</forename><surname>Kombrink</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Burget</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>ÄŒernockÃ½</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of Interspeech</title>\n",
       " <meeting>Interspeech</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2011\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b17\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Strategies for Training Large Scale Neural Network Language Models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Mikolov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Deoras</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><surname>Povey</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Burget</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>ÄŒernockÃ½</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proc. Automatic Speech Recognition and Understanding</title>\n",
       " <meeting>Automatic Speech Recognition and Understanding</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2011\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b18\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Statistical Language Models based on Neural Networks</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Mikolov</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2012\"/>\n",
       " </imprint>\n",
       " <respStmt>\n",
       " <orgName>Brno University of Technology</orgName>\n",
       " </respStmt>\n",
       " </monogr>\n",
       " <note type=\"report_type\">PhD thesis</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b19\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Mikolov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><forename type=\"middle\">T</forename><surname>Yih</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Zweig</surname></persName>\n",
       " </author>\n",
       " <title level=\"m\">Linguistic Regularities in Continuous Space Word Representations. NAACL HLT</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2013\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b20\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Distributed Representations of Words and Phrases and their Compositionality</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Mikolov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">I</forename><surname>Sutskever</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">K</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Corrado</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Dean</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2013\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>Accepted to NIPS</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b21\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Three new graphical models for statistical language modelling. ICML</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Mnih</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Hinton</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2007\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b22\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A Scalable Hierarchical Distributed Language Model</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Mnih</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Hinton</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Advances in Neural Information Processing Systems</title>\n",
       " <imprint>\n",
       " <publisher>MIT Press</publisher>\n",
       " <date type=\"published\" when=\"2009\"/>\n",
       " <biblScope unit=\"volume\">21</biblScope>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b23\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">A fast and simple algorithm for training neural probabilistic language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Mnih</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><forename type=\"middle\">W</forename><surname>Teh</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2012\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">ICML</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b24\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Hierarchical Probabilistic Neural Network Language Model</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">F</forename><surname>Morin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Bengio</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2005\"/>\n",
       " <publisher>AISTATS</publisher>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b25\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Learning internal representations by backpropagating errors</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">D</forename><forename type=\"middle\">E</forename><surname>Rumelhart</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><forename type=\"middle\">E</forename><surname>Hinton</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><forename type=\"middle\">J</forename><surname>Williams</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Nature</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">323</biblScope>\n",
       " <biblScope from=\"533\" to=\"536\" unit=\"page\"/>\n",
       " <date type=\"published\" when=\"1986\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b26\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Continuous space language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Schwenk</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Computer Speech and Language</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">21</biblScope>\n",
       " <date type=\"published\" when=\"2007\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b27\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">R</forename><surname>Socher</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">E</forename><forename type=\"middle\">H</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Pennington</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><forename type=\"middle\">Y</forename><surname>Ng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><forename type=\"middle\">D</forename><surname>Manning</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NIPS</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2011\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b28\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Word Representations: A Simple and General Method for Semi-Supervised Learning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">J</forename><surname>Turian</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">L</forename><surname>Ratinov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><surname>Bengio</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proc. Association for Computational Linguistics</title>\n",
       " <meeting>Association for Computational Linguistics</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2010\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b29\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Measuring Semantic Similarity by Latent Relational Analysis</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><forename type=\"middle\">D</forename><surname>Turney</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proc. International Joint Conference on Artificial Intelligence</title>\n",
       " <meeting>International Joint Conference on Artificial Intelligence</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2005\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b30\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Combining Heterogeneous Models for Measuring Relational Similarity</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><surname>Zhila</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">W</forename><forename type=\"middle\">T</forename><surname>Yih</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Meek</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Zweig</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">T</forename><surname>Mikolov</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2013\"/>\n",
       " <publisher>NAACL HLT</publisher>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b31\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">The Microsoft Research Sentence Completion Challenge</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">G</forename><surname>Zweig</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><forename type=\"middle\">J C</forename><surname>Burges</surname></persName>\n",
       " </author>\n",
       " <idno>MSR-TR-2011-129</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2011\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">Microsoft Research Technical Report</note>\n",
       " </biblStruct>\n",
       " </listBibl>\n",
       " </div>\n",
       " </back>\n",
       " </text>\n",
       " </TEI>,\n",
       " <?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       " <TEI xml:space=\"preserve\" xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd\">\n",
       " <teiHeader xml:lang=\"en\">\n",
       " <fileDesc>\n",
       " <titleStmt>\n",
       " <title level=\"a\" type=\"main\">Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>\n",
       " </titleStmt>\n",
       " <publicationStmt>\n",
       " <publisher>Association for Computational Linguistics</publisher>\n",
       " <availability status=\"unknown\"><p>Copyright Association for Computational Linguistics</p>\n",
       " </availability>\n",
       " <date type=\"published\" when=\"2020-05-05\">5 May 2020</date>\n",
       " </publicationStmt>\n",
       " <sourceDesc>\n",
       " <biblStruct>\n",
       " <analytic>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Suchin</forename><surname>Gururangan</surname></persName>\n",
       " <email>suching@allenai.org</email>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       " <address>\n",
       " <settlement>Seattle</settlement>\n",
       " <region>WA</region>\n",
       " <country key=\"US\">USA</country>\n",
       " </address>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ana</forename><surname>MarasoviÄ‡</surname></persName>\n",
       " <email>anam@allenai.org</email>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       " <address>\n",
       " <settlement>Seattle</settlement>\n",
       " <region>WA</region>\n",
       " <country key=\"US\">USA</country>\n",
       " </address>\n",
       " </affiliation>\n",
       " <affiliation key=\"aff1\">\n",
       " <orgName type=\"department\">Paul G</orgName>\n",
       " <orgName key=\"instit1\" type=\"institution\">Allen School of Computer Science &amp; Engineering</orgName>\n",
       " <orgName key=\"instit2\" type=\"institution\">University of Washington</orgName>\n",
       " <address>\n",
       " <settlement>Seattle</settlement>\n",
       " <region>WA</region>\n",
       " <country key=\"US\">USA</country>\n",
       " </address>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Swabha</forename><surname>Swayamdipta</surname></persName>\n",
       " <email>swabhas@allenai.org</email>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       " <address>\n",
       " <settlement>Seattle</settlement>\n",
       " <region>WA</region>\n",
       " <country key=\"US\">USA</country>\n",
       " </address>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kyle</forename><surname>Lo</surname></persName>\n",
       " <email>kylel@allenai.org</email>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       " <address>\n",
       " <settlement>Seattle</settlement>\n",
       " <region>WA</region>\n",
       " <country key=\"US\">USA</country>\n",
       " </address>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Iz</forename><surname>Beltagy</surname></persName>\n",
       " <email>beltagy@allenai.org</email>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       " <address>\n",
       " <settlement>Seattle</settlement>\n",
       " <region>WA</region>\n",
       " <country key=\"US\">USA</country>\n",
       " </address>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Doug</forename><surname>Downey</surname></persName>\n",
       " <email>dougd@allenai.org</email>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       " <address>\n",
       " <settlement>Seattle</settlement>\n",
       " <region>WA</region>\n",
       " <country key=\"US\">USA</country>\n",
       " </address>\n",
       " </affiliation>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noah</forename><forename type=\"middle\">A</forename><surname>Smith</surname></persName>\n",
       " <affiliation key=\"aff0\">\n",
       " <orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       " <address>\n",
       " <settlement>Seattle</settlement>\n",
       " <region>WA</region>\n",
       " <country key=\"US\">USA</country>\n",
       " </address>\n",
       " </affiliation>\n",
       " <affiliation key=\"aff1\">\n",
       " <orgName type=\"department\">Paul G</orgName>\n",
       " <orgName key=\"instit1\" type=\"institution\">Allen School of Computer Science &amp; Engineering</orgName>\n",
       " <orgName key=\"instit2\" type=\"institution\">University of Washington</orgName>\n",
       " <address>\n",
       " <settlement>Seattle</settlement>\n",
       " <region>WA</region>\n",
       " <country key=\"US\">USA</country>\n",
       " </address>\n",
       " </affiliation>\n",
       " </author>\n",
       " <title level=\"a\" type=\"main\">Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>\n",
       " <meeting>the 58th Annual Meeting of the Association for Computational Linguistics\t\t\t\t\t\t</meeting>\n",
       " <imprint>\n",
       " <publisher>Association for Computational Linguistics</publisher>\n",
       " <date type=\"published\" when=\"2020-05-05\">5 May 2020</date>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <idno type=\"DOI\">10.18653/v1/2020.acl-main.740</idno>\n",
       " <idno type=\"arXiv\">arXiv:2004.10964v3[cs.CL]</idno>\n",
       " </biblStruct>\n",
       " </sourceDesc>\n",
       " </fileDesc>\n",
       " <encodingDesc>\n",
       " <appInfo>\n",
       " <application ident=\"GROBID\" version=\"0.7.0-SNAPSHOT\" when=\"2024-04-27T16:15+0000\">\n",
       " <desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>\n",
       " <ref target=\"https://github.com/kermitt2/grobid\"/>\n",
       " </application>\n",
       " </appInfo>\n",
       " </encodingDesc>\n",
       " <profileDesc>\n",
       " <abstract>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.</p></div>\n",
       " </abstract>\n",
       " </profileDesc>\n",
       " </teiHeader>\n",
       " <text xml:lang=\"en\">\n",
       " <body>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\">Introduction</head><p>Today's pretrained language models are trained on massive, heterogeneous corpora <ref target=\"#b47\" type=\"bibr\">(Raffel et al., 2019;</ref><ref target=\"#b65\" type=\"bibr\">Yang et al., 2019)</ref>. For instance, ROBERTA  was trained on over 160GB of uncompressed text, with sources ranging from Englishlanguage encyclopedic and news articles, to literary works and web content. Representations learned by such models achieve strong performance across many tasks with datasets of varying sizes drawn from a variety of sources (e.g., <ref target=\"#b58\" type=\"bibr\">Wang et al., 2018</ref>. This leads us to ask whether a tasks textual domain-a term typically used to denote a distribution over language characterizing a given topic or genre (such as \"science\" or \"mystery novels\")-is still relevant. Do the latest large pretrained models work universally or is it still helpful to build Figure <ref type=\"figure\">1</ref>: An illustration of data distributions. Task data is comprised of an observable task distribution, usually non-randomly sampled from a wider distribution (light grey ellipsis) within an even larger target domain, which is not necessarily one of the domains included in the original LM pretraining domain -though overlap is possible. We explore the benefits of continued pretraining on data from the task distribution and the domain distribution.</p><p>separate pretrained models for specific domains?</p><p>While some studies have shown the benefit of continued pretraining on domain-specific unlabeled data (e.g., , these studies only consider a single domain at a time and use a language model that is pretrained on a smaller and less diverse corpus than the most recent language models. Moreover, it is not known how the benefit of continued pretraining may vary with factors like the amount of available labeled task data, or the proximity of the target domain to the original pretraining corpus (see Figure <ref type=\"figure\">1</ref>).</p><p>We address this question for one such highperforming model, ROBERTA ) ( Â§2). We consider four domains (biomedical and computer science publications, news, and reviews; Â§3) and eight classification tasks (two in each domain). For targets that are not already in-domain for ROBERTA, our experiments show that contin-ued pretraining on the domain (which we refer to as domain-adaptive pretraining or DAPT) consistently improves performance on tasks from the target domain, in both high-and low-resource settings.</p><p>Above, we consider domains defined around genres and forums, but it is also possible to induce a domain from a given corpus used for a task, such as the one used in supervised training of a model. This raises the question of whether pretraining on a corpus more directly tied to the task can further improve performance. We study how domainadaptive pretraining compares to task-adaptive pretraining, or TAPT, on a smaller but directly taskrelevant corpus: the unlabeled task dataset ( Â§4), drawn from the task distribution. Task-adaptive pretraining has been shown effective <ref target=\"#b21\" type=\"bibr\">(Howard and Ruder, 2018)</ref>, but is not typically used with the most recent models. We find that TAPT provides a large performance boost for ROBERTA, with or without domain-adaptive pretraining.</p><p>Finally, we show that the benefits from taskadaptive pretraining increase when we have additional unlabeled data from the task distribution that has been manually curated by task designers or annotators. Inspired by this success, we propose ways to automatically select additional task-relevant unlabeled text, and show how this improves performance in certain low-resource cases ( Â§5). On all tasks, our results using adaptive pretraining techniques are competitive with the state of the art.</p><p>In summary, our contributions include:</p><p>â€¢ a thorough analysis of domain-and taskadaptive pretraining across four domains and eight tasks, spanning low-and high-resource settings; â€¢ an investigation into the transferability of adapted LMs across domains and tasks; and â€¢ a study highlighting the importance of pretraining on human-curated datasets, and a simple data selection strategy to automatically approach this performance. Our code as well as pretrained models for multiple domains and tasks are publicly available. 1</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\">Background: Pretraining</head><p>Learning for most NLP research systems since 2018 consists of training in two stages. First, a neural language model (LM), often with millions of parameters, is trained on large unlabeled cor-pora. The word (or wordpiece; <ref target=\"#b62\" type=\"bibr\">Wu et al. 2016)</ref> representations learned in the pretrained model are then reused in supervised training for a downstream task, with optional updates (fine-tuning) of the representations and network from the first stage.</p><p>One such pretrained LM is ROBERTA , which uses the same transformerbased architecture <ref target=\"#b56\" type=\"bibr\">(Vaswani et al., 2017)</ref> as its predecessor, BERT . It is trained with a masked language modeling objective (i.e., cross-entropy loss on predicting randomly masked tokens). The unlabeled pretraining corpus for ROBERTA contains over 160 GB of uncompressed raw text from different English-language corpora (see Appendix Â§A.1). ROBERTA attains better performance on an assortment of tasks than its predecessors, making it our baseline of choice.</p><p>Although ROBERTA's pretraining corpus is derived from multiple sources, it has not yet been established if these sources are diverse enough to generalize to most of the variation in the English language. In other words, we would like to understand what is out of ROBERTA's domain. Towards this end, we explore further adaptation by continued pretraining of this large LM into two categories of unlabeled data: (i) large corpora of domain-specific text ( Â§3), and (ii) available unlabeled data associated with a given task ( Â§4).</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3\">Domain-Adaptive Pretraining</head><p>Our approach to domain-adaptive pretraining (DAPT) is straightforward-we continue pretraining ROBERTA on a large corpus of unlabeled domain-specific text. The four domains we focus on are biomedical (BIOMED) papers, computer science (CS) papers, newstext from REALNEWS, and AMAZON reviews. We choose these domains because they have been popular in previous work, and datasets for text classification are available in each. Table <ref target=\"#tab_1\" type=\"table\">1</ref> lists the specifics of the unlabeled datasets in all four domains, as well as ROBERTA's training corpus. 1</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1\">Analyzing Domain Similarity</head><p>Before performing DAPT, we attempt to quantify the similarity of the target domain to ROBERTA's pretraining domain. We consider domain vocabularies containing the top 10K most frequent unigrams (excluding stopwords) in comparably sized   <ref target=\"#b4\" type=\"bibr\">Beltagy et al. (2020)</ref>. For RCT, we represent all sentences in one long sequence for simultaneous prediction.</p><p>Baseline As our baseline, we use an off-the-shelf ROBERTA-base model and perform supervised fine-tuning of its parameters for each classification task. On average, ROBERTA is not drastically behind the state of the art (details in Appendix Â§A.2), and serves as a good baseline since it provides a single LM to adapt to different domains.</p><p>Classification Architecture Following standard practice  we pass the final layer [CLS] token representation to a task-specific feedforward layer for prediction (see Table <ref target=\"#tab_1\" type=\"table\">14</ref>  Table <ref type=\"table\">2</ref>: Specifications of the various target task datasets. â€  indicates high-resource settings. Sources: CHEMPROT <ref target=\"#b30\" type=\"bibr\">(Kringelum et al., 2016)</ref>, RCT <ref target=\"#b11\" type=\"bibr\">(Dernoncourt and Lee, 2017)</ref>, ACL-ARC <ref target=\"#b24\" type=\"bibr\">(Jurgens et al., 2018)</ref>, SCIERC <ref target=\"#b36\" type=\"bibr\">(Luan et al., 2018)</ref>, HYPERPARTISAN <ref target=\"#b26\" type=\"bibr\">(Kiesel et al., 2019)</ref>, AGNEWS <ref target=\"#b68\" type=\"bibr\">(Zhang et al., 2015)</ref>, HELPFULNESS <ref target=\"#b38\" type=\"bibr\">(McAuley et al., 2015)</ref>, IMDB <ref target=\"#b37\" type=\"bibr\">(Maas et al., 2011</ref>  ments over ROBERTA, demonstrating the benefit of DAPT when the target domain is more distant from ROBERTA's source domain. The pattern is consistent across high-and low-resource settings. Although DAPT does not increase performance on AGNEWS, the benefit we observe in HYPERPAR-TISAN suggests that DAPT may be useful even for tasks that align more closely with ROBERTA's source domain.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.3\">Domain Relevance for DAPT</head><p>Additionally, we compare DAPT against a setting where for each task, we adapt the LM to a domain outside the domain of interest. This controls for the case in which the improvements over ROBERTA might be attributed simply to exposure to more data, regardless of the domain. In this setting, for NEWS, we use a CS LM; for REVIEWS, a BIOMED LM; for CS, a NEWS LM; for BIOMED, a REVIEWS LM. We use the vocabulary overlap statistics in Figure <ref type=\"figure\">2</ref> to guide these choices.</p><p>Our results are shown in Table <ref target=\"#tab_4\" type=\"table\">3</ref>, where the last column (Â¬DAPT) corresponds to this setting. For each task, DAPT significantly outperforms adapting to an irrelevant domain, suggesting the importance of pretraining on domain-relevant data. Furthermore, we generally observe that Â¬DAPT results in worse performance than even ROBERTA on end-tasks. Taken together, these results indicate that in most settings, exposure to more data without considering domain relevance is detrimental to end-task performance. However, there are two tasks (SCIERC and ACL-ARC) in which Â¬DAPT marginally improves performance over ROBERTA. This may suggest that in some cases, continued pretraining on any additional data is useful, as noted in <ref target=\"#b2\" type=\"bibr\">Baevski et al. (2019)</ref>.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.4\">Domain Overlap</head><p>Our analysis of DAPT is based on prior intuitions about how task data is assigned to specific domains. For instance, to perform DAPT for HELPFULNESS, we only adapt to AMAZON reviews, but not to any REALNEWS articles. However, the gradations in Figure <ref type=\"figure\">2</ref> suggest that the boundaries between domains are in some sense fuzzy; for example, 40% of unigrams are shared between REVIEWS and NEWS. As further indication of this overlap, we also qualitatively identify documents that overlap cross-domain: in Table <ref target=\"#tab_6\" type=\"table\">4</ref>, we showcase reviews and REALNEWS articles that are similar to these reviews (other examples can be found in Appendix Â§D). In fact, we find that adapting ROBERTA to IMDB review</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>REALNEWS article</head><p>The Shop Around the Corner is one of the great films from director Ernst Lubitsch . In addition to the talents of James Stewart and Margaret Sullavan , it's filled with a terrific cast of top character actors such as Frank Morgan and Felix Bressart. <ref type=\"bibr\">[...]</ref> The makers of You've Got Mail claim their film to be a remake , but that's just nothing but a lot of inflated self praise. Anyway, if you have an affection for romantic comedies of the 1940 's, you'll find The Shop Around the Corner to be nothing short of wonderful. Just as good with repeat viewings.  Although this analysis is by no means comprehensive, it indicates that the factors that give rise to observable domain differences are likely not mutually exclusive. It is possible that pretraining beyond conventional domain boundaries could result in more effective DAPT; we leave this investigation to future work. In general, the provenance of data, including the processes by which corpora are curated, must be kept in mind when designing pretraining procedures and creating new benchmarks that test out-of-domain generalization abilities.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4\">Task-Adaptive Pretraining</head><p>Datasets curated to capture specific tasks of interest tend to cover only a subset of the text available within the broader domain. For example, the CHEMPROT dataset for extracting relations between chemicals and proteins focuses on abstracts of recently-published, high-impact articles from hand-selected PubMed categories <ref target=\"#b28\" type=\"bibr\">(Krallinger et al., 2017</ref><ref target=\"#b29\" type=\"bibr\">(Krallinger et al., , 2015</ref>. We hypothesize that such cases where the task data is a narrowly-defined subset of the broader domain, pretraining on the task dataset itself or data relevant to the task may be helpful.</p><p>Task-adaptive pretraining (TAPT) refers to pretraining on the unlabeled training set for a given task; prior work has shown its effectiveness (e.g. <ref target=\"#b21\" type=\"bibr\">Howard and Ruder, 2018)</ref>. Compared to domainadaptive pretraining (DAPT; Â§3), the task-adaptive approach strikes a different trade-off: it uses a far smaller pretraining corpus, but one that is much more task-relevant (under the assumption that the training set represents aspects of the task well). This makes TAPT much less expensive to run than DAPT, and as we show in our experiments, the performance of TAPT is often competitive with that of DAPT.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.1\">Experiments</head><p>Similar to DAPT, task-adaptive pretraining consists of a second phase of pretraining ROBERTA, but only on the available task-specific training data. In contrast to DAPT, which we train for 12.5K steps, we perform TAPT for 100 epochs. We artificially augment each dataset by randomly masking different words (using the masking probability of 0.15) across epochs. As in our DAPT experiments, we pass the final layer [CLS] token representation to a task-specific feedforward layer for classification (see Table <ref target=\"#tab_1\" type=\"table\">14</ref> in Appendix for more hyperparameter details).</p><p>Our results are shown in the TAPT column of Table 5. TAPT consistently improves the ROBERTA baseline for all tasks across domains. Even on the news domain, which was part of ROBERTA pretraining corpus, TAPT improves over ROBERTA, showcasing the advantage of task adaptation. Particularly remarkable are the relative differences between TAPT and DAPT. DAPT is more resource intensive (see Table <ref target=\"#tab_15\" type=\"table\">9</ref> in Â§5.3), but TAPT manages to match its performance in some of the tasks, such as SCIERC. In RCT, HYPERPARTISAN, AGNEWS, HELPFULNESS, and IMDB, the results even exceed those of DAPT, highlighting the efficacy of this cheaper adaptation technique.    <ref target=\"#tab_8\" type=\"table\">5</ref>), it is harmful when applied across tasks. These findings illustrate differences in task distributions within a domain.</p><p>Combined DAPT and TAPT We investigate the effect of using both adaptation techniques together. We begin with ROBERTA and apply DAPT then TAPT under this setting. The three phases of pretraining add up to make this the most computationally expensive of all our settings (see Table <ref target=\"#tab_15\" type=\"table\">9</ref>). As expected, combined domain-and task-adaptive pretraining achieves the best performance on all tasks (Table <ref target=\"#tab_8\" type=\"table\">5</ref>). 2</p><p>Overall, our results show that DAPT followed by TAPT achieves the best of both worlds of domain and task awareness, yielding the best performance. While we speculate that TAPT followed by DAPT would be susceptible to catastrophic forgetting of the task-relevant corpus <ref target=\"#b66\" type=\"bibr\">(Yogatama et al., 2019)</ref>, alternate methods of combining the procedures may result in better downstream performance. Future work may explore pretraining with a more sophisticated curriculum of domain and task distributions.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Cross-Task Transfer</head><p>We complete the comparison between DAPT and TAPT by exploring whether adapting to one task transfers to other tasks in the same domain. For instance, we further pretrain the LM using the RCT unlabeled data, fine-tune it with the CHEMPROT labeled data, and observe the effect. We refer to this setting as Transfer-TAPT. Our results for tasks in all four domains are shown in Table <ref target=\"#tab_9\" type=\"table\">6</ref>. We see that TAPT optimizes for single task performance, to the detriment of cross-task transfer. These results demonstrate that data distributions of tasks within a given domain might differ. Further, this could also explain why adapting only to a broad domain is not sufficient, and why TAPT after DAPT is effective.  typically curated by humans. We explore two scenarios. First, for three tasks (RCT, HYPERPARTISAN, and IMDB) we use this larger pool of unlabeled data from an available human-curated corpus ( Â§5.1). Next, we explore retrieving related unlabeled data for TAPT, from a large unlabeled in-domain corpus, for tasks where extra human-curated data is unavailable ( Â§5.2).</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\">Human Curated-TAPT</head><p>Dataset creation often involves collection of a large unlabeled corpus from known sources. This corpus is then downsampled to collect annotations, based on the annotation budget. The larger unlabeled corpus is thus expected to have a similar distribution to the task's training data. Moreover, it is usually available. We explore the role of such corpora in task-adaptive pretraining.</p><p>Data We simulate a low-resource setting RCT-500, by downsampling the training data of the RCT dataset to 500 examples (out of 180K available), and treat the rest of the training data as unlabeled. The HYPERPARTISAN shared task <ref target=\"#b26\" type=\"bibr\">(Kiesel et al., 2019)</ref> has two tracks: low-and high-resource. We use 5K documents from the high-resource setting as Curated-TAPT unlabeled data and the original lowresource training documents for task fine-tuning. For IMDB, we use the extra unlabeled data manually curated by task annotators, drawn from the same distribution as the labeled data <ref target=\"#b37\" type=\"bibr\">(Maas et al., 2011)</ref>.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Results</head><p>We compare Curated-TAPT to TAPT and DAPT + TAPT in Table <ref target=\"#tab_11\" type=\"table\">7</ref>. Curated-TAPT further improves our prior results from Â§4 across all three datasets. Applying Curated-TAPT after adapting to the domain results in the largest boost in performance on all tasks; in HYPERPARTISAN, DAPT + Curated-TAPT is within standard deviation of Curated-TAPT. Moreover, curated-TAPT achieves Figure <ref type=\"figure\">3</ref>: An illustration of automated data selection ( Â§5.2). We map unlabeled CHEMPROT and 1M BIOMED sentences to a shared vector space using the VAMPIRE model trained on these sentences. Then, for each CHEMPROT sentence, we identify k nearest neighbors, from the BIOMED domain.  95% of the performance of DAPT + TAPT with the fully labeled RCT corpus (Table <ref target=\"#tab_8\" type=\"table\">5</ref>) with only 0.3% of the labeled data. These results suggest that curating large amounts of data from the task distribution is extremely beneficial to end-task performance.</p><p>We recommend that task designers release a large pool of unlabeled task data for their tasks to aid model adaptation through pretraining.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.2\">Automated Data Selection for TAPT</head><p>Consider a low-resource scenario without access to large amounts of unlabeled data to adequately benefit from TAPT, as well as absence of computational resources necessary for DAPT (see Table <ref target=\"#tab_15\" type=\"table\">9</ref> for details of computational requirements for different pretraining phases). We propose simple unsuper-vised methods to retrieve unlabeled text that aligns with the task distribution, from a large in-domain corpus. Our approach finds task-relevant data from the domain by embedding text from both the task and domain in a shared space, then selects candidates from the domain based on queries using the task data. Importantly, the embedding method must be lightweight enough to embed possibly millions of sentences in a reasonable time. Given these constraints, we employ VAMPIRE <ref type=\"bibr\"/> Figure <ref type=\"figure\">3</ref>), a lightweight bag-of-words language model. We pretrain VAM-PIRE on a large deduplicated 3 sample of the domain (1M sentences) to obtain embeddings of the text from both the task and domain sample. We then select k candidates of each task sentence from the domain sample, in embeddings space. Candidates are selected (i) via nearest neighbors selection (kNN-TAPT) 4 , or (ii) randomly (RAND-TAPT). We continue pretraining ROBERTA on this augmented corpus with both the task data (as in TAPT) as well as the selected candidate pool.</p><p>Results Results in Table <ref target=\"#tab_13\" type=\"table\">8</ref> show that kNN-TAPT outperforms TAPT for all cases. RAND-TAPT is generally worse than kNN-TAPT, but within a standard deviation arising from 5 seeds for RCT and ACL-ARC. As we increase k, kNN-TAPT performance steadily increases, and approaches that of DAPT. Appendix F shows examples of nearest neighbors of task data. Future work might consider a closer study of kNN-TAPT, more sophisticated data selection methods, and the tradeoff between the diversity and task relevance of selected examples.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.3\">Computational Requirements</head><p>The computational requirements for all our adaptation techniques on RCT-500 in the BIOMED domain in Table <ref target=\"#tab_15\" type=\"table\">9</ref>. TAPT is nearly 60 times faster to train than DAPT on a single v3-8 TPU and storage requirements for DAPT on this task are 5.8M times that of TAPT. Our best setting of DAPT + TAPT amounts to three phases of pretraining, and at first glance appears to be very expensive. However, once the LM has been adapted to a broad domain, it can be reused for multiple tasks within that domain, with only a single additional TAPT phase per task. While Curated-TAPT tends to achieve the best cost-  benefit ratio in this comparison, one must also take into account the cost of curating large in-domain data. Automatic methods such as kNN-TAPT are much cheaper than DAPT.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6\">Related Work</head><p>Transfer learning for domain adaptation Prior work has shown the benefit of continued pretraining in domain <ref target=\"#b1\" type=\"bibr\">(Alsentzer et al., 2019;</ref><ref target=\"#b6\" type=\"bibr\">Chakrabarty et al., 2019;</ref>. <ref type=\"bibr\">5</ref> We have contributed further investigation of the effects of a shift between a large, diverse pretraining corpus and target domain on task performance.</p><p>Other studies (e.g.,  have trained language models (LMs) in their domain of interest, from scratch. In contrast, our work explores multiple domains, and is arguably more cost effective, since we continue pretraining an already powerful LM.</p><p>Task-adaptive pretraining Continued pretraining of a LM on the unlabeled data of a given task (TAPT) has been show to be beneficial for endtask performance (e.g. in <ref target=\"#b21\" type=\"bibr\">Howard and Ruder, 2018;</ref><ref target=\"#b44\" type=\"bibr\">Phang et al., 2018;</ref><ref target=\"#b52\" type=\"bibr\">Sun et al., 2019)</ref>. In the presence of domain shift between train and test data distributions of the same task, domain-adaptive pretraining (DAPT) is sometimes used to describe what we term TAPT <ref target=\"#b35\" type=\"bibr\">(Logeswaran et al., 2019;</ref><ref target=\"#b18\" type=\"bibr\">Han and Eisenstein, 2019)</ref>. Related approaches include language modeling as an auxiliary objective to task classifier fine-tuning <ref target=\"#b8\" type=\"bibr\">(Chronopoulou et al., 2019;</ref><ref target=\"#b46\" type=\"bibr\">Radford et al., 2018)</ref>   data <ref target=\"#b53\" type=\"bibr\">(Swayamdipta et al., 2019)</ref>. We compare DAPT and TAPT as well as their interplay with respect to dataset size for continued pretraining (hence, expense of more rounds of pretraining), relevance to a data sample of a given task, and transferability to other tasks and datasets. See Table <ref target=\"#tab_1\" type=\"table\">11</ref> in Appendix Â§A for a summary of multi-phase pretraining strategies from related work.</p><p>Data selection for transfer learning Selecting data for transfer learning has been explored in NLP <ref target=\"#b40\" type=\"bibr\">(Moore and Lewis, 2010;</ref><ref target=\"#b50\" type=\"bibr\">Ruder and Plank, 2017;</ref><ref type=\"bibr\">Zhang et al., 2019, among others)</ref>.  focus on identifying the most suitable corpus to pretrain a LM from scratch, for a single task: NER, whereas we select relevant examples for various tasks in Â§5.2. Concurrent to our work, <ref target=\"#b0\" type=\"bibr\">Aharoni and Goldberg (2020)</ref> propose data selection methods for NMT based on cosine similarity in embedding space, using DISTILBERT <ref target=\"#b51\" type=\"bibr\">(Sanh et al., 2019)</ref> for efficiency. In contrast, we use VAMPIRE, and focus on augmenting TAPT data for text classification tasks. <ref target=\"#b25\" type=\"bibr\">Khandelwal et al. (2020)</ref> introduced kNN-LMs that allows easy domain adaptation of pretrained LMs by simply adding a datastore per domain and no further training; an alternative to integrate domain information in an LM. Our study of human-curated data Â§5.1 is related to focused crawling <ref target=\"#b5\" type=\"bibr\">(Chakrabarti et al., 1999)</ref> for collection of suitable data, especially with LM reliance <ref target=\"#b48\" type=\"bibr\">(Remus and Biemann, 2016)</ref>.</p><p>What is a domain? Despite the popularity of domain adaptation techniques, most research and practice seems to use an intuitive understanding of domains. A small body of work has attempted to address this question <ref target=\"#b31\" type=\"bibr\">(Lee, 2001;</ref><ref target=\"#b14\" type=\"bibr\">Eisenstein et al., 2014;</ref><ref target=\"#b59\" type=\"bibr\">van der Wees et al., 2015;</ref><ref target=\"#b45\" type=\"bibr\">Plank, 2016;</ref><ref type=\"bibr\">Ruder et al., 2016, among others)</ref>. For instance, <ref target=\"#b0\" type=\"bibr\">Aharoni and Goldberg (2020)</ref> define domains by implicit clusters of sentence representations in pretrained LMs. Our results show that DAPT and TAPT complement each other, which suggests a spectra of domains defined around tasks at various levels of granularity (e.g., Amazon reviews for a specific product, all Amazon reviews, all reviews on the web, the web).</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"7\">Conclusion</head><p>We investigate several variations for adapting pretrained LMs to domains and tasks within those domains, summarized in Table <ref target=\"#tab_1\" type=\"table\">10</ref>. Our experiments reveal that even a model of hundreds of millions of parameters struggles to encode the complexity of a single textual domain, let alone all of language. We show that pretraining the model towards a specific task or small corpus can provide significant benefits. Our findings suggest it may be valuable to complement work on ever-larger LMs with parallel efforts to identify and use domain-and taskrelevant corpora to specialize models. While our results demonstrate how these approaches can improve ROBERTA, a powerful LM, the approaches we studied are general enough to be applied to any pretrained LM. Our work points to numerous future directions, such as better data selection for TAPT, efficient adaptation large pretrained language models to distant domains, and building reusable language models after adaptation.</p><p>Appendix F. Illustration of our data selection method and examples of nearest neighbours.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A Related Work</head><p>Table <ref target=\"#tab_1\" type=\"table\">11</ref> shows which of the strategies for continued pretraining have already been explored in the prior work from the Related Work ( Â§6). As evident from the table, our work compares various strategies as well as their interplay using a pretrained language model trained on a much more heterogeneous pretraining corpus.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>A.1 ROBERTA's Pretraining Corpus</head><p>ROBERTA was trained on data from BOOKCOR-PUS <ref target=\"#b70\" type=\"bibr\">(Zhu et al., 2015)</ref>, 6 WIKIPEDIA, 7 a portion of the CCNEWS dataset <ref target=\"#b41\" type=\"bibr\">(Nagel, 2016)</ref>, <ref type=\"bibr\">8</ref> OPENWEB-TEXT corpus of Web content extracted from URLs shared on Reddit <ref target=\"#b16\" type=\"bibr\">(Gokaslan and Cohen, 2019)</ref>, <ref type=\"bibr\">9</ref> and a subset of CommonCrawl that it is said to resemble the \"story-like\" style of WINOGRAD schemas <ref type=\"bibr\">(STORIES;</ref><ref target=\"#b55\" type=\"bibr\">Trinh and Le, 2018</ref> <ref target=\"#tab_8\" type=\"table\">5</ref> for the reported performance of these models. For ACL-ARC, that is SCIBERT , a BERT-base model for trained from scratch on scientific text. For CHEMPROT and SCI-ERC, that is S2ORC-BERT <ref target=\"#b34\" type=\"bibr\">(Lo et al., 2020)</ref>, a similar model to SCIBERT. For AGNEWS and IMDB, XLNet-large, a much larger model. For RCT, . For HYPERPARTISAN, LONGFORMER, a modified Transformer language model for long documents <ref target=\"#b4\" type=\"bibr\">(Beltagy et al., 2020)</ref>. <ref target=\"#b54\" type=\"bibr\">Thongtan and Phienthrakul (2019)</ref> report a higher number (97.42) on IMDB, but they train their word vectors on the test set. Our baseline establishes the first benchmark for the HELPFULNESS dataset.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>B Experimental Setup</head><p>Preprocessing for DAPT The unlabeled corpus in each domain was pre-processed prior to language model training. Abstracts and body paragraphs from biomedical and computer science articles were used after sentence splitting using scispaCy <ref target=\"#b42\" type=\"bibr\">(Neumann et al., 2019)</ref>. We used summaries and full text of each news article, and the entire body of review from Amazon reviews. For both news and reviews, we perform sentence splitting using spaCy <ref target=\"#b20\" type=\"bibr\">(Honnibal and Montani, 2017)</ref>.</p><p>Training details for DAPT We train ROBERTA on each domain for 12.5K steps. We focused on matching all the domain dataset sizes (see Table <ref target=\"#tab_1\" type=\"table\">1</ref>) such that each domain is exposed to the same amount of data as for 12.5K steps it is trained for. AMAZON reviews contain more documents, but each is shorter. We used an effective batch size of 2048 through gradient accumulation, as recommended in . See Table <ref target=\"#tab_1\" type=\"table\">13</ref>   used a maximum learning rate of 0.0001, as recommended in . We observe a high variance in performance between random seeds when fine-tuning ROBERTA to HYPERPARTISAN, because the dataset is extremely small. To produce final results on this task, we discard and resample degenerate seeds. We display the full hyperparameter settings in Table <ref target=\"#tab_1\" type=\"table\">13</ref>. iment was performed on a single v3-8 TPU from Google Cloud. <ref type=\"bibr\">13</ref> For the text classification tasks, we used AllenNLP <ref target=\"#b15\" type=\"bibr\">(Gardner et al., 2018)</ref>. Following standard practice  we pass the final layer [CLS] token representation to a task-specific feedforward layer for prediction.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>C Development Set Results</head><p>Adhering to the standards suggested by <ref target=\"#b13\" type=\"bibr\">Dodge et al. (2019)</ref> for replication, we report our development set results in Tables 15, 17, and 18.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>D Analysis of Domain Overlap</head><p>In </p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>E Analysis of Cross-Domain Masked LM Loss</head><p>In Section Â§3.2, we provide ROBERTA's masked LM loss before and after DAPT. We display crossdomain masked-LM loss in Table <ref target=\"#tab_1\" type=\"table\">12</ref>, where we evaluate masked LM loss on text samples in other domains after performing DAPT.</p><p>We observe that the cross-domain masked-LM loss mostly follows our intuition and insights from the paper, i.e. ROBERTA's pretraining corpus and NEWS are closer, and BIOMED to CS (relative to other domains). However, our analysis in Â§3.1 illustrates that REVIEWS and NEWS also have some similarities. This is supported with the loss of ROBERTA that is adapted to NEWS, calculated on a sample of REVIEWS. However, ROBERTA that is adapted to REVIEWS results in the highest loss for a NEWS sample. This is the case for all domains. One of the properties that distinguishes REVIEWS from all other domains is that its documents are significantly shorter. In general, we find that cross-DAPT masked-LM loss can in some cases be a noisy predictor of domain similarity.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>F k-Nearest Neighbors Data Selection</head><p>In Table <ref target=\"#tab_1\" type=\"table\">21</ref>, we display nearest neighbor documents in the BIOMED domain identified by our selection method, on the RCT dataset.    </p></div><figure xml:id=\"fig_0\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>or consider simple syntactic structure of the input while adapting to task-specific</figDesc></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_1\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 1 :</head><label>1</label><figDesc>List of the domain-specific unlabeled datasets. In columns 5 and 6, we report ROBERTA's masked LM loss on 50K randomly sampled held-out documents from each domain before (L ROB. ) and after (L DAPT ) DAPT (lower implies a better fit on the sample). â€¡ indicates that the masked LM loss is estimated on data sampled from sources similar to ROBERTA's pretraining corpus.</figDesc><table><row><cell/><cell/><cell/><cell/><cell/><cell/><cell>3.2 Experiments</cell></row><row><cell>PT</cell><cell cols=\"2\">100.0 54.1</cell><cell>34.5</cell><cell>27.3</cell><cell>19.2</cell><cell>Our LM adaptation follows the settings prescribed</cell></row><row><cell/><cell/><cell/><cell/><cell/><cell/><cell>for training ROBERTA. We train ROBERTA on</cell></row><row><cell>News</cell><cell cols=\"3\">54.1 100.0 40.0</cell><cell>24.9</cell><cell>17.3</cell><cell>each domain for 12.5K steps, which amounts to</cell></row><row><cell/><cell/><cell/><cell/><cell/><cell/><cell>single pass on each domain dataset, on a v3-8 TPU;</cell></row><row><cell>Reviews</cell><cell>34.5</cell><cell cols=\"3\">40.0 100.0 18.3</cell><cell>12.7</cell><cell>see other details in Appendix B. This second phase</cell></row><row><cell/><cell/><cell/><cell/><cell/><cell/><cell>of pretraining results in four domain-adapted LMs,</cell></row><row><cell>BioMed</cell><cell>27.3</cell><cell>24.9</cell><cell cols=\"3\">18.3 100.0 21.4</cell><cell>one for each domain. We present the masked LM</cell></row><row><cell/><cell/><cell/><cell/><cell/><cell/><cell>loss of ROBERTA on each domain before and after</cell></row><row><cell>CS</cell><cell>19.2</cell><cell>17.3</cell><cell>12.7</cell><cell cols=\"2\">21.4 100.0</cell><cell>DAPT in Table 1. We observe that masked LM loss</cell></row><row><cell/><cell/><cell/><cell/><cell/><cell/><cell>decreases in all domains except NEWS after DAPT,</cell></row><row><cell/><cell>PT</cell><cell cols=\"3\">News Reviews BioMed</cell><cell>CS</cell><cell>where we observe a marginal increase. We discuss</cell></row><row><cell/><cell/><cell/><cell/><cell/><cell/><cell>cross-domain masked LM loss in Appendix  Â§E.</cell></row><row><cell cols=\"6\">Figure 2: Vocabulary overlap (%) between do-</cell><cell>Under each domain, we consider two text clas-</cell></row><row><cell cols=\"6\">mains. PT denotes a sample from sources similar to</cell><cell>sification tasks, as shown in Table 2. Our tasks</cell></row><row><cell cols=\"6\">ROBERTA's pretraining corpus. Vocabularies for each domain are created by considering the top 10K most frequent words (excluding stopwords) in documents sampled from each domain.</cell><cell>represent both high-and low-resource (â‰¤ 5K la-beled training examples, and no additional unla-beled data) settings. For HYPERPARTISAN, we use the data splits from</cell></row><row><cell cols=\"6\">random samples of held-out documents in each do-</cell></row><row><cell cols=\"6\">main's corpus. We use 50K held-out documents</cell></row><row><cell cols=\"6\">for each domain other than REVIEWS, and 150K</cell></row><row><cell cols=\"6\">held-out documents in REVIEWS, since they are</cell></row><row><cell cols=\"6\">much shorter. We also sample 50K documents from</cell></row><row><cell cols=\"6\">sources similar to ROBERTA's pretraining corpus</cell></row><row><cell cols=\"6\">(i.e., BOOKCORPUS, STORIES, WIKIPEDIA, and</cell></row><row><cell cols=\"6\">REALNEWS) to construct the pretraining domain</cell></row><row><cell cols=\"6\">vocabulary, since the original pretraining corpus</cell></row><row><cell cols=\"6\">is not released. Figure 2 shows the vocabulary</cell></row><row><cell cols=\"6\">overlap across these samples. We observe that</cell></row><row><cell cols=\"6\">ROBERTA's pretraining domain has strong vocab-</cell></row><row><cell cols=\"6\">ulary overlap with NEWS and REVIEWS, while</cell></row><row><cell cols=\"6\">CS and BIOMED are far more dissimilar to the</cell></row><row><cell cols=\"6\">other domains. This simple analysis suggests the</cell></row><row><cell cols=\"6\">degree of benefit to be expected by adaptation of</cell></row><row><cell cols=\"6\">ROBERTA to different domains-the more dissim-</cell></row><row><cell cols=\"6\">ilar the domain, the higher the potential for DAPT.</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_3\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>). 88.2 5.9 76.4 4.9 â€  AGNEWS 93.9 0.2 93.9 0.2 93.5 0.2 REV.</figDesc><table><row><cell cols=\"2\">Dom. Task</cell><cell>ROBA.</cell><cell>DAPT Â¬DAPT</cell></row><row><cell>BM</cell><cell cols=\"3\">CHEMPROT 81.9 1.0 84.2 0.2 79.4 1.3  â€  RCT 87.2 0.1 87.6 0.1 86.9 0.1</cell></row><row><cell>CS</cell><cell>ACL-ARC SCIERC</cell><cell cols=\"2\">63.0 5.8 75.4 2.5 66.4 4.1 77.3 1.9 80.8 1.5 79.2 0.9</cell></row><row><cell>NEWS</cell><cell>HYP.</cell><cell>86.6 0.9</cell></row></table><note>â€  HELPFUL. 65.1 3.4 66.5 1.4 65.1 2.8 â€  IMDB 95.0 0.2 95.4 0.2 94.1 0.4</note></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_4\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 3 :</head><label>3</label><figDesc/><table/><note>Comparison of ROBERTA (ROBA.) and DAPT to adaptation to an irrelevant domain (Â¬ DAPT). Reported results are test macro-F 1 , except for CHEMPROT and RCT, for which we report micro-F 1 , following. We report averages across five random seeds, with standard deviations as subscripts. â€  indicates high-resource settings. Best task performance is boldfaced. See Â§3.3 for our choice of irrelevant domains.</note></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_5\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>[...]  Three great festive films... The Shop Around the Corner (1940) Delightful Comedy by Ernst Lubitsch stars James Stewart and Margaret Sullavan falling in love at Christmas. Remade as Youve Got Mail.[...]    Simply the Best! I've owned countless Droids and iPhones, but this one destroys them all. Samsung really nailed it with this one, extremely fast , very pocketable, gorgeous display , exceptional battery life , good audio quality, perfect GPS &amp; WiFi performance, transparent status bar, battery percentage, ability to turn off soft key lights, superb camera for a smartphone and more![...]    Were living in a world with a new Samsung. [...] more on battery life later[...]  Exposure is usually spot on and focusing is very fast. [...] The design, display, camera and performance are all best in class, and the phone feels smaller than it looks.[...]    </figDesc><table><row><cell>HELPFULNESS review</cell><cell>REALNEWS article</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_6\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 4 :</head><label>4</label><figDesc>Examples that illustrate how some domains might have overlaps with others, leading to unexpected positive transfer. We highlight expressions in the reviews that are also found in the REALNEWS articles.NEWS not as harmful to its performance on RE-VIEWS tasks (DAPT on NEWS achieves 65.5 2.3 on HELPFULNESS and 95.0 0.1 on IMDB).</figDesc><table/></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_8\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 5 :</head><label>5</label><figDesc>Results on different phases of adaptive pretraining compared to the baseline ROBERTA (col. 1). Our approaches are DAPT (col. 2, Â§3), TAPT (col. 3, Â§4), and a combination of both (col. 4). Reported results follow the same format as Table3. State-of-the-art results we can compare to: CHEMPROT (84.</figDesc><table><row><cell>6), RCT (92.9), ACL-ARC</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_9\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 6 :</head><label>6</label><figDesc/><table/><note>Though TAPT is effective (Table</note></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_11\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 7 :</head><label>7</label><figDesc>Mean test set macro-F 1 (for HYP. and IMDB) and micro-F 1 (for RCT-500), with Curated-TAPT across five random seeds, with standard deviations as subscripts. â€  indicates high-resource settings.</figDesc><table/></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_13\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 8 :</head><label>8</label><figDesc>Mean test set micro-F 1 (for CHEMPROT and RCT) and macro-F 1 (for ACL-ARC), across five random seeds, with standard deviations as subscripts, comparing RAND-TAPT (with 50 candidates) and kNN-TAPT selection. Neighbors of the task data are selected from the domain data.</figDesc><table/></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_15\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 9 :</head><label>9</label><figDesc>Computational requirements for adapting to the RCT-500 task, comparing DAPT ( Â§3) and the various TAPT modifications described in Â§4 and Â§5.</figDesc><table/></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_16\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 10 :</head><label>10</label><figDesc>Summary of strategies for multi-phase pretraining explored in this paper.</figDesc><table/></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_18\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table</head><label/><figDesc/><table/></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_19\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>Training details for TAPT We use the same pretraining hyperparameters as DAPT, but we artificially augmented each dataset for TAPT by randomly masking different tokens across epochs, using the masking probability of 0.15. Each dataset was trained for 100 epochs. For tasks with less than 5K examples, we used a batch size of 256 through gradient accumulation. See Table13for more hyperparameter details.</figDesc><table><row><cell/><cell>DAPT Domains (if applicable)</cell><cell>Tasks</cell><cell>Model</cell><cell>DAPT</cell><cell>TAPT</cell><cell>DAPT + TAPT</cell><cell>kNN-TAPT</cell><cell>Curated-TAPT</cell></row><row><cell>This Paper</cell><cell>biomedical &amp; computer science papers, news, reviews</cell><cell>8 classification tasks</cell><cell>ROBERTA</cell><cell/><cell/><cell/><cell/><cell/></row><row><cell>Aharoni and Goldberg (2020)</cell><cell>-</cell><cell>NMT</cell><cell>DISTILBERT + Transformer NMT</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>similar</cell><cell>-</cell></row><row><cell>Alsentzer et al. (2019)</cell><cell>clinical text</cell><cell>NER, NLI, de-identification</cell><cell>(BIO)BERT</cell><cell/><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Chakrabarty et al. (2019)</cell><cell>opinionated claims from Reddit</cell><cell>claim detection</cell><cell>ULMFIT</cell><cell/><cell/><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Chronopoulou et al. (2019)</cell><cell>-</cell><cell>5 classification tasks</cell><cell>ULMFIT  â€ </cell><cell>-</cell><cell>similar</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Han and Eisenstein (2019)</cell><cell>-</cell><cell>NER in historical texts</cell><cell>ELMO, BERT</cell><cell>-</cell><cell/><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Howard and Ruder (2018)</cell><cell>-</cell><cell>6 classification tasks</cell><cell>ULMFIT</cell><cell>-</cell><cell/><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Khandelwal et al. (2020)</cell><cell>-</cell><cell cols=\"2\">language modeling Transformer LM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>similar</cell><cell>-</cell></row><row><cell>Lee et al. (2019)</cell><cell>biomedical papers</cell><cell>NER, QA, relation extraction</cell><cell>BERT</cell><cell/><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Logeswaran et al. (2019)</cell><cell>-</cell><cell>zero-shot entity linking in Wikia</cell><cell>BERT</cell><cell>-</cell><cell/><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Mitra et al. (2020)</cell><cell>-</cell><cell>commonsense QA</cell><cell>BERT</cell><cell>-</cell><cell/><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Phang et al. (2018)</cell><cell>-</cell><cell>GLUE tasks</cell><cell>ELMO, BERT, GPT</cell><cell>-</cell><cell/><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell/><cell/><cell>NLI, QA,</cell><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>Radford et al. (2018)</cell><cell>-</cell><cell>similarity,</cell><cell>GPT</cell><cell>-</cell><cell>similar</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell/><cell/><cell>classification</cell><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>Sun et al. (2019)</cell><cell>sentiment, question, topic</cell><cell>7 classification tasks</cell><cell>BERT</cell><cell/><cell/><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Swayamdipta et al. (2019)</cell><cell>-</cell><cell>NER, parsing, classification</cell><cell>ELMO</cell><cell>-</cell><cell>similar</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell/><cell/><cell>RC, aspect extract.,</cell><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>Xu et al. (2019a)</cell><cell>reviews</cell><cell>sentiment</cell><cell>BERT</cell><cell/><cell/><cell/><cell>-</cell><cell>-</cell></row><row><cell/><cell/><cell>classification</cell><cell/><cell/><cell/><cell/><cell/><cell/></row><row><cell>Xu et al. (2019b)</cell><cell>restaurant reviews, laptop reviews</cell><cell>conversational RC</cell><cell>BERT</cell><cell/><cell/><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell/><cell/><cell/><cell/><cell/><cell/><cell/><cell cols=\"2\">for more</cell></row><row><cell/><cell/><cell cols=\"3\">hyperparameter details.</cell><cell/><cell/><cell/><cell/></row><row><cell/><cell/><cell cols=\"7\">Optimization We used the Adam optimizer</cell></row><row><cell/><cell/><cell cols=\"7\">(Kingma and Ba, 2015), a linear learning rate sched-</cell></row><row><cell/><cell/><cell cols=\"7\">uler with 6% warm-up, a maximum learning rate</cell></row><row><cell/><cell/><cell cols=\"7\">of 0.0005. When we used a batch size of 256, we</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_20\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 11 :</head><label>11</label><figDesc>Overview of prior work across strategies for continued pre-training summarized in Table10. ULMFIT is pretrained on English Wikipedia; ULMFIT â€  on English tweets; ELMO on the 1BWORDBENCHMARK (newswire;<ref target=\"#b7\" type=\"bibr\">Chelba et al., 2014)</ref>; GPT on BOOKCORPUS; BERT on English Wikipedia and BOOKCORPUS. In comparison to these pretraining corpora, ROBERTA's pretraining corpus is substantially more diverse (see Appendix Â§A.1).</figDesc><table/></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_21\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head/><label/><figDesc>Implementation Our LM implementation uses the HuggingFace transformers library<ref target=\"#b51\" type=\"bibr\">(Wolf et al., 2019)</ref> 11  and PyTorch XLA for TPU compatibility. 12 Each adaptive pretraining exper-</figDesc><table/><note>11 https://github.com/huggingface/ transformers 12 https://github.com/pytorch/xla</note></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_22\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 20</head><label>20</label><figDesc>we display additional examples that highlight the overlap between IMDB reviews and REALNEWS articles, relevant for analysis in Â§3.1.</figDesc><table/></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_24\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 12 :</head><label>12</label><figDesc>ROBERTA's (row 1) and domain-adapted ROBERTA's (rows 2-5) masked LM loss on randomly sampled held-out documents from each domain (lower implies a better fit). PT denotes a sample from sources similar to ROBERTA's pretraining corpus. The lowest masked LM for each domain sample is boldfaced.</figDesc><table><row><cell>Computing Infrastructure</cell><cell/><cell>Google Cloud v3-8 TPU</cell></row><row><cell>Model implementations</cell><cell cols=\"2\">https://github.com/allenai/tpu_pretrain</cell></row><row><cell>Hyperparameter</cell><cell/><cell>Assignment</cell></row><row><cell>number of steps</cell><cell/><cell>100 epochs (TAPT) or 12.5K steps (DAPT)</cell></row><row><cell>batch size</cell><cell/><cell>256 or 2058</cell></row><row><cell cols=\"2\">maximum learning rate</cell><cell>0.0001 or 0.0005</cell></row><row><cell cols=\"2\">learning rate optimizer</cell><cell>Adam</cell></row><row><cell>Adam epsilon</cell><cell/><cell>1e-6</cell></row><row><cell cols=\"2\">Adam beta weights</cell><cell>0.9, 0.98</cell></row><row><cell cols=\"2\">learning rate scheduler</cell><cell>None or warmup linear</cell></row><row><cell>Weight decay</cell><cell/><cell>0.01</cell></row><row><cell cols=\"2\">Warmup proportion</cell><cell>0.06</cell></row><row><cell cols=\"2\">learning rate decay</cell><cell>linear</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_25\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 13 :</head><label>13</label><figDesc>Hyperparameters for domain-and task-adaptive pretraining.</figDesc><table><row><cell>Computing Infrastructure</cell><cell cols=\"2\">Quadro RTX 8000 GPU</cell></row><row><cell>Model implementation</cell><cell cols=\"2\">https://github.com/allenai/dont-stop-pretraining</cell></row><row><cell/><cell>Hyperparameter</cell><cell>Assignment</cell></row><row><cell/><cell>number of epochs</cell><cell>3 or 10</cell></row><row><cell/><cell>patience</cell><cell>3</cell></row><row><cell/><cell>batch size</cell><cell>16</cell></row><row><cell/><cell>learning rate</cell><cell>2e-5</cell></row><row><cell/><cell>dropout</cell><cell>0.1</cell></row><row><cell/><cell>feedforward layer</cell><cell>1</cell></row><row><cell/><cell>feedforward nonlinearity</cell><cell>tanh</cell></row><row><cell/><cell>classification layer</cell><cell>1</cell></row></table></figure>\n",
       " <figure type=\"table\" xml:id=\"tab_26\" xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Table 14 :</head><label>14</label><figDesc>Hyperparameters for ROBERTA text classifier.</figDesc><table/></figure>\n",
       " <note n=\"1\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">https://github.com/allenai/ dont-stop-pretraining</note>\n",
       " <note n=\"1\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">For BIOMED and CS, we used an internal version of S2ORC that contains papers that cannot be released due to copyright restrictions.</note>\n",
       " <note n=\"2\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">Results on HYPERPARTISAN match those of TAPT, within a standard deviation arising from the five seeds.</note>\n",
       " <note n=\"3\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">We deduplicated this set to limit computation, since different sentences can share neighbors.4  We use a flat search index with cosine similarity between embeddings with the FAISS<ref target=\"#b23\" type=\"bibr\">(Johnson et al., 2019)</ref> library.</note>\n",
       " <note n=\"5\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">In contrast, find that the Jensen-Shannon divergence on term distributions between BERT's pretraining corpora and each MULTINLI domain<ref target=\"#b60\" type=\"bibr\">(Williams et al., 2018)</ref> does not predict its performance, though this might be an isolated finding specific to the MultiNLI dataset.</note>\n",
       " <note n=\"13\" place=\"foot\" xmlns=\"http://www.tei-c.org/ns/1.0\">http://github.com/allenai/ tpu-pretrain</note>\n",
       " </body>\n",
       " <back>\n",
       " <div type=\"acknowledgement\">\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Acknowledgments</head><p>The authors thank Dallas Card, Mark Neumann, Nelson Liu, Eric Wallace, members of the Al-lenNLP team, and anonymous reviewers for helpful feedback, and Arman Cohan for providing data. This research was supported in part by the Office of Naval Research under the MURI grant N00014-18-1-2670. TPU machines for conducting experiments were provided by Google.</p></div>\n",
       " </div>\n",
       " <div type=\"annex\">\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Appendix Overview</head><p>In this supplementary material, we provide: (i) additional information for producing the results in the paper, and (ii) results that we could not fit into the main body of the paper. Appendix A. A tabular overview of related work described in Section Â§6, a description of the corpus used to train ROBERTA in , and references to the state of the art on our tasks.      Table <ref type=\"table\">19</ref>: Mean development set macro-F 1 (for HYP. and IMDB) and micro-F 1 (for RCT), across five random seeds, with standard deviations as subscripts, comparing RAND-TAPT (with 50 candidates) and kNN-TAPT selection. Neighbors of the task data are selected from the domain data.</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>IMDB review REALNEWS article</head><p>Spooks is enjoyable trash, featuring some well directed sequences, ridiculous plots and dialogue, and some third rate acting. Many have described this is a UK version of 24, and one can see the similarities. The American version shares the weak silly plots, but the execution is so much slicker, sexier and I suspect, expensive. Some people describe weak comedy as gentle comedy. This is gentle spy story hour, the exact opposite of anything created by John Le Carre. Give me Smiley any day.</p><p>[...] Remember poor Helen Flynn from Spooks? In 2002, the headlong BBC spy caper was in such a hurry to establish the high-wire stakes of its morally compromised world that Lisa Faulkners keen-as-mustard MI5 rookie turned out to be a lot more expendable than her prominent billing suggested. [...] Functioning as both a shocking twist and rather callous statement that No-One Is Safe, it gave the slick drama an instant patina of edginess while generating a record-breaking number of complaints.</p><p>[</p><p>The Sopranos is perhaps the most mind-opening series you could possibly ever want to watch. It's smart, it's quirky, it's funny -and it carries the mafia genre so well that most people can't resist watching.</p><p>The best aspect of this show is the overwhelming realism of the characters, set in the subterranean world of the New York crime families.</p><p>For most of the time, you really don't know whether the wise guys will stab someone in the back, or buy them lunch. Further adding to the realistic approach of the characters in this show is the depth of their personalities -These are dangerous men, most of them murderers, but by God if you don't love them too. I've laughed at their wisecracks, been torn when they've made err in judgement, and felt scared at the sheer ruthlessness of a serious criminal. <ref type=\"bibr\">[...]</ref> The drumbeat regarding the Breaking Bad finale has led to the inevitable speculation on whether the final chapter in this serialized gem will live up to the hype or disappoint (thank you, Dexter, for setting that bar pretty low), with debate, second-guessing and graduate-thesis-length analysis sure to follow. The Most Memorable TV Series Finales of All-Time <ref type=\"bibr\">[...]</ref> No ending in recent years has been more divisive than The Sopranos for some, a brilliant flash (literally, in a way) of genius; for others (including yours truly), a too-cute copout, cryptically leaving its characters in perpetual limbo. The precedent to that would be St. Elsewhere, which irked many with its provocative, surreal notion that the whole series was, in fact, conjured in the mind of an autistic child.</p><p>[...]</p><p>The Wicker Man, starring Nicolas Cage, is by no means a good movie, but I can't really say it's one I regret watching. I could go on and on about the negative aspects of the movie, like the terrible acting and the lengthy scenes where Cage is looking for the girl, has a hallucination, followed by another hallucination, followed by a dream sequence-with a hallucination, etc., but it's just not worth dwelling on when it comes to a movie like this. Instead, here's five reasons why you SHOULD watch The Wicker Man, even though it's bad: 5. It's hard to deny that it has some genuinely creepy ideas to it, the only problem is in its cheesy, unintentionally funny execution. If nothing else, this is a movie that may inspire you to see the original 1973 film, or even read the short story on which it is based. 4. For a cheesy horror/thriller, it is really aesthetically pleasing. [...] NOTE: The Unrated version of the movie is the best to watch, and it's better to watch the Theatrical version just for its little added on epilogue, which features a cameo from James Franco.</p><p>[...] What did you ultimately feel about \"The Wicker Man\" movie when all was said and done? [...] Im a fan of the original and Im glad that I made the movie because they dont make movies like that anymore and probably the result of what \"Wicker Man\" did is the reason why they dont make movies like that anymore. Again, its kind of that 70s sensibility, but Im trying to do things that are outside the box. Sometimes that means itll work and other times it wont. Again though Im going to try and learn from anything that I do. I think that it was a great cast, and Neil La Bute is one of the easiest directors that Ive ever worked with. He really loves actors and he really gives you a relaxed feeling on the set, that you can achieve whatever it is that youre trying to put together, but at the end of the day the frustration that I had with The Wicker Man, which I think has been remedied on the DVD because I believe the DVD has the directors original cut, is that they cut the horror out of the horror film to try and get a PG-13 rating. I mean, I dont know how to stop something like that. So Im not happy with the way that the picture ended, but Im happy with the spirit with which it was made. [...]</p><p>Dr. Seuss would sure be mad right now if he was alive. Cat in the Hat proves to show how movie productions can take a classic story and turn it into a mindless pile of goop. We have Mike Myers as the infamous Cat in the Hat, big mistake! Myers proves he can't act in this film. He acts like a prissy show girl with a thousand tricks up his sleeve. The kids in this movie are all right, somewhere in between the lines of dull and annoying. The story is just like the original with a couple of tweaks and like most movies based on other stories, never tweak with the original story! Bringing in the evil neighbor Quin was a bad idea. He is a stupid villain that would never get anywhere in life. <ref type=\"bibr\">[...]</ref> The Cat in the Hat, <ref type=\"bibr\">[...]</ref> Based on the book by Dr. Seuss <ref type=\"bibr\">[...]</ref> From the moment his tall, red-and-white-striped hat appears at their door, Sally and her brother know that the Cat in the Hat is the most mischievous cat they will ever meet. Suddenly the rainy afternoon is transformed by the Cat and his antics. Will their house ever be the same? Can the kids clean up before mom comes home? With some tricks (and a fish) and Thing Two and Thing One, with the Cat in The Hat, the fun's never done!Dr. Seuss is known worldwide as the imaginative master of children's literature. His books include a wonderful blend of invented and actual words, and his rhymes have helped many children and adults learn and better their understanding of the English language. <ref type=\"bibr\">[...]</ref>   Neighbor 0 Of this group, 26% died after discharge from hospital, and the median time to death was 11 days (interquartile range, 4.0-15.0 days) after discharge. Neighbor 1</p><p>The median hospital stay was 17 days (range 8-26 days), and all the patients were discharged within 1 month. Neighbor 2</p><p>The median hospital stay was 17 days (range 8-26 days). Neighbor 3</p><p>The median time between discharge and death was 25 days (mean, 59.1 days) and no patient was alive after 193 days. Neighbor 4</p><p>The length of hospital stay after colostomy formation ranged from 3 days to 14 days with a median duration of 6 days (+IQR of 4 to 8 days).</p></div>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Source</head><p>Randomized , controlled , parallel clinical trial . Source Forty primary molar teeth in 40 healthy children aged 5-9 years were treated by direct pulp capping .</p><p>Neighbor 0 In our study, we specifically determined the usefulness of the Er:YAG laser in caries removal and cavity preparation of primary and young permanent teeth in children ages 4 to 1 8 years. Neighbor 1</p><p>Males watched more TV than females, although it was only in primary school-aged children and on weekdays. Neighbor 2</p><p>Assent was obtained from children and adolescents aged 7-17 years. Neighbor 3 Cardiopulmonary resuscitation was not applied to children aged Â¡5 years (Table <ref type=\"table\">2</ref>). Neighbor 4</p><p>It measures HRQoL in children and adolescents aged 2 to 25 years. </p></div> </div>\n",
       " <div type=\"references\">\n",
       " <listBibl>\n",
       " <biblStruct xml:id=\"b0\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Unsupervised domain clusters in pretrained language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Roee</forename><surname>Aharoni</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yoav</forename><surname>Goldberg</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>To appear</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b1\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Publicly available clinical BERT embeddings</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Emily</forename><surname>Alsentzer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">John</forename><surname>Murphy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">William</forename><surname>Boag</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wei-Hung</forename><surname>Weng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Di</forename><surname>Jindi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tristan</forename><surname>Naumann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matthew</forename><surname>Mcdermott</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 2nd Clinical Natural Language Processing Workshop</title>\n",
       " <meeting>the 2nd Clinical Natural Language Processing Workshop</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b2\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Cloze-driven pretraining of self-attention networks</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alexei</forename><surname>Baevski</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sergey</forename><surname>Edunov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yinhan</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Michael</forename><surname>Auli</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">EMNLP</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b3\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">SciB-ERT: A pretrained language model for scientific text</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Iz</forename><surname>Beltagy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kyle</forename><surname>Lo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Arman</forename><surname>Cohan</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">EMNLP</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b4\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Longformer: The long-document transformer</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Iz</forename><surname>Beltagy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matthew</forename><forename type=\"middle\">E</forename><surname>Peters</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Arman</forename><surname>Cohan</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:2004.05150</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b5\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Focused Crawling: A New Approach to Topic-Specific Web Resource Discovery</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Soumen</forename><surname>Chakrabarti</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Martin</forename><surname>Van Den</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Byron</forename><surname>Berg</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Dom</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Comput. Networks</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">31</biblScope>\n",
       " <biblScope from=\"1623\" to=\"1640\" unit=\"page\"/>\n",
       " <date type=\"published\" when=\"1999\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b6\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">IMHO fine-tuning improves claim detection</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tuhin</forename><surname>Chakrabarty</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><surname>Hidey</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kathy</forename><surname>Mckeown</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NAACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b7\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">One billion word benchmark for measuring progress in statistical language modeling</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ciprian</forename><surname>Chelba</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tomas</forename><surname>Mikolov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Michael</forename><surname>Schuster</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Qi</forename><surname>Ge</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Thorsten</forename><surname>Brants</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Phillipp</forename><surname>Koehn</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tony</forename><surname>Robinson</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">INTERSPEECH</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b8\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">An embarrassingly simple approach for transfer learning from pretrained language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alexandra</forename><surname>Chronopoulou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christos</forename><surname>Baziotis</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alexandros</forename><surname>Potamianos</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NAACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b9\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Pretrained language models for sequential sentence classification</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Arman</forename><surname>Cohan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Iz</forename><surname>Beltagy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Daniel</forename><surname>King</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bhavana</forename><surname>Dalvi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dan</forename><surname>Weld</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">EMNLP</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b10\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Using similarity measures to select pretraining data for NER</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xiang</forename><surname>Dai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sarvnaz</forename><surname>Karimi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ben</forename><surname>Hachey</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Cecile</forename><surname>Paris</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NAACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b11\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Pubmed 200k RCT: a dataset for sequential sentence classification in medical abstracts</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Franck</forename><surname>Dernoncourt</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ji</forename><forename type=\"middle\">Young</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">IJCNLP</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b12\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">BERT: Pre-training of deep bidirectional transformers for language understanding</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jacob</forename><surname>Devlin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ming-Wei</forename><surname>Chang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kristina</forename><surname>Toutanova</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NAACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b13\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Show your work: Improved reporting of experimental results</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jesse</forename><surname>Dodge</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Suchin</forename><surname>Gururangan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dallas</forename><surname>Card</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Roy</forename><surname>Schwartz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noah A</forename><surname>Smith</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">EMNLP</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b14\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Diffusion of lexical change in social media</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jacob</forename><surname>Eisenstein</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">O'</forename><surname>Brendan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noah</forename><forename type=\"middle\">A</forename><surname>Connor</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Eric</forename><forename type=\"middle\">P</forename><surname>Smith</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Xing</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">PloS ONE</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2014\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b15\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">AllenNLP: A deep semantic natural language processing platform</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matt</forename><surname>Gardner</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Joel</forename><surname>Grus</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mark</forename><surname>Neumann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Oyvind</forename><surname>Tafjord</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pradeep</forename><surname>Dasigi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nelson</forename><forename type=\"middle\">F</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matthew</forename><surname>Peters</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Michael</forename><surname>Schmitz</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n",
       " </author>\n",
       " <editor>NLP-OSS</editor>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b16\">\n",
       " <monogr>\n",
       " <title/>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Aaron</forename><surname>Gokaslan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Vanya</forename><surname>Cohen</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note type=\"report_type\">OpenWeb-Text Corpus</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b17\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Variational pretraining for semi-supervised text classification</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Suchin</forename><surname>Gururangan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tam</forename><surname>Dang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dallas</forename><surname>Card</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noah</forename><forename type=\"middle\">A</forename><surname>Smith</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b18\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Unsupervised domain adaptation of contextualized embeddings for sequence labeling</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xiaochuang</forename><surname>Han</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jacob</forename><surname>Eisenstein</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">EMNLP</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b19\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ruining</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Julian</forename><surname>Mcauley</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">WWW</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b20\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matthew</forename><surname>Honnibal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ines</forename><surname>Montani</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b21\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Universal language model fine-tuning for text classification</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeremy</forename><surname>Howard</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sebastian</forename><surname>Ruder</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b22\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kexin</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jaan</forename><surname>Altosaar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rajesh</forename><surname>Ranganath</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1904.05342</idno>\n",
       " <title level=\"m\">ClinicalBERT: Modeling clinical notes and predicting hospital readmission</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b23\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Billion-scale similarity search with gpus</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jeff</forename><surname>Johnson</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matthijs</forename><surname>Douze</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">HervÃ©</forename><surname>JÃ©gou</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">IEEE Transactions on Big Data</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b24\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Measuring the evolution of a scientific field through citation frames</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">David</forename><surname>Jurgens</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Srijan</forename><surname>Kumar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Raine</forename><surname>Hoover</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Daniel</forename><forename type=\"middle\">A</forename><surname>Mcfarland</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dan</forename><surname>Jurafsky</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " <publisher>TACL</publisher>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b25\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Generalization through memorization: Nearest neighbor language models</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Urvashi</forename><surname>Khandelwal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Omer</forename><surname>Levy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dan</forename><surname>Jurafsky</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mike</forename><surname>Lewis</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>In ICLR. To appear</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b26\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">SemEval-2019 Task 4: Hyperpartisan news detection</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Johannes</forename><surname>Kiesel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Maria</forename><surname>Mestre</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rishabh</forename><surname>Shukla</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Emmanuel</forename><surname>Vincent</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Payam</forename><surname>Adineh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">David</forename><surname>Corney</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Benno</forename><surname>Stein</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Martin</forename><surname>Potthast</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>In Se-mEval</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b27\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Adam: A method for stochastic optimization</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">P</forename><surname>Diederik</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jimmy</forename><surname>Kingma</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Ba</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>In ICLR</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b28\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Overview of the biocreative vi chemical-protein interaction track</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Martin</forename><surname>Krallinger</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Obdulia</forename><surname>Rabal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ahmad</forename><surname>Saber</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">MartÃ­n</forename><surname>Akhondi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">JÃ©sÃºs</forename><forename type=\"middle\">LÃ³pez</forename><surname>PÃ©rez PÃ©rez</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Gael</forename><forename type=\"middle\">PÃ©rez</forename><surname>SantamarÃ­a</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Georgios</forename><surname>RodrÃ­guez</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ander</forename><surname>Tsatsaronis</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">JosÃ©</forename><surname>Intxaurrondo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Antonio Baso</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Umesh</forename><surname>LÃ³pez</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">E</forename><forename type=\"middle\">M</forename><surname>Nandal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">A</forename><forename type=\"middle\">Poorna</forename><surname>Van Buel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Marleen</forename><surname>Chandrasekhar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Astrid</forename><surname>Rodenburg</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Marius</forename><forename type=\"middle\">A</forename><surname>Laegreid</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Julen</forename><surname>Doornenbal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">AnÃ¡lia</forename><surname>OyarzÃ¡bal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alfonso</forename><surname>Loureno</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Valencia</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the BioCreative VI Workshop</title>\n",
       " <meeting>the BioCreative VI Workshop</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b29\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">The chemdner corpus of chemicals and drugs and its annotation principles</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Martin</forename><surname>Krallinger</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Obdulia</forename><surname>Rabal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Florian</forename><surname>Leitner</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Miguel</forename><surname>Vazquez</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">David</forename><surname>Salgado</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhiyong</forename><surname>Lu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Robert</forename><surname>Leaman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yanan</forename><surname>Lu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Donghong</forename><surname>Ji</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">M</forename><surname>Daniel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Lowe</surname></persName>\n",
       " </author>\n",
       " <idno type=\"DOI\">10.1186/1758-2946-7-S1-S2</idno>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Journal of cheminformatics</title>\n",
       " <imprint>\n",
       " <biblScope unit=\"volume\">7</biblScope>\n",
       " <biblScope unit=\"issue\">1</biblScope>\n",
       " <biblScope unit=\"page\">S2</biblScope>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b30\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">ChemProt-3.0: a global chemical biology diseases mapping</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jens</forename><surname>Kringelum</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sonny</forename><forename type=\"middle\">Kim</forename><surname>Kjaerulff</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">SÃ¸ren</forename><surname>Brunak</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ole</forename><surname>Lund</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tudor</forename><forename type=\"middle\">I</forename><surname>Oprea</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Olivier</forename><surname>Taboureau</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Database</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b31\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Genres, registers, text types, domains and styles: Clarifying the concepts and navigating a path through the BNC jungle</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Y</forename><forename type=\"middle\">W</forename><surname>David</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Lee</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Language Learning &amp; Technology</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2001\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b32\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">BioBERT: A pre-trained biomedical language representation model for biomedical text mining</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jinhyuk</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wonjin</forename><surname>Yoon</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sungdong</forename><surname>Kim</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Donghyeon</forename><surname>Kim</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sunkyu</forename><surname>Kim</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chan</forename><surname>Ho So</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jaewoo</forename><surname>Kang</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"j\">Bioinformatics</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b33\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yinhan</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Myle</forename><surname>Ott</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Naman</forename><surname>Goyal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jingfei</forename><surname>Du</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mandar</forename><surname>Joshi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Danqi</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Omer</forename><surname>Levy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mike</forename><surname>Lewis</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Veselin</forename><surname>Stoyanov</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1907.11692</idno>\n",
       " <title level=\"m\">RoBERTa: A robustly optimized BERT pretraining approach</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b34\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">S2ORC: The Semantic Scholar Open Research Corpus</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kyle</forename><surname>Lo</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lucy</forename><forename type=\"middle\">Lu</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mark</forename><surname>Neumann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rodney</forename><surname>Kinney</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Daniel</forename><forename type=\"middle\">S</forename><surname>Weld</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>In ACL. To appear</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b35\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Zero-shot entity linking by reading entity descriptions</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lajanugen</forename><surname>Logeswaran</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ming-Wei</forename><surname>Chang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kristina</forename><surname>Toutanova</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jacob</forename><surname>Devlin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Honglak</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b36\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yi</forename><surname>Luan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Luheng</forename><surname>He</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mari</forename><surname>Ostendorf</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hannaneh</forename><surname>Hajishirzi</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">EMNLP</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b37\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Learning word vectors for sentiment analysis</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrew</forename><forename type=\"middle\">L</forename><surname>Maas</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Raymond</forename><forename type=\"middle\">E</forename><surname>Daly</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Peter</forename><forename type=\"middle\">T</forename><surname>Pham</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dan</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Andrew</forename><forename type=\"middle\">Y</forename><surname>Ng</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><surname>Potts</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2011\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b38\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Image-based recommendations on styles and substitutes</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Julian</forename><surname>Mcauley</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christopher</forename><surname>Targett</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Qinfeng</forename><surname>Shi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Anton</forename><surname>Van Den</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Hengel</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ACM SI-GIR</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b39\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Exploring ways to incorporate additional knowledge to improve natural language commonsense question answering</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Arindam</forename><surname>Mitra</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pratyay</forename><surname>Banerjee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kuntal</forename><surname>Kumar Pal</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chitta</forename><surname>Swaroop Ranjan Mishra</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Baral</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1909.08855v3</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2020\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b40\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Intelligent selection of language model training data</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">C</forename><surname>Robert</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">William</forename><surname>Moore</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Lewis</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2010\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b41\">\n",
       " <monogr>\n",
       " <title/>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sebastian</forename><surname>Nagel</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " <publisher>CC-NEWS</publisher>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b42\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Scispacy: Fast and robust models for biomedical natural language processing</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mark</forename><surname>Neumann</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Daniel</forename><surname>King</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Iz</forename><surname>Beltagy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Waleed</forename><surname>Ammar</surname></persName>\n",
       " </author>\n",
       " <idno type=\"DOI\">10.18653/v1/W19-5034</idno>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Proceedings of the 18th BioNLP Workshop and Shared Task</title>\n",
       " <meeting>the 18th BioNLP Workshop and Shared Task</meeting>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b43\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">To tune or not to tune? Adapting pretrained representations to diverse tasks</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matthew</forename><forename type=\"middle\">E</forename><surname>Peters</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sebastian</forename><surname>Ruder</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noah</forename><forename type=\"middle\">A</forename><surname>Smith</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">RepL4NLP</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b44\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Bowman</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jason</forename><surname>Phang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Thibault</forename><surname>FÃ©vry</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samuel</forename><forename type=\"middle\">R</forename></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1811.01088</idno>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b45\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">What to do about non-standard (or non-canonical) language in NLP</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Barbara</forename><surname>Plank</surname></persName>\n",
       " </author>\n",
       " <editor>KONVENS</editor>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b46\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Improving language understanding by generative pre-training</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alec</forename><surname>Radford</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Karthik</forename><surname>Narasimhan</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>Tim Salimans, and Ilya Sutskever</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b47\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Exploring the limits of transfer learning with a unified text-to-text transformer</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Colin</forename><surname>Raffel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noam</forename><surname>Shazeer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Adam</forename><forename type=\"middle\">Kaleo</forename><surname>Roberts</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Katherine</forename><surname>Lee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sharan</forename><surname>Narang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Michael</forename><surname>Matena</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yanqi</forename><surname>Zhou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wei</forename><surname>Li</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Peter</forename><forename type=\"middle\">J</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1910.10683</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b48\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Domain-Specific Corpus Expansion with Focused Webcrawling</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Steffen</forename><surname>Remus</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chris</forename><surname>Biemann</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">LREC</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b49\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Towards a continuous modeling of natural language domains</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sebastian</forename><surname>Ruder</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Parsa</forename><surname>Ghaffari</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">John</forename><forename type=\"middle\">G</forename><surname>Breslin</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">Workshop on Uphill Battles in Language Processing: Scaling Early Achievements to Robust Methods</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b50\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Learning to select data for transfer learning with Bayesian optimization</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sebastian</forename><surname>Ruder</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Barbara</forename><surname>Plank</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">EMNLP</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b51\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Victor</forename><surname>Sanh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lysandre</forename><surname>Debut</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Julien</forename><surname>Chaumond</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Thomas</forename><surname>Wolf</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">EMC2 @ NeurIPS</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b52\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">How to fine-tune BERT for text classification</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chi</forename><surname>Sun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xipeng</forename><surname>Qiu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yige</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xuanjing</forename><surname>Huang</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">CCL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b53\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Swabha</forename><surname>Swayamdipta</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Matthew</forename><surname>Peters</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Brendan</forename><surname>Roof</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chris</forename><surname>Dyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noah A</forename><surname>Smith</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1908.11047</idno>\n",
       " <title level=\"m\">Shallow syntax in deep water</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b54\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Sentiment classification using document embeddings trained with cosine similarity</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tan</forename><surname>Thongtan</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tanasanee</forename><surname>Phienthrakul</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ACL SRW</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b55\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">A simple method for commonsense reasoning</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">H</forename><surname>Trieu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Quoc</forename><forename type=\"middle\">V</forename><surname>Trinh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Le</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1806.02847</idno>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b56\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Attention is all you need</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ashish</forename><surname>Vaswani</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Noam</forename><surname>Shazeer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Niki</forename><surname>Parmar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jakob</forename><surname>Uszkoreit</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Llion</forename><surname>Jones</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Aidan</forename><forename type=\"middle\">N</forename><surname>Gomez</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Åukasz</forename><surname>Kaiser</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Illia</forename><surname>Polosukhin</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NeurIPS</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2017\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b57\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">SuperGLUE: A stickier benchmark for general-purpose language understanding systems</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yada</forename><surname>Pruksachatkun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nikita</forename><surname>Nangia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Amanpreet</forename><surname>Singh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Julian</forename><surname>Michael</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Felix</forename><surname>Hill</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Omer</forename><surname>Levy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samuel</forename><forename type=\"middle\">R</forename><surname>Bowman</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NeurIPS</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b58\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Alex</forename><surname>Wang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Amanpreet</forename><surname>Singh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Julian</forename><surname>Michael</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Felix</forename><surname>Hill</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Omer</forename><surname>Levy</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samuel</forename><surname>Bowman</surname></persName>\n",
       " </author>\n",
       " <editor>Black-boxNLP @ EMNLP</editor>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b59\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">What's in a domain? Analyzing genre and topic differences in statistical machine translation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Arianna</forename><surname>Marlies Van Der Wees</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wouter</forename><surname>Bisazza</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Christof</forename><surname>Weerkamp</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Monz</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b60\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">A broad-coverage challenge corpus for sentence understanding through inference</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Adina</forename><surname>Williams</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Nikita</forename><surname>Nangia</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Samuel</forename><surname>Bowman</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NAACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2018\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b61\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Thomas</forename><surname>Wolf</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lysandre</forename><surname>Debut</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Victor</forename><surname>Sanh</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Julien</forename><surname>Chaumond</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Clement</forename><surname>Delangue</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Anthony</forename><surname>Moi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pierric</forename><surname>Cistac</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Tim</forename><surname>Rault</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rmi</forename><surname>Louf</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1910.03771</idno>\n",
       " <title level=\"m\">Morgan Funtowicz, and Jamie Brew. 2019. HuggingFace's Transformers: State-of-the-art natural language processing</title>\n",
       " <imprint/>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b62\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Google's neural machine translation system: Bridging the gap between human and machine translation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yonghui</forename><surname>Wu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mike</forename><surname>Schuster</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhifeng</forename><surname>Chen</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><surname>Quoc</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mohammad</forename><surname>Le</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wolfgang</forename><surname>Norouzi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Maxim</forename><surname>Macherey</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yuan</forename><surname>Krikun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Qin</forename><surname>Cao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Klaus</forename><surname>Gao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Macherey</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2016\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b63\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">BERT post-training for review reading comprehension and aspect-based sentiment analysis</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hu</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bing</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lei</forename><surname>Shu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Philip</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NAACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b64\">\n",
       " <monogr>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hu</forename><surname>Xu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Bing</forename><surname>Liu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lei</forename><surname>Shu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Philip</forename><forename type=\"middle\">S</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <idno type=\"arXiv\">arXiv:1902.00821v2</idno>\n",
       " <title level=\"m\">Review conversational reading comprehension</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b65\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">XLNet: Generalized autoregressive pretraining for language understanding</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zhilin</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Zihang</forename><surname>Dai</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yiming</forename><surname>Yang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jaime</forename><forename type=\"middle\">G</forename><surname>Carbonell</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ruslan</forename><surname>Salakhutdinov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">V</forename><surname>Quoc</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><surname>Le</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NeurIPS</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b66\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Learning and evaluating general linguistic intelligence</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Dani</forename><surname>Yogatama</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Cyprien</forename><surname>De Masson D'autume</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Jerome</forename><surname>Connor</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">TomÃ¡s</forename><surname>KociskÃ½</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Mike</forename><surname>Chrzanowski</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lingpeng</forename><surname>Kong</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Angeliki</forename><surname>Lazaridou</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Wang</forename><surname>Ling</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Lei</forename><surname>Yu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Chris</forename><surname>Dyer</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Phil</forename><surname>Blunsom</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b67\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Defending against neural fake news</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Rowan</forename><surname>Zellers</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ari</forename><surname>Holtzman</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Hannah</forename><surname>Rashkin</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yonatan</forename><surname>Bisk</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ali</forename><surname>Farhadi</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Franziska</forename><surname>Roesner</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yejin</forename><surname>Choi</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NeurIPS</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b68\">\n",
       " <monogr>\n",
       " <title level=\"m\" type=\"main\">Character-level convolutional networks for text classification</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xiang</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Junbo</forename><forename type=\"middle\">Jake</forename><surname>Zhao</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yann</forename><surname>Lecun</surname></persName>\n",
       " </author>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " <note>In NeurIPS</note>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b69\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Curriculum learning for domain adaptation in neural machine translation</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Xuan</forename><surname>Zhang</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Pamela</forename><surname>Shapiro</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Gaurav</forename><surname>Kumar</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Paul</forename><surname>Mc-Namee</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Marine</forename><surname>Carpuat</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Kevin</forename><surname>Duh</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">NAACL</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2019\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " <biblStruct xml:id=\"b70\">\n",
       " <analytic>\n",
       " <title level=\"a\" type=\"main\">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Yukun</forename><surname>Zhu</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ryan</forename><surname>Kiros</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Richard</forename><forename type=\"middle\">S</forename><surname>Zemel</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Ruslan</forename><surname>Salakhutdinov</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Raquel</forename><surname>Urtasun</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Antonio</forename><surname>Torralba</surname></persName>\n",
       " </author>\n",
       " <author>\n",
       " <persName><forename type=\"first\">Sanja</forename><surname>Fidler</surname></persName>\n",
       " </author>\n",
       " </analytic>\n",
       " <monogr>\n",
       " <title level=\"m\">ICCV</title>\n",
       " <imprint>\n",
       " <date type=\"published\" when=\"2015\"/>\n",
       " </imprint>\n",
       " </monogr>\n",
       " </biblStruct>\n",
       " </listBibl>\n",
       " </div>\n",
       " </back>\n",
       " </text>\n",
       " </TEI>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"teiHeader\"\n",
    "element = \"author\"\n",
    "type = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = None\n",
    "element = \"abstract\"\n",
    "type = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<abstract>\n",
       " <div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p><p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p></div>\n",
       " </abstract>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "extract_element_soup(files[i],dir,element,type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cosine_similarity\n",
    "\n",
    "def calculate_similarity(text1, text2, model, tokenizer):\n",
    "    \"\"\"Calculate the similarity between 2 texts\"\"\"\n",
    "    tokens1, tokens2 = tokenizer(text1, return_tensors='pt'), tokenizer(text2, return_tensors='pt')\n",
    "    vec1 = model(tokens1['input_ids'],\n",
    "                       tokens1['attention_mask']).last_hidden_state[-1].squeeze().mean(axis=0)\n",
    "    vec2 = model(tokens2['input_ids'],\n",
    "                       tokens2['attention_mask']).last_hidden_state[-1].squeeze().mean(axis=0)\n",
    "    \n",
    "    return cosine_similarity(vec1.reshape(1,-1), vec2.reshape(1,-1)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p><p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p><p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p><p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p><p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p><p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p><p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p><p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p><p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p><p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.</p><p>In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.</p><p>In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.</p><p>In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.</p><p>In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.</p><p>In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.</p><p>In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.</p><p>In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.</p><p>In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.</p><p>In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   </p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   </p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   </p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   </p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   </p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   </p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   </p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   </p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   </p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.</p><p>â€  Work performed while at Google Brain. â€¡ Work performed while at Google Research.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.</p><p>â€  Work performed while at Google Brain. â€¡ Work performed while at Google Research.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.</p><p>â€  Work performed while at Google Brain. â€¡ Work performed while at Google Research.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.</p><p>â€  Work performed while at Google Brain. â€¡ Work performed while at Google Research.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.</p><p>â€  Work performed while at Google Brain. â€¡ Work performed while at Google Research.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.</p><p>â€  Work performed while at Google Brain. â€¡ Work performed while at Google Research.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.</p><p>â€  Work performed while at Google Brain. â€¡ Work performed while at Google Research.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.</p><p>â€  Work performed while at Google Brain. â€¡ Work performed while at Google Research.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.</p><p>â€  Work performed while at Google Brain. â€¡ Work performed while at Google Research.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p></div>\n",
      "</abstract>]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# {\n",
    "#     'paper1': 0,\n",
    "#     'paper2': 1,\n",
    "#     'value': 0.68\n",
    "# }\n",
    "\n",
    "resultados=[]\n",
    "\n",
    "for idx in range(len(files)):\n",
    "    for idy in range(len(files)):\n",
    "        if(idx==idy):\n",
    "            continue\n",
    "\n",
    "        paper_1 = extract_element_soup(files[idx],dir,element,type)\n",
    "        print(paper_1)\n",
    "        resultado_1 =\"\"\n",
    "        for element_text in paper_1:\n",
    "            resultado_1 = resultado_1 + \" \" + element_text.text\n",
    "            \n",
    "        paper_2 = extract_element_soup(files[idy],dir,element,type)\n",
    "\n",
    "        resultado_2 =\"\"\n",
    "        for element_text in paper_2:\n",
    "            resultado_2 = resultado_2 + \" \" + element_text.text\n",
    "\n",
    "        result_dict = {}\n",
    "        result_dict['paper1'] = idx\n",
    "        result_dict['paper2'] = idy\n",
    "        result_dict['degree'] = calculate_similarity(resultado_1, resultado_2, model, tokenizer)\n",
    "        \n",
    "        resultados.append((resultado_1, resultado_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcociccalebaztan/miniconda3/envs/testing_pdf/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Instalar PyTorch\n",
    "\n",
    "# pip install torch torchvision torchaudio\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cosine_similarity\n",
    "\n",
    "def calculate_similarity(text1, text2, model, tokenizer):\n",
    "    tokens1, tokens2 = tokenizer(text1, return_tensors='pt'), tokenizer(text2, return_tensors='pt')\n",
    "    vec1 = model(tokens1['input_ids'],\n",
    "                       tokens1['attention_mask']).last_hidden_state[-1].squeeze().mean(axis=0)\n",
    "    vec2 = model(tokens2['input_ids'],\n",
    "                       tokens2['attention_mask']).last_hidden_state[-1].squeeze().mean(axis=0)\n",
    "    \n",
    "    return cosine_similarity(vec1.reshape(1,-1), vec2.reshape(1,-1)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6244330406188965"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_similarity(extract_element_soup(files[0], None, 'abstract', None)[0].text,\n",
    "                     extract_element_soup(files[1], None, 'abstract', None)[0].text,\n",
    "                     model,\n",
    "                     tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' \\nWe introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a;Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\n',\n",
       " ' \\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.\\n')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for par_texto in resultados:\n",
    "    texto_1 =par_texto[0]\n",
    "    texto_2 =par_texto[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAcknowledgmentsThe authors thank Dallas Card, Mark Neumann, Nelson Liu, Eric Wallace, members of the Al-lenNLP team, and anonymous reviewers for helpful feedback, and Arman Cohan for providing data. This research was supported in part by the Office of Naval Research under the MURI grant N00014-18-1-2670. TPU machines for conducting experiments were provided by Google.\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_element_soup(files[2],dir,element,type)[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"token-classification\", model=\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_final_ner(org_viejo,orgs,type_ent):\n",
    "    org_viejo[\"type\"]=type_ent\n",
    "    if org_viejo[\"score\"] >0.6 and (org_viejo[\"name\"] not in [dict[\"name\"] for dict in orgs]):\n",
    "        orgs.append(org_viejo)\n",
    "    org_temp=dict()\n",
    "    org_temp[\"name\"]=\"\"\n",
    "    org_temp[\"score\"]=0\n",
    "    return org_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ners(files,i,pipe):\n",
    "    \n",
    "    acno = extract_element_soup(files[i],None,\"div\",\"acknowledgement\")\n",
    "    ners=[]\n",
    "    if len(acno)>0:\n",
    "        for elements in acno:\n",
    "            ners.extend(pipe(elements.text))\n",
    "\n",
    "\n",
    "\n",
    "    org_parser=False\n",
    "    misc_parser=False\n",
    "    continue_parser=True\n",
    "\n",
    "    org_temp=\"\"\n",
    "    org_temp=dict()\n",
    "    org_temp[\"name\"]=\"\"\n",
    "    org_temp[\"score\"]=0\n",
    "    org_temp[\"type\"]=\"\"\n",
    "    orgs=[]\n",
    "    pos_anterior=0\n",
    "    for ner in ners:\n",
    "        \n",
    "        \n",
    "        \n",
    "        if org_parser and continue_parser and (ner[\"entity\"]=='B-ORG') and ner[\"start\"]!=pos_anterior:\n",
    "            org_temp=case_final_ner(org_temp,orgs,\"ORG\")\n",
    "            org_parser=False\n",
    "            continue_parser=True\n",
    "        if misc_parser and continue_parser and (ner[\"entity\"]=='B-MISC') and ner[\"start\"]!=pos_anterior:\n",
    "            org_temp=case_final_ner(org_temp,orgs,\"MISC\")\n",
    "            org_parser=False\n",
    "            continue_parser=True \n",
    "            \n",
    "        if org_parser and not continue_parser and (ner[\"entity\"]!='I-ORG'):\n",
    "            org_temp=case_final_ner(org_temp,orgs,\"ORG\")\n",
    "            org_parser=False\n",
    "            continue_parser=True   \n",
    "        \n",
    "            \n",
    "        if misc_parser and not continue_parser and (ner[\"entity\"]!='I-MISC'):\n",
    "            org_temp=case_final_ner(org_temp,orgs,\"MISC\")\n",
    "            misc_parser=False\n",
    "            continue_parser=True\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        if ner[\"entity\"]=='B-ORG' and not org_parser:\n",
    "            org_parser=True\n",
    "            org_temp[\"name\"]=ner[\"word\"]\n",
    "            org_temp[\"score\"]=ner[\"score\"]\n",
    "            pos_anterior=ner[\"end\"]\n",
    "            continue\n",
    "        \n",
    "        if ner[\"entity\"]=='B-ORG' and org_parser and continue_parser:\n",
    "            \n",
    "            org_temp[\"name\"]=org_temp[\"name\"]+ner[\"word\"].replace(\"#\", \"\")\n",
    "            org_temp[\"score\"]=org_temp[\"score\"]*ner[\"score\"]\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if ner[\"entity\"]=='B-MISC' and not misc_parser:\n",
    "            misc_parser=True\n",
    "            \n",
    "            org_temp[\"name\"]=ner[\"word\"]\n",
    "            org_temp[\"score\"]=ner[\"score\"]\n",
    "            pos_anterior=ner[\"end\"]\n",
    "            continue\n",
    "        \n",
    "        if ner[\"entity\"]=='B-MISC' and misc_parser and continue_parser:\n",
    "            \n",
    "            org_temp[\"name\"]=org_temp[\"name\"]+ner[\"word\"].replace(\"#\", \"\")\n",
    "            org_temp[\"score\"]=org_temp[\"score\"]*ner[\"score\"]\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if ner[\"entity\"]=='I-ORG' and org_parser:\n",
    "            \n",
    "            \n",
    "            addit=\"\"\n",
    "            if pos_anterior+1 <= ner[\"start\"]:\n",
    "                addit=addit +\" \"\n",
    "            continue_parser=False\n",
    "            org_temp[\"name\"]=org_temp[\"name\"]+addit+ner[\"word\"].replace(\"#\", \"\")\n",
    "            org_temp[\"score\"]=org_temp[\"score\"]*ner[\"score\"]\n",
    "            pos_anterior=ner[\"end\"]\n",
    "            continue\n",
    "        \n",
    "\n",
    "        if ner[\"entity\"]=='I-MISC' and misc_parser:\n",
    "            \n",
    "            \n",
    "            addit=\"\"\n",
    "            if pos_anterior+1 <= ner[\"start\"]:\n",
    "                addit=addit +\" \"\n",
    "            continue_parser=False\n",
    "            org_temp[\"name\"]=org_temp[\"name\"]+addit+ner[\"word\"].replace(\"#\", \"\")\n",
    "            org_temp[\"score\"]=org_temp[\"score\"]*ner[\"score\"]\n",
    "            pos_anterior=ner[\"end\"]\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if org_parser:\n",
    "            \n",
    "            org_temp=case_final_ner(org_temp,orgs,\"ORG\")\n",
    "            misc_parser=False\n",
    "            continue_parser=True\n",
    "        \n",
    "    \n",
    "        if misc_parser:\n",
    "            org_temp=case_final_ner(org_temp,orgs,\"MISC\")\n",
    "            misc_parser=False\n",
    "            continue_parser=True\n",
    "            \n",
    "        org_parser=False\n",
    "        misc_parser=False\n",
    "        continue_parser=True\n",
    "\n",
    "    if org_parser:\n",
    "        \n",
    "        org_temp=case_final_ner(org_temp,orgs,\"ORG\")\n",
    "        misc_parser=False\n",
    "        continue_parser=True\n",
    "        \n",
    "    \n",
    "    if misc_parser:\n",
    "        org_temp=case_final_ner(org_temp,orgs,\"MISC\")\n",
    "        misc_parser=False\n",
    "        continue_parser=True   \n",
    "    return orgs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_projects(files,i):\n",
    "    \n",
    "    acno = extract_element_soup(files[i],None,\"div\",\"acknowledgement\")\n",
    "    texts =\"\"\n",
    "    if len(acno)>0:\n",
    "        for elements in acno:\n",
    "            texts = texts + \" \"+ elements.text\n",
    "       \n",
    "    return re.findall(r'\\b#?[A-Z\\d-]+(?:-\\d+){3,}\\b', texts),re.findall(r'(\\b[A-Z\\d&-]+\\b)\\s*(?:award[s]?|grant)\\s*#?[A-Z\\d-]+(?:-\\d+){3,}\\b',texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['N00014-18-1-2670'], ['MURI'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 9\n",
    "extract_projects(files,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#W911NF-13-1-0246', 'ONR'), ('#N00014-13-1-0023', 'ONR'), ('N00014-18-1-2670', 'MURI')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "texto = '''\n",
    "This work was supported in part by ONR awards #W911NF-13-1-0246 and #N00014-13-1-0023, and in part by TerraSwarm. This research was supported in part by the Office of Naval Research under the MURI grant N00014-18-1-2670.\n",
    "'''\n",
    "\n",
    "# Definimos un patrÃ³n para buscar nombres y sus respectivos identificadores con #\n",
    "patron_awards = r'([A-Z\\d&-]+)\\s*awards\\s*((?:#?[A-Z\\d-]+(?:-\\d+){3,}(?:\\s*and\\s*#?[A-Z\\d-]+(?:-\\d+){3,})*)+)\\b'\n",
    "\n",
    "# Definimos un patrÃ³n para buscar nombres y sus respectivos identificadores con grant\n",
    "patron_grant = r'([A-Z\\d&-]+)\\s*grant\\s*((?:#?[A-Z\\d-]+(?:-\\d+){3,}))\\b'\n",
    "\n",
    "# Encontramos todas las coincidencias con awards\n",
    "resultados_awards = re.findall(patron_awards, texto)\n",
    "\n",
    "# Encontramos todas las coincidencias con grant\n",
    "resultados_grant = re.findall(patron_grant, texto)\n",
    "\n",
    "# Creamos una lista de tuplas para almacenar los resultados\n",
    "resultado_final = []\n",
    "\n",
    "# FunciÃ³n para agregar resultados a la lista final\n",
    "def agregar_resultados(resultados):\n",
    "    for match in resultados:\n",
    "        nombre = match[0]\n",
    "        identificadores = match[1].split(' and ')\n",
    "        for identificador in identificadores:\n",
    "            resultado_final.append((identificador, nombre))\n",
    "\n",
    "# Agregamos los resultados de awards\n",
    "agregar_resultados(resultados_awards)\n",
    "\n",
    "# Agregamos los resultados de grant\n",
    "for match in resultados_grant:\n",
    "    nombre = match[0]\n",
    "    identificador = match[1]\n",
    "    resultado_final.append((identificador, nombre))\n",
    "\n",
    "print(resultado_final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Al-lenNLP', 'score': 0.6749817, 'type': 'ORG'},\n",
       " {'name': 'Office of Naval Research', 'score': 0.99672663, 'type': 'ORG'},\n",
       " {'name': 'Google', 'score': 0.99887425, 'type': 'ORG'}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_ners(files,i,pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks to Jakub Pachocki and Szymon Sidor for suggesting tasks, and Greg Brockman, Michael Petrov, Brooke Chan, and Chelsea Voss for helping run evaluations on OpenAI's infrastructure. Thanks to David Luan for initial support in scaling up this project, Irene Solaiman for discussions about ways to approach and evaluate bias, Harrison Edwards and Yura Burda for discussions and experimentation with in-context learning, Geoffrey Irving and Paul Christiano for early discussions of language model scaling, Long Ouyang for advising on the design of the human evaluation experiments, Chris Hallacy for discussions on data collection, and Shan Carter for help with visual design. Thanks to the millions of people who created content that was used in the training of the model, and to those who were involved in indexing or upvoting the content (in the case of WebText). Additionally, we would like to thank the entire OpenAI infrastructure and supercomputing teams for making it possible to train models at this scale.\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_element_soup(files[i],dir,element,type)[0].p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': 0.99979836,\n",
       "  'index': 15,\n",
       "  'word': 'Ryan',\n",
       "  'start': 49,\n",
       "  'end': 53},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9997751,\n",
       "  'index': 16,\n",
       "  'word': 'Lowe',\n",
       "  'start': 54,\n",
       "  'end': 58},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9995851,\n",
       "  'index': 30,\n",
       "  'word': 'J',\n",
       "  'start': 122,\n",
       "  'end': 123},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.93610704,\n",
       "  'index': 31,\n",
       "  'word': '##ak',\n",
       "  'start': 123,\n",
       "  'end': 125},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.994306,\n",
       "  'index': 32,\n",
       "  'word': '##ub',\n",
       "  'start': 125,\n",
       "  'end': 127},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9998161,\n",
       "  'index': 33,\n",
       "  'word': 'Pac',\n",
       "  'start': 128,\n",
       "  'end': 131},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99935037,\n",
       "  'index': 34,\n",
       "  'word': '##ho',\n",
       "  'start': 131,\n",
       "  'end': 133},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99779344,\n",
       "  'index': 35,\n",
       "  'word': '##cki',\n",
       "  'start': 133,\n",
       "  'end': 136},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9995432,\n",
       "  'index': 37,\n",
       "  'word': 'S',\n",
       "  'start': 141,\n",
       "  'end': 142},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9842854,\n",
       "  'index': 38,\n",
       "  'word': '##zy',\n",
       "  'start': 142,\n",
       "  'end': 144},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.98108345,\n",
       "  'index': 39,\n",
       "  'word': '##mon',\n",
       "  'start': 144,\n",
       "  'end': 147},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99975055,\n",
       "  'index': 40,\n",
       "  'word': 'Sid',\n",
       "  'start': 148,\n",
       "  'end': 151},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.96134436,\n",
       "  'index': 41,\n",
       "  'word': '##or',\n",
       "  'start': 151,\n",
       "  'end': 153},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.99978906,\n",
       "  'index': 47,\n",
       "  'word': 'Greg',\n",
       "  'start': 180,\n",
       "  'end': 184},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99977845,\n",
       "  'index': 48,\n",
       "  'word': 'Brock',\n",
       "  'start': 185,\n",
       "  'end': 190},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.999348,\n",
       "  'index': 49,\n",
       "  'word': '##man',\n",
       "  'start': 190,\n",
       "  'end': 193},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9998023,\n",
       "  'index': 51,\n",
       "  'word': 'Michael',\n",
       "  'start': 195,\n",
       "  'end': 202},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9997756,\n",
       "  'index': 52,\n",
       "  'word': 'Pet',\n",
       "  'start': 203,\n",
       "  'end': 206},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99942243,\n",
       "  'index': 53,\n",
       "  'word': '##rov',\n",
       "  'start': 206,\n",
       "  'end': 209},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.99975866,\n",
       "  'index': 55,\n",
       "  'word': 'Brooke',\n",
       "  'start': 211,\n",
       "  'end': 217},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99973685,\n",
       "  'index': 56,\n",
       "  'word': 'Chan',\n",
       "  'start': 218,\n",
       "  'end': 222},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9997445,\n",
       "  'index': 59,\n",
       "  'word': 'Chelsea',\n",
       "  'start': 228,\n",
       "  'end': 235},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9996543,\n",
       "  'index': 60,\n",
       "  'word': 'V',\n",
       "  'start': 236,\n",
       "  'end': 237},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9994543,\n",
       "  'index': 61,\n",
       "  'word': '##oss',\n",
       "  'start': 237,\n",
       "  'end': 240},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.99268204,\n",
       "  'index': 68,\n",
       "  'word': 'Open',\n",
       "  'start': 272,\n",
       "  'end': 276},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.97998464,\n",
       "  'index': 69,\n",
       "  'word': '##A',\n",
       "  'start': 276,\n",
       "  'end': 277},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9895727,\n",
       "  'index': 70,\n",
       "  'word': '##I',\n",
       "  'start': 277,\n",
       "  'end': 278},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.99977833,\n",
       "  'index': 77,\n",
       "  'word': 'David',\n",
       "  'start': 307,\n",
       "  'end': 312},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99973184,\n",
       "  'index': 78,\n",
       "  'word': 'Lu',\n",
       "  'start': 313,\n",
       "  'end': 315},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.991676,\n",
       "  'index': 79,\n",
       "  'word': '##an',\n",
       "  'start': 315,\n",
       "  'end': 317},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9997328,\n",
       "  'index': 91,\n",
       "  'word': 'Irene',\n",
       "  'start': 366,\n",
       "  'end': 371},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99973327,\n",
       "  'index': 92,\n",
       "  'word': 'Sol',\n",
       "  'start': 372,\n",
       "  'end': 375},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99529296,\n",
       "  'index': 93,\n",
       "  'word': '##ai',\n",
       "  'start': 375,\n",
       "  'end': 377},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99432445,\n",
       "  'index': 94,\n",
       "  'word': '##man',\n",
       "  'start': 377,\n",
       "  'end': 380},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9997532,\n",
       "  'index': 105,\n",
       "  'word': 'Harrison',\n",
       "  'start': 439,\n",
       "  'end': 447},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9996686,\n",
       "  'index': 106,\n",
       "  'word': 'Edwards',\n",
       "  'start': 448,\n",
       "  'end': 455},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9996786,\n",
       "  'index': 108,\n",
       "  'word': 'Yu',\n",
       "  'start': 460,\n",
       "  'end': 462},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.99922764,\n",
       "  'index': 109,\n",
       "  'word': '##ra',\n",
       "  'start': 462,\n",
       "  'end': 464},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99965084,\n",
       "  'index': 110,\n",
       "  'word': 'B',\n",
       "  'start': 465,\n",
       "  'end': 466},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.96306765,\n",
       "  'index': 111,\n",
       "  'word': '##ur',\n",
       "  'start': 466,\n",
       "  'end': 468},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9759705,\n",
       "  'index': 112,\n",
       "  'word': '##da',\n",
       "  'start': 468,\n",
       "  'end': 470},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.99976736,\n",
       "  'index': 123,\n",
       "  'word': 'Geoffrey',\n",
       "  'start': 533,\n",
       "  'end': 541},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99972445,\n",
       "  'index': 124,\n",
       "  'word': 'Irving',\n",
       "  'start': 542,\n",
       "  'end': 548},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.99974984,\n",
       "  'index': 126,\n",
       "  'word': 'Paul',\n",
       "  'start': 553,\n",
       "  'end': 557},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9997532,\n",
       "  'index': 127,\n",
       "  'word': 'Christian',\n",
       "  'start': 558,\n",
       "  'end': 567},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9247357,\n",
       "  'index': 128,\n",
       "  'word': '##o',\n",
       "  'start': 567,\n",
       "  'end': 568},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.99978465,\n",
       "  'index': 139,\n",
       "  'word': 'Long',\n",
       "  'start': 618,\n",
       "  'end': 622},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9996934,\n",
       "  'index': 140,\n",
       "  'word': 'O',\n",
       "  'start': 623,\n",
       "  'end': 624},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99843997,\n",
       "  'index': 141,\n",
       "  'word': '##uy',\n",
       "  'start': 624,\n",
       "  'end': 626},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99650383,\n",
       "  'index': 142,\n",
       "  'word': '##ang',\n",
       "  'start': 626,\n",
       "  'end': 629},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9997676,\n",
       "  'index': 154,\n",
       "  'word': 'Chris',\n",
       "  'start': 694,\n",
       "  'end': 699},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9997309,\n",
       "  'index': 155,\n",
       "  'word': 'Hall',\n",
       "  'start': 700,\n",
       "  'end': 704},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99950194,\n",
       "  'index': 156,\n",
       "  'word': '##acy',\n",
       "  'start': 704,\n",
       "  'end': 707},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9997645,\n",
       "  'index': 164,\n",
       "  'word': 'Shan',\n",
       "  'start': 748,\n",
       "  'end': 752},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99975485,\n",
       "  'index': 165,\n",
       "  'word': 'Carter',\n",
       "  'start': 753,\n",
       "  'end': 759},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.98398834,\n",
       "  'index': 211,\n",
       "  'word': 'Web',\n",
       "  'start': 969,\n",
       "  'end': 972},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9741402,\n",
       "  'index': 212,\n",
       "  'word': '##T',\n",
       "  'start': 972,\n",
       "  'end': 973},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9101558,\n",
       "  'index': 213,\n",
       "  'word': '##ex',\n",
       "  'start': 973,\n",
       "  'end': 975},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.81520927,\n",
       "  'index': 214,\n",
       "  'word': '##t',\n",
       "  'start': 975,\n",
       "  'end': 976},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.980161,\n",
       "  'index': 226,\n",
       "  'word': 'Open',\n",
       "  'start': 1027,\n",
       "  'end': 1031},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9492247,\n",
       "  'index': 227,\n",
       "  'word': '##A',\n",
       "  'start': 1031,\n",
       "  'end': 1032},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.96556985,\n",
       "  'index': 228,\n",
       "  'word': '##I',\n",
       "  'start': 1032,\n",
       "  'end': 1033}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "\n",
    "acno = extract_element_soup(files[i],None,\"div\",\"acknowledgement\")\n",
    "ners=[]\n",
    "if len(acno)>0:\n",
    "    for elements_text in acno:\n",
    "        ners.extend(pipe(elements_text.text))\n",
    "ners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
