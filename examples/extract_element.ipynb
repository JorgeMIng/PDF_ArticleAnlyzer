{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JorgeM\\Desktop\\upm\\pdf_analizer\\PDF_ArticleAnlyzer\\examples\n",
      "c:\\Users\\JorgeM\\Desktop\\upm\\pdf_analizer\\PDF_ArticleAnlyzer\n"
     ]
    }
   ],
   "source": [
    "# add the local libraries to the path\n",
    "\n",
    "## in case it has been added with setup.py this cell doenst need to be executed\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "print(os.getcwd())\n",
    "father_path = Path(os.path.dirname(os.getcwd()))\n",
    "print(father_path)\n",
    "sys.path.append(os.path.join(father_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                       Version\n",
      "----------------------------- ---------------\n",
      "absl-py                       2.0.0\n",
      "accelerate                    0.24.1\n",
      "aiobotocore                   2.5.0\n",
      "aiofiles                      22.1.0\n",
      "aiohttp                       3.8.5\n",
      "aioitertools                  0.7.1\n",
      "aiosignal                     1.2.0\n",
      "aiosqlite                     0.18.0\n",
      "alabaster                     0.7.12\n",
      "altair                        5.1.1\n",
      "anaconda-anon-usage           0.4.3\n",
      "anaconda-catalogs             0.2.0\n",
      "anaconda-client               1.12.1\n",
      "anaconda-project              0.11.1\n",
      "annotated-types               0.6.0\n",
      "antlr4-python3-runtime        4.9.3\n",
      "anyascii                      0.3.2\n",
      "anyio                         3.5.0\n",
      "appdirs                       1.4.4\n",
      "argon2-cffi                   21.3.0\n",
      "argon2-cffi-bindings          21.2.0\n",
      "arrow                         1.2.3\n",
      "astroid                       2.14.2\n",
      "astropy                       5.3.4\n",
      "asttokens                     2.0.5\n",
      "async-timeout                 4.0.2\n",
      "atomicwrites                  1.4.0\n",
      "attrs                         23.1.0\n",
      "Automat                       20.2.0\n",
      "autopep8                      1.6.0\n",
      "Babel                         2.11.0\n",
      "backcall                      0.2.0\n",
      "backports.functools-lru-cache 1.6.4\n",
      "backports.tempfile            1.0\n",
      "backports.weakref             1.0.post1\n",
      "bcrypt                        3.2.0\n",
      "beautifulsoup4                4.9.1\n",
      "binaryornot                   0.4.4\n",
      "black                         0.0\n",
      "bleach                        4.1.0\n",
      "blinker                       1.6.2\n",
      "blis                          0.7.9\n",
      "bokeh                         3.2.1\n",
      "boltons                       23.0.0\n",
      "boto3                         1.26.76\n",
      "botocore                      1.29.76\n",
      "Bottleneck                    1.3.5\n",
      "brotlipy                      0.7.0\n",
      "bs4                           0.0.2\n",
      "build                         0.5.0\n",
      "cachetools                    5.3.1\n",
      "catalogue                     2.0.7\n",
      "certifi                       2023.7.22\n",
      "cffi                          1.15.1\n",
      "chardet                       4.0.0\n",
      "charset-normalizer            2.0.4\n",
      "click                         7.1.2\n",
      "cloudpickle                   2.2.1\n",
      "clyent                        1.2.2\n",
      "colorama                      0.4.6\n",
      "colorcet                      3.0.1\n",
      "comm                          0.1.2\n",
      "conda                         23.9.0\n",
      "conda-build                   3.27.0\n",
      "conda-content-trust           0.2.0\n",
      "conda_index                   0.3.0\n",
      "conda-libmamba-solver         23.9.2\n",
      "conda-pack                    0.6.0\n",
      "conda-package-handling        2.2.0\n",
      "conda_package_streaming       0.9.0\n",
      "conda-token                   0.4.0\n",
      "conda-verify                  3.4.2\n",
      "confection                    0.0.4\n",
      "constantly                    15.1.0\n",
      "contourpy                     1.0.5\n",
      "contractions                  0.1.73\n",
      "cookiecutter                  1.7.3\n",
      "cryptography                  41.0.3\n",
      "cssselect                     1.1.0\n",
      "cupy-cuda12x                  12.2.0\n",
      "cycler                        0.11.0\n",
      "cymem                         2.0.6\n",
      "cytoolz                       0.12.0\n",
      "daal4py                       2023.1.1\n",
      "dask                          2023.6.0\n",
      "dataclasses-json              0.6.1\n",
      "datashader                    0.15.2\n",
      "datashape                     0.5.4\n",
      "debugpy                       1.6.7\n",
      "decorator                     5.1.1\n",
      "defusedxml                    0.7.1\n",
      "diff-match-patch              20200713\n",
      "dill                          0.3.7\n",
      "distlib                       0.3.8\n",
      "distributed                   2023.6.0\n",
      "docstring-to-markdown         0.11\n",
      "docutils                      0.18.1\n",
      "dominate                      2.5.1\n",
      "duckdb                        0.7.1\n",
      "elementpath                   4.1.5\n",
      "en-core-web-lg                3.5.0\n",
      "en-core-web-md                3.5.0\n",
      "en-core-web-sm                3.5.0\n",
      "entrypoints                   0.4\n",
      "et-xmlfile                    1.1.0\n",
      "executing                     0.8.3\n",
      "Faker                         19.4.0\n",
      "falcon                        3.1.1\n",
      "fastjsonschema                2.16.2\n",
      "fastrlock                     0.8.2\n",
      "favicon                       0.7.0\n",
      "filelock                      3.13.1\n",
      "flake8                        6.0.0\n",
      "Flask                         1.1.2\n",
      "Flask-Cors                    3.0.10\n",
      "Flask-Script                  2.0.6\n",
      "Flask-SQLAlchemy              2.5.1\n",
      "Flask-Uploads                 0.2.1\n",
      "fonttools                     4.25.0\n",
      "frozendict                    2.3.8\n",
      "frozenlist                    1.3.3\n",
      "fsspec                        2023.10.0\n",
      "fst-pso                       1.8.1\n",
      "future                        0.18.3\n",
      "FuzzyTM                       2.0.5\n",
      "gensim                        4.3.0\n",
      "gh                            0.0.4\n",
      "gitdb                         4.0.10\n",
      "GitPython                     3.1.35\n",
      "glob2                         0.7\n",
      "google-auth                   2.23.3\n",
      "google-auth-oauthlib          1.1.0\n",
      "graphviz                      0.20.1\n",
      "greenlet                      2.0.1\n",
      "grpcio                        1.59.0\n",
      "gsitk                         0.2.5\n",
      "h5py                          3.9.0\n",
      "HeapDict                      1.0.1\n",
      "holoviews                     1.17.1\n",
      "htbuilder                     0.6.2\n",
      "huggingface                   0.0.1\n",
      "huggingface-hub               0.17.3\n",
      "hvplot                        0.9.0\n",
      "hyperlink                     21.0.0\n",
      "idna                          3.4\n",
      "imagecodecs                   2023.1.23\n",
      "imageio                       2.31.4\n",
      "imagesize                     1.4.1\n",
      "imbalanced-learn              0.10.1\n",
      "importlib-metadata            6.0.0\n",
      "importlib-resources           6.1.0\n",
      "incremental                   21.3.0\n",
      "inexactsearch                 1.0.2\n",
      "inflection                    0.5.1\n",
      "iniconfig                     1.1.1\n",
      "inquirerpy                    0.3.4\n",
      "intake                        0.6.8\n",
      "intervaltree                  3.1.0\n",
      "ipykernel                     6.25.0\n",
      "ipython                       8.15.0\n",
      "ipython-genutils              0.2.0\n",
      "ipywidgets                    8.0.4\n",
      "isodate                       0.6.0\n",
      "isort                         5.9.3\n",
      "itemadapter                   0.3.0\n",
      "itemloaders                   1.0.4\n",
      "itsdangerous                  1.1.0\n",
      "jaraco.classes                3.2.1\n",
      "jedi                          0.18.1\n",
      "jellyfish                     1.0.1\n",
      "Jinja2                        3.1.3\n",
      "jinja2-time                   0.2.0\n",
      "jmespath                      0.10.0\n",
      "joblib                        1.2.0\n",
      "json-normalize                1.0.1\n",
      "json5                         0.9.6\n",
      "jsonpatch                     1.33\n",
      "jsonpath-python               1.0.6\n",
      "jsonpointer                   2.1\n",
      "jsonref                       1.1.0\n",
      "jsonschema                    4.17.3\n",
      "jupyter                       1.0.0\n",
      "jupyter_client                7.4.9\n",
      "jupyter-console               6.6.3\n",
      "jupyter_core                  5.3.0\n",
      "jupyter-events                0.6.3\n",
      "jupyter-server                1.23.4\n",
      "jupyter_server_fileid         0.9.0\n",
      "jupyter_server_ydoc           0.8.0\n",
      "jupyter-ydoc                  0.2.4\n",
      "jupyterlab                    3.6.3\n",
      "jupyterlab-pygments           0.1.2\n",
      "jupyterlab_server             2.22.0\n",
      "jupyterlab-widgets            3.0.5\n",
      "kaleido                       0.2.1\n",
      "keyring                       23.13.1\n",
      "kiwisolver                    1.4.4\n",
      "langchain                     0.0.331\n",
      "langcodes                     3.3.0\n",
      "langsmith                     0.0.60\n",
      "LatLon                        1.0.2\n",
      "latlon3                       1.0.4\n",
      "lazy_loader                   0.3\n",
      "lazy-object-proxy             1.6.0\n",
      "Levenshtein                   0.21.1\n",
      "libarchive-c                  2.9\n",
      "libmambapy                    1.5.1\n",
      "lime                          0.2.0.1\n",
      "linkify-it-py                 2.0.0\n",
      "llvmlite                      0.41.0\n",
      "lmdb                          1.4.1\n",
      "locket                        1.0.0\n",
      "lxml                          4.9.3\n",
      "lz4                           4.3.2\n",
      "Markdown                      3.4.1\n",
      "markdown-it-py                3.0.0\n",
      "markdownlit                   0.0.7\n",
      "MarkupSafe                    2.1.5\n",
      "marshmallow                   3.20.1\n",
      "matplotlib                    3.7.2\n",
      "matplotlib-inline             0.1.6\n",
      "mccabe                        0.7.0\n",
      "mdit-py-plugins               0.4.0\n",
      "mdurl                         0.1.0\n",
      "menuinst                      1.4.19\n",
      "miniful                       0.0.6\n",
      "mistune                       2.0.5\n",
      "mkl-fft                       1.3.8\n",
      "mkl-random                    1.2.4\n",
      "mkl-service                   2.4.0\n",
      "more-itertools                8.12.0\n",
      "morph_kgc                     2.6.3\n",
      "mpmath                        1.3.0\n",
      "msgpack                       1.0.3\n",
      "multidict                     6.0.2\n",
      "multipledispatch              0.6.0\n",
      "munkres                       1.1.4\n",
      "murmurhash                    1.0.7\n",
      "mypy-extensions               1.0.0\n",
      "myst-parser                   2.0.0\n",
      "names-dataset                 2.1.0\n",
      "nbclassic                     0.5.5\n",
      "nbclient                      0.5.13\n",
      "nbconvert                     6.5.4\n",
      "nbformat                      5.9.2\n",
      "nest-asyncio                  1.5.6\n",
      "networkx                      3.1\n",
      "nltk                          3.8.1\n",
      "notebook                      6.5.4\n",
      "notebook_shim                 0.2.2\n",
      "numba                         0.58.0\n",
      "numexpr                       2.8.7\n",
      "numpy                         1.24.3\n",
      "numpydoc                      1.5.0\n",
      "oauthlib                      3.2.2\n",
      "omegaconf                     2.3.0\n",
      "opencv-python                 4.9.0.80\n",
      "openpyxl                      3.0.10\n",
      "packaging                     23.2\n",
      "pandas                        1.5.2\n",
      "pandocfilters                 1.5.0\n",
      "panel                         1.2.3\n",
      "param                         1.13.0\n",
      "paramiko                      2.8.1\n",
      "parsel                        1.6.0\n",
      "parso                         0.8.3\n",
      "partd                         1.4.0\n",
      "pathspec                      0.10.3\n",
      "pathy                         0.10.1\n",
      "patsy                         0.5.3\n",
      "pep517                        0.10.0\n",
      "pep8                          1.7.1\n",
      "pexpect                       4.8.0\n",
      "pfzy                          0.3.4\n",
      "pickleshare                   0.7.5\n",
      "Pillow                        9.5.0\n",
      "pip                           23.3\n",
      "pkginfo                       1.9.6\n",
      "platformdirs                  3.10.0\n",
      "plotly                        5.9.0\n",
      "pluggy                        1.0.0\n",
      "ply                           3.11\n",
      "poyo                          0.5.0\n",
      "preshed                       3.0.6\n",
      "prettytable                   3.9.0\n",
      "prometheus-client             0.14.1\n",
      "prompt-toolkit                3.0.36\n",
      "Protego                       0.1.16\n",
      "protobuf                      4.23.4\n",
      "psutil                        5.9.0\n",
      "psycopg2                      2.9.5\n",
      "ptyprocess                    0.7.0\n",
      "pure-eval                     0.2.2\n",
      "py-cpuinfo                    9.0.0\n",
      "pyahocorasick                 2.0.0\n",
      "pyarrow                       11.0.0\n",
      "pyasn1                        0.4.8\n",
      "pyasn1-modules                0.2.8\n",
      "pycodestyle                   2.10.0\n",
      "pycosat                       0.6.6\n",
      "pycparser                     2.21\n",
      "pyct                          0.5.0\n",
      "pycurl                        7.45.2\n",
      "pydantic                      1.10.13\n",
      "pydantic_core                 2.10.1\n",
      "pydantic-extra-types          2.1.0\n",
      "pydantic-secret-decimal       0.1.2\n",
      "pydeck                        0.8.0\n",
      "PyDispatcher                  2.0.5\n",
      "pydocstyle                    6.3.0\n",
      "pyecharts                     2.0.4\n",
      "pyerfa                        2.0.0\n",
      "pyflakes                      3.0.1\n",
      "pyFUME                        0.2.25\n",
      "pygame                        2.5.2\n",
      "Pygments                      2.15.1\n",
      "PyLD                          2.0.3\n",
      "pylint                        2.16.2\n",
      "pylint-venv                   2.3.0\n",
      "pyls-spyder                   0.4.0\n",
      "pymdown-extensions            10.3\n",
      "Pympler                       1.0.1\n",
      "PyNaCl                        1.5.0\n",
      "pyodbc                        4.0.39\n",
      "pyOpenSSL                     23.2.0\n",
      "pyoxigraph                    0.3.19\n",
      "pyparsing                     2.4.7\n",
      "pyproj                        3.6.1\n",
      "PyQt5                         5.15.7\n",
      "PyQt5-sip                     12.11.0\n",
      "PyQtWebEngine                 5.15.4\n",
      "pyrsistent                    0.18.0\n",
      "PySocks                       1.7.1\n",
      "pyspellchecker                0.7.2\n",
      "pytest                        7.4.0\n",
      "python-dateutil               2.8.1\n",
      "python-decouple               3.8\n",
      "python-json-logger            2.0.7\n",
      "python-Levenshtein            0.21.1\n",
      "python-lsp-black              1.2.1\n",
      "python-lsp-jsonrpc            1.0.0\n",
      "python-lsp-server             1.7.2\n",
      "python-slugify                5.0.2\n",
      "python-snappy                 0.6.1\n",
      "pytoolconfig                  1.2.5\n",
      "pytreebank                    0.2.7\n",
      "pytz                          2020.1\n",
      "pytz-deprecation-shim         0.1.0.post0\n",
      "pyviz-comms                   2.3.0\n",
      "PyWavelets                    1.4.1\n",
      "pywin32                       305.1\n",
      "pywin32-ctypes                0.2.0\n",
      "pywinpty                      2.0.10\n",
      "PyYAML                        6.0.1\n",
      "pyzmq                         23.2.0\n",
      "QDarkStyle                    3.0.2\n",
      "qstylizer                     0.2.2\n",
      "QtAwesome                     1.2.2\n",
      "qtconsole                     5.4.2\n",
      "QtPy                          2.2.0\n",
      "queuelib                      1.6.2\n",
      "rapidfuzz                     3.2.0\n",
      "rdflib                        5.0.0\n",
      "readability                   0.3.1\n",
      "regex                         2023.10.3\n",
      "requests                      2.28.2\n",
      "requests-file                 1.5.1\n",
      "requests-oauthlib             1.3.1\n",
      "requests-toolbelt             1.0.0\n",
      "responses                     0.23.3\n",
      "rfc3339-validator             0.1.4\n",
      "rfc3986-validator             0.1.1\n",
      "rich                          13.5.2\n",
      "rope                          1.7.0\n",
      "rsa                           4.9\n",
      "Rtree                         1.0.1\n",
      "ruamel.yaml                   0.17.21\n",
      "ruamel-yaml-conda             0.17.21\n",
      "s3fs                          2023.4.0\n",
      "s3transfer                    0.6.0\n",
      "sacremoses                    0.0.43\n",
      "safetensors                   0.4.0\n",
      "scikit-image                  0.20.0\n",
      "scikit-learn                  1.3.2\n",
      "scikit-learn-intelex          20230426.121932\n",
      "scipy                         1.11.3\n",
      "Scrapy                        2.8.0\n",
      "seaborn                       0.12.2\n",
      "Send2Trash                    1.8.0\n",
      "senpy                         1.0.6\n",
      "sentence-transformers         2.2.2\n",
      "sentencepiece                 0.1.99\n",
      "service-identity              18.1.0\n",
      "setuptools                    68.0.0\n",
      "shap                          0.42.1\n",
      "shellingham                   1.5.0\n",
      "silpa-common                  0.3\n",
      "simpful                       2.11.0\n",
      "simplejson                    3.19.1\n",
      "sip                           6.6.2\n",
      "six                           1.14.0\n",
      "slicer                        0.0.7\n",
      "smart-open                    5.2.1\n",
      "smmap                         5.0.0\n",
      "sniffio                       1.2.0\n",
      "snowballstemmer               2.2.0\n",
      "sortedcontainers              2.4.0\n",
      "soundex                       1.1.3\n",
      "soupsieve                     2.0.1\n",
      "spacy                         3.5.3\n",
      "spacy-legacy                  3.0.12\n",
      "spacy-loggers                 1.0.4\n",
      "spellchecker                  0.4\n",
      "Sphinx                        7.2.6\n",
      "sphinx_mdinclude              0.5.3\n",
      "sphinx-rtd-theme              2.0.0\n",
      "sphinxcontrib-applehelp       1.0.2\n",
      "sphinxcontrib-devhelp         1.0.2\n",
      "sphinxcontrib-htmlhelp        2.0.0\n",
      "sphinxcontrib-jquery          4.1\n",
      "sphinxcontrib-jsmath          1.0.1\n",
      "sphinxcontrib-qthelp          1.0.3\n",
      "sphinxcontrib-serializinghtml 1.1.10\n",
      "spyder                        5.4.3\n",
      "spyder-kernels                2.4.4\n",
      "sql_metadata                  2.9.0\n",
      "SQLAlchemy                    1.4.46\n",
      "sqlparse                      0.4.4\n",
      "srsly                         2.4.6\n",
      "st-annotated-text             4.0.1\n",
      "stack-data                    0.2.0\n",
      "statsmodels                   0.14.0\n",
      "streamlit                     1.28.0\n",
      "streamlit-aggrid              0.3.4.post3\n",
      "streamlit-awesome-table       0.1.0\n",
      "streamlit-camera-input-live   0.2.0\n",
      "streamlit-card                0.0.61\n",
      "streamlit-echarts             0.4.0\n",
      "streamlit-elements            0.1.0\n",
      "streamlit-embedcode           0.1.2\n",
      "streamlit-extras              0.3.1\n",
      "streamlit-faker               0.0.2\n",
      "streamlit-image-coordinates   0.1.6\n",
      "streamlit-keyup               0.2.0\n",
      "streamlit-lottie              0.0.5\n",
      "streamlit-plotly-events       0.0.6\n",
      "streamlit-pydantic            0.6.0\n",
      "streamlit-toggle-switch       1.0.2\n",
      "streamlit-vertical-slider     1.0.2\n",
      "sympy                         1.11.1\n",
      "syntok                        1.4.4\n",
      "tables                        3.8.0\n",
      "tabulate                      0.8.10\n",
      "TBB                           0.2\n",
      "tblib                         1.7.0\n",
      "tenacity                      8.2.2\n",
      "tensorboard                   2.15.0\n",
      "tensorboard-data-server       0.7.2\n",
      "terminado                     0.17.1\n",
      "text-preprocessing            0.1.1\n",
      "text-unidecode                1.3\n",
      "textdistance                  4.2.1\n",
      "textsearch                    0.0.24\n",
      "thinc                         8.1.10\n",
      "threadpoolctl                 2.2.0\n",
      "three-merge                   0.1.1\n",
      "tifffile                      2023.4.12\n",
      "tinycss2                      1.2.1\n",
      "tldextract                    3.2.0\n",
      "tokenizers                    0.14.1\n",
      "toml                          0.10.2\n",
      "tomlkit                       0.11.1\n",
      "toolz                         0.12.0\n",
      "torch                         2.1.0\n",
      "torchaudio                    2.1.0\n",
      "torchsummary                  1.5.1\n",
      "torchvision                   0.16.0\n",
      "torchviz                      0.0.2\n",
      "tornado                       6.3.3\n",
      "tqdm                          4.65.0\n",
      "traitlets                     5.7.1\n",
      "transformers                  4.34.1\n",
      "truststore                    0.8.0\n",
      "Twisted                       22.10.0\n",
      "twisted-iocpsupport           1.0.2\n",
      "typer                         0.4.1\n",
      "types-PyYAML                  6.0.12.11\n",
      "typing_extensions             4.7.1\n",
      "typing-inspect                0.9.0\n",
      "tzdata                        2023.3\n",
      "tzlocal                       4.3.1\n",
      "uc-micro-py                   1.0.1\n",
      "ujson                         5.4.0\n",
      "Unidecode                     1.2.0\n",
      "unittest-xml-reporting        3.2.0\n",
      "urllib3                       1.26.16\n",
      "validators                    0.22.0\n",
      "virtualenv                    20.25.0\n",
      "visitor                       0.1.3\n",
      "w3lib                         1.21.0\n",
      "wasabi                        0.9.1\n",
      "watchdog                      2.1.6\n",
      "wcwidth                       0.2.5\n",
      "webencodings                  0.5.1\n",
      "websocket-client              0.58.0\n",
      "Werkzeug                      0.16.1\n",
      "wget                          3.2\n",
      "whatthepatch                  1.0.2\n",
      "wheel                         0.41.2\n",
      "widgetsnbextension            4.0.5\n",
      "win-inet-pton                 1.1.0\n",
      "wordcloud                     1.9.3\n",
      "wrapt                         1.14.1\n",
      "xarray                        2023.6.0\n",
      "xgboost                       2.0.0\n",
      "xlwings                       0.29.1\n",
      "xyzservices                   2022.9.0\n",
      "y-py                          0.5.9\n",
      "yapf                          0.31.0\n",
      "yarl                          1.8.1\n",
      "ypy-websocket                 0.8.2\n",
      "zict                          3.0.0\n",
      "zipp                          3.11.0\n",
      "zope.interface                5.4.0\n",
      "zstandard                     0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "from pdf_analyzer.api.extract.elements import extract_elements\n",
    "from pdf_analyzer.config_load import load_config\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVER_CONFIG\n",
      "url:\n",
      "  protocol: http\n",
      "  api_domain: yordi111nas.synology.me\n",
      "  port: 8070\n",
      "\n",
      "CLOUD_CONFIG\n",
      "data:\n",
      "  data_dir: data/PDFs\n",
      "  format: .pdf\n",
      "  recursive: true\n",
      "grobid:\n",
      "  cache: true\n",
      "  cache_dir: data/xmls\n",
      "  operation_key: processFulltextDocument\n",
      "  format: .grobid.tei.xml\n",
      "  recursive: true\n",
      "extract:\n",
      "  dir: null\n",
      "  element: null\n",
      "  type: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "server_config = load_config(\"config/api/grobid-server-config.yaml\")\n",
    "extract_config = load_config(\"config/api/extract_element.yaml\")\n",
    "print(\"SERVER_CONFIG\\n\"+OmegaConf.to_yaml(server_config))\n",
    "print(\"CLOUD_CONFIG\\n\"+OmegaConf.to_yaml(extract_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_config.grobid.recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the config files before starting or here with code\n",
    " For example\n",
    " \n",
    " server_config.protocol = https   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n",
      "data/xmls\\nlp\\Bert.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\DistillBERT.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\Dont_stop_pretraining.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\GPT-3.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\LIME.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\LoRA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\RoBERTa.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\SORA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\Transformers.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\word2vec.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "grobid_logger: 2024-04-26 17:41:57,926 | INFO | API.py:44 | 50960 >>> All files have been process by the api\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grobid_logger: 2024-04-26 17:41:58,002 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Bert have been extracted\n",
      "grobid_logger: 2024-04-26 17:41:58,030 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file DistillBERT have been extracted\n",
      "grobid_logger: 2024-04-26 17:41:58,202 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Dont_stop_pretraining have been extracted\n",
      "grobid_logger: 2024-04-26 17:41:58,395 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file GPT-3 have been extracted\n",
      "grobid_logger: 2024-04-26 17:41:58,435 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file LIME have been extracted\n",
      "grobid_logger: 2024-04-26 17:41:58,523 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file LoRA have been extracted\n",
      "grobid_logger: 2024-04-26 17:41:58,587 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file RoBERTa have been extracted\n",
      "grobid_logger: 2024-04-26 17:41:58,974 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file SORA have been extracted\n",
      "grobid_logger: 2024-04-26 17:41:59,066 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Transformers have been extracted\n",
      "grobid_logger: 2024-04-26 17:41:59,122 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file word2vec have been extracted\n"
     ]
    }
   ],
   "source": [
    "extract_config.extract.dir = \"teiHeader.fileDesc.sourceDesc.biblStruct.analytic\"\n",
    "extract_config.extract.element = \"author\"\n",
    "authors = extract_elements(extract_config,server_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing elements author in paper Bert\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Jacob</forename><surname>Devlin</surname></persName>\n",
      "<email>jacobdevlin@google.com</email>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Ming-Wei</forename><surname>Chang</surname></persName>\n",
      "<email>mingweichang@google.com</email>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n",
      "<email>kentonl@google.com</email>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Kristina</forename><surname>Toutanova</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Showing elements author in paper DistillBERT\n",
      "\n",
      "\n",
      "<author role=\"corresp\">\n",
      "<persName><forename type=\"first\">Victor</forename><surname>Sanh</surname></persName>\n",
      "<email>victor@huggingface.co</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"department\">Lysandre DEBUT</orgName>\n",
      "<orgName key=\"instit1\" type=\"institution\">Julien CHAUMOND</orgName>\n",
      "<orgName key=\"instit2\" type=\"institution\">Thomas WOLF Hugging Face</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Showing elements author in paper Dont_stop_pretraining\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Suchin</forename><surname>Gururangan</surname></persName>\n",
      "<email>suching@allenai.org</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
      "<address>\n",
      "<settlement>Seattle</settlement>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Ana</forename><surname>MarasoviÄ‡</surname></persName>\n",
      "<email>anam@allenai.org</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
      "<address>\n",
      "<settlement>Seattle</settlement>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "<affiliation key=\"aff1\">\n",
      "<orgName type=\"department\">Paul G</orgName>\n",
      "<orgName key=\"instit1\" type=\"institution\">Allen School of Computer Science &amp; Engineering</orgName>\n",
      "<orgName key=\"instit2\" type=\"institution\">University of Washington</orgName>\n",
      "<address>\n",
      "<settlement>Seattle</settlement>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Swabha</forename><surname>Swayamdipta</surname></persName>\n",
      "<email>swabhas@allenai.org</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
      "<address>\n",
      "<settlement>Seattle</settlement>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Kyle</forename><surname>Lo</surname></persName>\n",
      "<email>kylel@allenai.org</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
      "<address>\n",
      "<settlement>Seattle</settlement>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Iz</forename><surname>Beltagy</surname></persName>\n",
      "<email>beltagy@allenai.org</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
      "<address>\n",
      "<settlement>Seattle</settlement>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Doug</forename><surname>Downey</surname></persName>\n",
      "<email>dougd@allenai.org</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
      "<address>\n",
      "<settlement>Seattle</settlement>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Noah</forename><forename type=\"middle\">A</forename><surname>Smith</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
      "<address>\n",
      "<settlement>Seattle</settlement>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "<affiliation key=\"aff1\">\n",
      "<orgName type=\"department\">Paul G</orgName>\n",
      "<orgName key=\"instit1\" type=\"institution\">Allen School of Computer Science &amp; Engineering</orgName>\n",
      "<orgName key=\"instit2\" type=\"institution\">University of Washington</orgName>\n",
      "<address>\n",
      "<settlement>Seattle</settlement>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Showing elements author in paper GPT-3\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Tom</forename><forename type=\"middle\">B</forename><surname>Brown</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Benjamin</forename><surname>Mann</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Nick</forename><surname>Ryder</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Melanie</forename><surname>Subbiah</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Jared</forename><surname>Kaplan</surname></persName>\n",
      "<affiliation key=\"aff1\">\n",
      "<orgName type=\"institution\">Johns Hopkins University</orgName>\n",
      "<address>\n",
      "<settlement>OpenAI</settlement>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Prafulla</forename><surname>Dhariwal</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Arvind</forename><surname>Neelakantan</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Pranav</forename><surname>Shyam</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Girish</forename><surname>Sastry</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Amanda</forename><surname>Askell</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Sandhini</forename><surname>Agarwal</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Ariel</forename><surname>Herbert-Voss</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Gretchen</forename><surname>Krueger</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Tom</forename><surname>Henighan</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Rewon</forename><surname>Child</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Aditya</forename><surname>Ramesh</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Daniel</forename><forename type=\"middle\">M</forename><surname>Ziegler</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Jeffrey</forename><surname>Wu</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Christopher</forename><surname>Hesse</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Mark</forename><surname>Chen</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Eric</forename><surname>Sigler</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Mateusz</forename><surname>Litwin</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Scott</forename><surname>Gray</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Benjamin</forename><surname>Chess</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Jack</forename><surname>Clark</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Christopher</forename><surname>Berner</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Sam</forename><surname>Mccandlish</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Alec</forename><surname>Radford</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Ilya</forename><surname>Sutskever</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Dario</forename><forename type=\"middle\">Amodei</forename><surname>Openai</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"department\">Clemens Winter</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Showing elements author in paper LIME\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Marco</forename><forename type=\"middle\">Tulio</forename><surname>Ribeiro</surname></persName>\n",
      "<email>marcotcr@cs.uw.edu</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">University of Washington Seattle</orgName>\n",
      "<address>\n",
      "<postCode>98105</postCode>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Sameer</forename><surname>Singh</surname></persName>\n",
      "<email>sameer@cs.uw.edu</email>\n",
      "<affiliation key=\"aff1\">\n",
      "<orgName type=\"institution\">University of Washington Seattle</orgName>\n",
      "<address>\n",
      "<postCode>98105</postCode>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Carlos</forename><surname>Guestrin</surname></persName>\n",
      "<email>guestrin@cs.uw.edu</email>\n",
      "<affiliation key=\"aff2\">\n",
      "<orgName type=\"institution\">University of Washington Seattle</orgName>\n",
      "<address>\n",
      "<postCode>98105</postCode>\n",
      "<region>WA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Showing elements author in paper LoRA\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Edward</forename><surname>Hu</surname></persName>\n",
      "<email>edwardhu@microsoft.com</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Microsoft Corporation</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Yelong</forename><surname>Shen</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Microsoft Corporation</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Phillip</forename><surname>Wallis</surname></persName>\n",
      "<email>phwallis@microsoft.com</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Microsoft Corporation</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Zeyuan</forename><surname>Allen-Zhu</surname></persName>\n",
      "<email>zeyuana@microsoft.com</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Microsoft Corporation</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Yuanzhi</forename><surname>Li</surname></persName>\n",
      "<email>yuanzhil@microsoft.com</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Microsoft Corporation</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Shean</forename><surname>Wang</surname></persName>\n",
      "<email>swang@microsoft.com</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Microsoft Corporation</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Lu</forename><surname>Wang</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Microsoft Corporation</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Weizhu</forename><surname>Chen</surname></persName>\n",
      "<email>wzchen@microsoft.com</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Microsoft Corporation</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Showing elements author in paper RoBERTa\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Zhuang</forename><surname>Liu</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Wayne</forename><surname>Lin</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Ya</forename><surname>Shi</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Jun</forename><surname>Zhao</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Showing elements author in paper SORA\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Yixin</forename><surname>Liu</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Kai</forename><surname>Zhang</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Yuan</forename><surname>Li</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Zhiling</forename><surname>Yan</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Chujie</forename><surname>Gao</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Ruoxi</forename><surname>Chen</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Zhengqing</forename><surname>Yuan</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Yue</forename><surname>Huang</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Hanchi</forename><surname>Sun</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Jianfeng</forename><surname>Gao</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "<affiliation key=\"aff1\">\n",
      "<orgName type=\"department\">Microsoft Research</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Lifang</forename><surname>He</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Lichao</forename><surname>Sun</surname></persName>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Lehigh University</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Showing elements author in paper Transformers\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Ashish</forename><surname>Vaswani</surname></persName>\n",
      "<email>avaswani@google.com</email>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Noam</forename><surname>Shazeer</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Google</forename><surname>Brain</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Niki</forename><surname>Parmar</surname></persName>\n",
      "<email>nikip@google.com</email>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Jakob</forename><surname>Uszkoreit</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Llion</forename><surname>Jones</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Aidan</forename><forename type=\"middle\">N</forename><surname>Gomez</surname></persName>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Åukasz</forename><surname>Kaiser</surname></persName>\n",
      "<email>lukaszkaiser@google.com</email>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"department\">Google Brain</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<affiliation key=\"aff1\">\n",
      "<orgName type=\"department\">Google Research</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<affiliation key=\"aff2\">\n",
      "<orgName type=\"department\">Google Research</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<affiliation key=\"aff3\">\n",
      "<orgName type=\"department\">Google Research</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<affiliation key=\"aff4\">\n",
      "<orgName type=\"institution\">University of Toronto</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<affiliation key=\"aff5\">\n",
      "<orgName type=\"department\">Google Brain</orgName>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<affiliation key=\"aff6\">\n",
      "<orgName type=\"laboratory\">31st Conference on Neural Information Processing Systems (NIPS 2017)</orgName>\n",
      "<address>\n",
      "<settlement>Long Beach</settlement>\n",
      "<region>CA</region>\n",
      "<country key=\"US\">USA</country>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Showing elements author in paper word2vec\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Tomas</forename><surname>Mikolov</surname></persName>\n",
      "<email>tmikolov@google.com</email>\n",
      "<affiliation key=\"aff0\">\n",
      "<orgName type=\"institution\">Google Inc</orgName>\n",
      "<address>\n",
      "<settlement>Mountain View</settlement>\n",
      "<region>CA</region>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Kai</forename><surname>Chen</surname></persName>\n",
      "<email>kaichen@google.com</email>\n",
      "<affiliation key=\"aff1\">\n",
      "<orgName type=\"institution\">Google Inc</orgName>\n",
      "<address>\n",
      "<settlement>Mountain View</settlement>\n",
      "<region>CA</region>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Greg</forename><surname>Corrado</surname></persName>\n",
      "<email>gcorrado@google.com</email>\n",
      "<affiliation key=\"aff2\">\n",
      "<orgName type=\"institution\">Google Inc</orgName>\n",
      "<address>\n",
      "<settlement>Mountain View</settlement>\n",
      "<region>CA</region>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "<author>\n",
      "<persName><forename type=\"first\">Jeffrey</forename><surname>Dean</surname></persName>\n",
      "<affiliation key=\"aff3\">\n",
      "<orgName type=\"institution\">Google Inc</orgName>\n",
      "<address>\n",
      "<settlement>Mountain View</settlement>\n",
      "<region>CA</region>\n",
      "</address>\n",
      "</affiliation>\n",
      "</author>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "authors.print_elements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(authors.proccesed_files[0][\"elements\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jacob Devlin'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(authors.proccesed_files[0][\"elements\"][0].persName.strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jacob'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors.proccesed_files[0][\"elements\"][0].persName.forename.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n",
      "data/xmls\\nlp\\Bert.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\DistillBERT.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\Dont_stop_pretraining.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\GPT-3.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\LIME.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\LoRA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\RoBERTa.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\SORA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\Transformers.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\word2vec.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "grobid_logger: 2024-04-26 17:41:59,266 | INFO | API.py:44 | 50960 >>> All files have been process by the api\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grobid_logger: 2024-04-26 17:41:59,350 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Bert have been extracted\n",
      "grobid_logger: 2024-04-26 17:41:59,382 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file DistillBERT have been extracted\n",
      "grobid_logger: 2024-04-26 17:41:59,478 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Dont_stop_pretraining have been extracted\n",
      "grobid_logger: 2024-04-26 17:41:59,853 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file GPT-3 have been extracted\n",
      "grobid_logger: 2024-04-26 17:41:59,897 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file LIME have been extracted\n",
      "grobid_logger: 2024-04-26 17:41:59,989 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file LoRA have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:00,069 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file RoBERTa have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:00,342 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file SORA have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:00,398 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Transformers have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:00,461 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file word2vec have been extracted\n"
     ]
    }
   ],
   "source": [
    "extract_config.extract.dir = None\n",
    "extract_config.extract.element = \"abstract\"\n",
    "abstract = extract_elements(extract_config,server_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a;Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract.proccesed_files[0][\"elements\"][0].p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n",
      "data/xmls\\nlp\\Bert.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\DistillBERT.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\Dont_stop_pretraining.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\GPT-3.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\LIME.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\LoRA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\RoBERTa.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\SORA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\Transformers.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\word2vec.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "grobid_logger: 2024-04-26 17:42:00,602 | INFO | API.py:44 | 50960 >>> All files have been process by the api\n",
      "grobid_logger: 2024-04-26 17:42:00,678 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Bert have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:00,949 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file DistillBERT have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:01,046 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Dont_stop_pretraining have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:01,271 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file GPT-3 have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:01,330 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file LIME have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:01,430 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file LoRA have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:01,499 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file RoBERTa have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:01,772 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file SORA have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:01,825 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Transformers have been extracted\n",
      "grobid_logger: 2024-04-26 17:42:02,202 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file word2vec have been extracted\n"
     ]
    }
   ],
   "source": [
    "extract_config.extract.dir = None\n",
    "extract_config.extract.element = \"div\"\n",
    "extract_config.extract.type = \"acknowledgement\"\n",
    "ackno = extract_elements(extract_config,server_config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'LIME',\n",
       " 'elements': [<div type=\"acknowledgement\">\n",
       "  <div xmlns=\"http://www.tei-c.org/ns/1.0\"><head>Acknowledgements</head><p>We would like to thank Scott Lundberg, Tianqi Chen, and Tyler Johnson for helpful discussions and feedback. This work was supported in part by ONR awards #W911NF-13-1-0246 and #N00014-13-1-0023, and in part by TerraSwarm, one of six centers of STARnet, a Semiconductor Research Corporation program sponsored by MARCO and DARPA.</p></div>\n",
       "  </div>]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ackno.proccesed_files[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigAttributeError",
     "evalue": "Missing key extract\n    full_key: extract\n    object_type=dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConfigAttributeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m extract_config\u001b[38;5;241m.\u001b[39mextract\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JorgeM\\anaconda3\\Lib\\site-packages\\omegaconf\\dictconfig.py:355\u001b[0m, in \u001b[0;36mDictConfig.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_impl(\n\u001b[0;32m    352\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey, default_value\u001b[38;5;241m=\u001b[39m_DEFAULT_MARKER_, validate_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    353\u001b[0m     )\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConfigKeyError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_and_raise(\n\u001b[0;32m    356\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cause\u001b[38;5;241m=\u001b[39me, type_override\u001b[38;5;241m=\u001b[39mConfigAttributeError\n\u001b[0;32m    357\u001b[0m     )\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_and_raise(key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cause\u001b[38;5;241m=\u001b[39me)\n",
      "File \u001b[1;32mc:\\Users\\JorgeM\\anaconda3\\Lib\\site-packages\\omegaconf\\base.py:231\u001b[0m, in \u001b[0;36mNode._format_and_raise\u001b[1;34m(self, key, value, cause, msg, type_override)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_and_raise\u001b[39m(\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    225\u001b[0m     key: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m     type_override: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 231\u001b[0m     format_and_raise(\n\u001b[0;32m    232\u001b[0m         node\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    233\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey,\n\u001b[0;32m    234\u001b[0m         value\u001b[38;5;241m=\u001b[39mvalue,\n\u001b[0;32m    235\u001b[0m         msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(cause) \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m msg,\n\u001b[0;32m    236\u001b[0m         cause\u001b[38;5;241m=\u001b[39mcause,\n\u001b[0;32m    237\u001b[0m         type_override\u001b[38;5;241m=\u001b[39mtype_override,\n\u001b[0;32m    238\u001b[0m     )\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JorgeM\\anaconda3\\Lib\\site-packages\\omegaconf\\_utils.py:899\u001b[0m, in \u001b[0;36mformat_and_raise\u001b[1;34m(node, key, value, msg, cause, type_override)\u001b[0m\n\u001b[0;32m    896\u001b[0m     ex\u001b[38;5;241m.\u001b[39mref_type \u001b[38;5;241m=\u001b[39m ref_type\n\u001b[0;32m    897\u001b[0m     ex\u001b[38;5;241m.\u001b[39mref_type_str \u001b[38;5;241m=\u001b[39m ref_type_str\n\u001b[1;32m--> 899\u001b[0m _raise(ex, cause)\n",
      "File \u001b[1;32mc:\\Users\\JorgeM\\anaconda3\\Lib\\site-packages\\omegaconf\\_utils.py:797\u001b[0m, in \u001b[0;36m_raise\u001b[1;34m(ex, cause)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    796\u001b[0m     ex\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 797\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\JorgeM\\anaconda3\\Lib\\site-packages\\omegaconf\\dictconfig.py:351\u001b[0m, in \u001b[0;36mDictConfig.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m()\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_impl(\n\u001b[0;32m    352\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey, default_value\u001b[38;5;241m=\u001b[39m_DEFAULT_MARKER_, validate_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    353\u001b[0m     )\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConfigKeyError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_and_raise(\n\u001b[0;32m    356\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cause\u001b[38;5;241m=\u001b[39me, type_override\u001b[38;5;241m=\u001b[39mConfigAttributeError\n\u001b[0;32m    357\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\JorgeM\\anaconda3\\Lib\\site-packages\\omegaconf\\dictconfig.py:442\u001b[0m, in \u001b[0;36mDictConfig._get_impl\u001b[1;34m(self, key, default_value, validate_key)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_impl\u001b[39m(\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28mself\u001b[39m, key: DictKeyType, default_value: Any, validate_key: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    440\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 442\u001b[0m         node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_child(\n\u001b[0;32m    443\u001b[0m             key\u001b[38;5;241m=\u001b[39mkey, throw_on_missing_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, validate_key\u001b[38;5;241m=\u001b[39mvalidate_key\n\u001b[0;32m    444\u001b[0m         )\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (ConfigAttributeError, ConfigKeyError):\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m default_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _DEFAULT_MARKER_:\n",
      "File \u001b[1;32mc:\\Users\\JorgeM\\anaconda3\\Lib\\site-packages\\omegaconf\\basecontainer.py:73\u001b[0m, in \u001b[0;36mBaseContainer._get_child\u001b[1;34m(self, key, validate_access, validate_key, throw_on_missing_value, throw_on_missing_key)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_child\u001b[39m(\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     66\u001b[0m     key: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m     throw_on_missing_key: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     71\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Optional[Node], List[Optional[Node]]]:\n\u001b[0;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Like _get_node, passing through to the nearest concrete Node.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     child \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_node(\n\u001b[0;32m     74\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey,\n\u001b[0;32m     75\u001b[0m         validate_access\u001b[38;5;241m=\u001b[39mvalidate_access,\n\u001b[0;32m     76\u001b[0m         validate_key\u001b[38;5;241m=\u001b[39mvalidate_key,\n\u001b[0;32m     77\u001b[0m         throw_on_missing_value\u001b[38;5;241m=\u001b[39mthrow_on_missing_value,\n\u001b[0;32m     78\u001b[0m         throw_on_missing_key\u001b[38;5;241m=\u001b[39mthrow_on_missing_key,\n\u001b[0;32m     79\u001b[0m     )\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child, UnionNode) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_special(child):\n\u001b[0;32m     81\u001b[0m         value \u001b[38;5;241m=\u001b[39m child\u001b[38;5;241m.\u001b[39m_value()\n",
      "File \u001b[1;32mc:\\Users\\JorgeM\\anaconda3\\Lib\\site-packages\\omegaconf\\dictconfig.py:480\u001b[0m, in \u001b[0;36mDictConfig._get_node\u001b[1;34m(self, key, validate_access, validate_key, throw_on_missing_value, throw_on_missing_key)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m throw_on_missing_key:\n\u001b[1;32m--> 480\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConfigKeyError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m throw_on_missing_value \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39m_is_missing():\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingMandatoryValue(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing mandatory value: $KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mConfigAttributeError\u001b[0m: Missing key extract\n    full_key: extract\n    object_type=dict"
     ]
    }
   ],
   "source": [
    "extract_config.extract.type ==None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n",
      "data/xmls\\nlp\\Bert.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\DistillBERT.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\Dont_stop_pretraining.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\GPT-3.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\LIME.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\LoRA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\RoBERTa.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\SORA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\Transformers.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\word2vec.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "grobid_logger: 2024-04-26 17:44:50,208 | INFO | API.py:44 | 50960 >>> All files have been process by the api\n",
      "grobid_logger: 2024-04-26 17:44:50,300 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Bert have been extracted\n",
      "grobid_logger: 2024-04-26 17:44:50,336 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file DistillBERT have been extracted\n",
      "grobid_logger: 2024-04-26 17:44:50,475 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Dont_stop_pretraining have been extracted\n",
      "grobid_logger: 2024-04-26 17:44:50,665 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file GPT-3 have been extracted\n",
      "grobid_logger: 2024-04-26 17:44:50,701 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file LIME have been extracted\n",
      "grobid_logger: 2024-04-26 17:44:51,815 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file LoRA have been extracted\n",
      "grobid_logger: 2024-04-26 17:44:51,899 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file RoBERTa have been extracted\n",
      "grobid_logger: 2024-04-26 17:44:52,116 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file SORA have been extracted\n",
      "grobid_logger: 2024-04-26 17:44:52,164 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file Transformers have been extracted\n",
      "grobid_logger: 2024-04-26 17:44:52,212 | INFO | elements.py:69 | 50960 >>> All elements of of the xml_file word2vec have been extracted\n"
     ]
    }
   ],
   "source": [
    "extract_config.extract.dir = \"teiHeader.fileDesc\"\n",
    "extract_config.extract.element = None\n",
    "papers_info = extract_elements(extract_config,server_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fileDesc>\n",
       "<titleStmt>\n",
       "<title level=\"a\" type=\"main\">Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>\n",
       "</titleStmt>\n",
       "<publicationStmt>\n",
       "<publisher>Association for Computational Linguistics</publisher>\n",
       "<availability status=\"unknown\"><p>Copyright Association for Computational Linguistics</p>\n",
       "</availability>\n",
       "<date type=\"published\" when=\"2020-05-05\">5 May 2020</date>\n",
       "</publicationStmt>\n",
       "<sourceDesc>\n",
       "<biblStruct>\n",
       "<analytic>\n",
       "<author>\n",
       "<persName><forename type=\"first\">Suchin</forename><surname>Gururangan</surname></persName>\n",
       "<email>suching@allenai.org</email>\n",
       "<affiliation key=\"aff0\">\n",
       "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       "<address>\n",
       "<settlement>Seattle</settlement>\n",
       "<region>WA</region>\n",
       "<country key=\"US\">USA</country>\n",
       "</address>\n",
       "</affiliation>\n",
       "</author>\n",
       "<author>\n",
       "<persName><forename type=\"first\">Ana</forename><surname>MarasoviÄ‡</surname></persName>\n",
       "<email>anam@allenai.org</email>\n",
       "<affiliation key=\"aff0\">\n",
       "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       "<address>\n",
       "<settlement>Seattle</settlement>\n",
       "<region>WA</region>\n",
       "<country key=\"US\">USA</country>\n",
       "</address>\n",
       "</affiliation>\n",
       "<affiliation key=\"aff1\">\n",
       "<orgName type=\"department\">Paul G</orgName>\n",
       "<orgName key=\"instit1\" type=\"institution\">Allen School of Computer Science &amp; Engineering</orgName>\n",
       "<orgName key=\"instit2\" type=\"institution\">University of Washington</orgName>\n",
       "<address>\n",
       "<settlement>Seattle</settlement>\n",
       "<region>WA</region>\n",
       "<country key=\"US\">USA</country>\n",
       "</address>\n",
       "</affiliation>\n",
       "</author>\n",
       "<author>\n",
       "<persName><forename type=\"first\">Swabha</forename><surname>Swayamdipta</surname></persName>\n",
       "<email>swabhas@allenai.org</email>\n",
       "<affiliation key=\"aff0\">\n",
       "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       "<address>\n",
       "<settlement>Seattle</settlement>\n",
       "<region>WA</region>\n",
       "<country key=\"US\">USA</country>\n",
       "</address>\n",
       "</affiliation>\n",
       "</author>\n",
       "<author>\n",
       "<persName><forename type=\"first\">Kyle</forename><surname>Lo</surname></persName>\n",
       "<email>kylel@allenai.org</email>\n",
       "<affiliation key=\"aff0\">\n",
       "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       "<address>\n",
       "<settlement>Seattle</settlement>\n",
       "<region>WA</region>\n",
       "<country key=\"US\">USA</country>\n",
       "</address>\n",
       "</affiliation>\n",
       "</author>\n",
       "<author>\n",
       "<persName><forename type=\"first\">Iz</forename><surname>Beltagy</surname></persName>\n",
       "<email>beltagy@allenai.org</email>\n",
       "<affiliation key=\"aff0\">\n",
       "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       "<address>\n",
       "<settlement>Seattle</settlement>\n",
       "<region>WA</region>\n",
       "<country key=\"US\">USA</country>\n",
       "</address>\n",
       "</affiliation>\n",
       "</author>\n",
       "<author>\n",
       "<persName><forename type=\"first\">Doug</forename><surname>Downey</surname></persName>\n",
       "<email>dougd@allenai.org</email>\n",
       "<affiliation key=\"aff0\">\n",
       "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       "<address>\n",
       "<settlement>Seattle</settlement>\n",
       "<region>WA</region>\n",
       "<country key=\"US\">USA</country>\n",
       "</address>\n",
       "</affiliation>\n",
       "</author>\n",
       "<author>\n",
       "<persName><forename type=\"first\">Noah</forename><forename type=\"middle\">A</forename><surname>Smith</surname></persName>\n",
       "<affiliation key=\"aff0\">\n",
       "<orgName type=\"institution\">Allen Institute for Artificial Intelligence</orgName>\n",
       "<address>\n",
       "<settlement>Seattle</settlement>\n",
       "<region>WA</region>\n",
       "<country key=\"US\">USA</country>\n",
       "</address>\n",
       "</affiliation>\n",
       "<affiliation key=\"aff1\">\n",
       "<orgName type=\"department\">Paul G</orgName>\n",
       "<orgName key=\"instit1\" type=\"institution\">Allen School of Computer Science &amp; Engineering</orgName>\n",
       "<orgName key=\"instit2\" type=\"institution\">University of Washington</orgName>\n",
       "<address>\n",
       "<settlement>Seattle</settlement>\n",
       "<region>WA</region>\n",
       "<country key=\"US\">USA</country>\n",
       "</address>\n",
       "</affiliation>\n",
       "</author>\n",
       "<title level=\"a\" type=\"main\">Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>\n",
       "</analytic>\n",
       "<monogr>\n",
       "<title level=\"m\">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>\n",
       "<meeting>the 58th Annual Meeting of the Association for Computational Linguistics\t\t\t\t\t\t</meeting>\n",
       "<imprint>\n",
       "<publisher>Association for Computational Linguistics</publisher>\n",
       "<date type=\"published\" when=\"2020-05-05\">5 May 2020</date>\n",
       "</imprint>\n",
       "</monogr>\n",
       "<idno type=\"DOI\">10.18653/v1/2020.acl-main.740</idno>\n",
       "<idno type=\"arXiv\">arXiv:2004.10964v3[cs.CL]</idno>\n",
       "</biblStruct>\n",
       "</sourceDesc>\n",
       "</fileDesc>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_info.proccesed_files[2][\"elements\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVER_CONFIG\n",
      "url:\n",
      "  protocol: http\n",
      "  api_domain: yordi111nas.synology.me\n",
      "  port: 8070\n",
      "\n",
      "CLOUD_CONFIG\n",
      "data:\n",
      "  data_dir: data/PDFs\n",
      "  format: .pdf\n",
      "  recursive: true\n",
      "grobid:\n",
      "  cache: true\n",
      "  cache_dir: data/xmls\n",
      "  operation_key: processFulltextDocument\n",
      "  format: .grobid.tei.xml\n",
      "  recursive: true\n",
      "\n",
      "GROBID server is up and running\n",
      "data/xmls\\nlp\\Bert.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\DistillBERT.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\Dont_stop_pretraining.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\GPT-3.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\LIME.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\LoRA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\RoBERTa.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\SORA.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\Transformers.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "data/xmls\\nlp\\word2vec.grobid.tei.xml already exist, skipping... (use --force to reprocess pdf input files)\n",
      "grobid_logger: 2024-04-26 17:48:23,772 | INFO | API.py:44 | 50960 >>> All files have been process by the api\n"
     ]
    }
   ],
   "source": [
    "from pdf_analyzer.api import API\n",
    "from pdf_analyzer.api.extract.elements import extract_element_soup\n",
    "\n",
    "\n",
    "server_config = load_config(\"config/api/grobid-server-config.yaml\")\n",
    "extract_config = load_config(\"config/api/api-base-config.yaml\")\n",
    "print(\"SERVER_CONFIG\\n\"+OmegaConf.to_yaml(server_config))\n",
    "print(\"CLOUD_CONFIG\\n\"+OmegaConf.to_yaml(extract_config))\n",
    "\n",
    "base_api = API.BaseAPI(extract_config,server_config)\n",
    "\n",
    "files = base_api.proccesed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = None\n",
    "element = \"div\"\n",
    "type = \"acknowledgement\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = None\n",
    "element = \"abstract\"\n",
    "type = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p><p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p><p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p><p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p><p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p><p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p><p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p><p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p><p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models <ref target=\"#b35\" type=\"bibr\">(Peters et al., 2018a;</ref><ref target=\"#b37\" type=\"bibr\">Radford et al., 2018)</ref>, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p><p>BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions -something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.</p><p>In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.</p><p>In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.</p><p>In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.</p><p>In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.</p><p>In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.</p><p>In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.</p><p>In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.</p><p>In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.</p><p>In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA. * Equal contribution. 0 Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency. 1 While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   </p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   </p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   </p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   </p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   </p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   </p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   </p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   </p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1   </p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Warning: This is not an official technical report from OpenAI. Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this \"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.</p><p>â€  Work performed while at Google Brain. â€¡ Work performed while at Google Research.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.</p><p>â€  Work performed while at Google Brain. â€¡ Work performed while at Google Research.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.</p><p>â€  Work performed while at Google Brain. â€¡ Work performed while at Google Research.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.</p><p>â€  Work performed while at Google Brain. â€¡ Work performed while at Google Research.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.</p><p>â€  Work performed while at Google Brain. â€¡ Work performed while at Google Research.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.</p><p>â€  Work performed while at Google Brain. â€¡ Work performed while at Google Research.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.</p><p>â€  Work performed while at Google Brain. â€¡ Work performed while at Google Research.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.</p><p>â€  Work performed while at Google Brain. â€¡ Work performed while at Google Research.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.</p><p>â€  Work performed while at Google Brain. â€¡ Work performed while at Google Research.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p></div>\n",
      "</abstract>]\n",
      "[<abstract>\n",
      "<div xmlns=\"http://www.tei-c.org/ns/1.0\"><p>We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.</p></div>\n",
      "</abstract>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "resultados=[]\n",
    "\n",
    "for idx in range(len(files)):\n",
    "    for idy in range(len(files)):\n",
    "        if(idx==idy):\n",
    "            continue\n",
    "\n",
    "        paper_1 = extract_element_soup(files[idx],dir,element,type)\n",
    "        print(paper_1)\n",
    "        resultado_1 =\"\"\n",
    "        for element_text in paper_1:\n",
    "            resultado_1 = resultado_1 + \" \" + element_text.text\n",
    "            \n",
    "        paper_2 = extract_element_soup(files[idy],dir,element,type)\n",
    "\n",
    "        resultado_2 =\"\"\n",
    "        for element_text in paper_2:\n",
    "            resultado_2 = resultado_2 + \" \" + element_text.text\n",
    "        resultados.append((resultado_1,resultado_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for par_texto in resultados:\n",
    "    texto_1 =par_texto[0]\n",
    "    texto_2 =par_texto[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAcknowledgmentsThe authors thank Dallas Card, Mark Neumann, Nelson Liu, Eric Wallace, members of the Al-lenNLP team, and anonymous reviewers for helpful feedback, and Arman Cohan for providing data. This research was supported in part by the Office of Naval Research under the MURI grant N00014-18-1-2670. TPU machines for conducting experiments were provided by Google.\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_element_soup(files[2],dir,element,type)[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"token-classification\", model=\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_final_ner(org_viejo,orgs,type_ent):\n",
    "    org_viejo[\"type\"]=type_ent\n",
    "    if org_viejo[\"score\"] >0.6 and (org_viejo[\"name\"] not in [dict[\"name\"] for dict in orgs]):\n",
    "        orgs.append(org_viejo)\n",
    "    org_temp=dict()\n",
    "    org_temp[\"name\"]=\"\"\n",
    "    org_temp[\"score\"]=0\n",
    "    return org_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ners(files,i,pipe):\n",
    "    \n",
    "    acno = extract_element_soup(files[i],None,\"div\",\"acknowledgement\")\n",
    "    ners=[]\n",
    "    if len(acno)>0:\n",
    "        for elements in acno:\n",
    "            ners.extend(pipe(elements.text))\n",
    "\n",
    "\n",
    "\n",
    "    org_parser=False\n",
    "    misc_parser=False\n",
    "    continue_parser=True\n",
    "\n",
    "    org_temp=\"\"\n",
    "    org_temp=dict()\n",
    "    org_temp[\"name\"]=\"\"\n",
    "    org_temp[\"score\"]=0\n",
    "    org_temp[\"type\"]=\"\"\n",
    "    orgs=[]\n",
    "    pos_anterior=0\n",
    "    for ner in ners:\n",
    "        \n",
    "        \n",
    "        \n",
    "        if org_parser and continue_parser and (ner[\"entity\"]=='B-ORG') and ner[\"start\"]!=pos_anterior:\n",
    "            org_temp=case_final_ner(org_temp,orgs,\"ORG\")\n",
    "            org_parser=False\n",
    "            continue_parser=True\n",
    "        if misc_parser and continue_parser and (ner[\"entity\"]=='B-MISC') and ner[\"start\"]!=pos_anterior:\n",
    "            org_temp=case_final_ner(org_temp,orgs,\"MISC\")\n",
    "            org_parser=False\n",
    "            continue_parser=True \n",
    "            \n",
    "        if org_parser and not continue_parser and (ner[\"entity\"]!='I-ORG'):\n",
    "            org_temp=case_final_ner(org_temp,orgs,\"ORG\")\n",
    "            org_parser=False\n",
    "            continue_parser=True   \n",
    "        \n",
    "            \n",
    "        if misc_parser and not continue_parser and (ner[\"entity\"]!='I-MISC'):\n",
    "            org_temp=case_final_ner(org_temp,orgs,\"MISC\")\n",
    "            misc_parser=False\n",
    "            continue_parser=True\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        if ner[\"entity\"]=='B-ORG' and not org_parser:\n",
    "            org_parser=True\n",
    "            org_temp[\"name\"]=ner[\"word\"]\n",
    "            org_temp[\"score\"]=ner[\"score\"]\n",
    "            pos_anterior=ner[\"end\"]\n",
    "            continue\n",
    "        \n",
    "        if ner[\"entity\"]=='B-ORG' and org_parser and continue_parser:\n",
    "            \n",
    "            org_temp[\"name\"]=org_temp[\"name\"]+ner[\"word\"].replace(\"#\", \"\")\n",
    "            org_temp[\"score\"]=org_temp[\"score\"]*ner[\"score\"]\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if ner[\"entity\"]=='B-MISC' and not misc_parser:\n",
    "            misc_parser=True\n",
    "            \n",
    "            org_temp[\"name\"]=ner[\"word\"]\n",
    "            org_temp[\"score\"]=ner[\"score\"]\n",
    "            pos_anterior=ner[\"end\"]\n",
    "            continue\n",
    "        \n",
    "        if ner[\"entity\"]=='B-MISC' and misc_parser and continue_parser:\n",
    "            \n",
    "            org_temp[\"name\"]=org_temp[\"name\"]+ner[\"word\"].replace(\"#\", \"\")\n",
    "            org_temp[\"score\"]=org_temp[\"score\"]*ner[\"score\"]\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if ner[\"entity\"]=='I-ORG' and org_parser:\n",
    "            \n",
    "            \n",
    "            addit=\"\"\n",
    "            if pos_anterior+1 <= ner[\"start\"]:\n",
    "                addit=addit +\" \"\n",
    "            continue_parser=False\n",
    "            org_temp[\"name\"]=org_temp[\"name\"]+addit+ner[\"word\"].replace(\"#\", \"\")\n",
    "            org_temp[\"score\"]=org_temp[\"score\"]*ner[\"score\"]\n",
    "            pos_anterior=ner[\"end\"]\n",
    "            continue\n",
    "        \n",
    "\n",
    "        if ner[\"entity\"]=='I-MISC' and misc_parser:\n",
    "            \n",
    "            \n",
    "            addit=\"\"\n",
    "            if pos_anterior+1 <= ner[\"start\"]:\n",
    "                addit=addit +\" \"\n",
    "            continue_parser=False\n",
    "            org_temp[\"name\"]=org_temp[\"name\"]+addit+ner[\"word\"].replace(\"#\", \"\")\n",
    "            org_temp[\"score\"]=org_temp[\"score\"]*ner[\"score\"]\n",
    "            pos_anterior=ner[\"end\"]\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if org_parser:\n",
    "            \n",
    "            org_temp=case_final_ner(org_temp,orgs,\"ORG\")\n",
    "            misc_parser=False\n",
    "            continue_parser=True\n",
    "        \n",
    "    \n",
    "        if misc_parser:\n",
    "            org_temp=case_final_ner(org_temp,orgs,\"MISC\")\n",
    "            misc_parser=False\n",
    "            continue_parser=True\n",
    "            \n",
    "        org_parser=False\n",
    "        misc_parser=False\n",
    "        continue_parser=True\n",
    "\n",
    "    if org_parser:\n",
    "        \n",
    "        org_temp=case_final_ner(org_temp,orgs,\"ORG\")\n",
    "        misc_parser=False\n",
    "        continue_parser=True\n",
    "        \n",
    "    \n",
    "    if misc_parser:\n",
    "        org_temp=case_final_ner(org_temp,orgs,\"MISC\")\n",
    "        misc_parser=False\n",
    "        continue_parser=True   \n",
    "    return orgs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_projects(files,i):\n",
    "    \n",
    "    acno = extract_element_soup(files[i],None,\"div\",\"acknowledgement\")\n",
    "    texts =\"\"\n",
    "    if len(acno)>0:\n",
    "        for elements in acno:\n",
    "            texts = texts + \" \"+ elements.text\n",
    "       \n",
    "    return re.findall(r'\\b#?[A-Z\\d-]+(?:-\\d+){3,}\\b', texts),re.findall(r'(\\b[A-Z\\d&-]+\\b)\\s*(?:award[s]?|grant)\\s*#?[A-Z\\d-]+(?:-\\d+){3,}\\b',texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['W911NF-13-1-0246', 'N00014-13-1-0023'], ['ONR'])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_projects(files,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#W911NF-13-1-0246', 'ONR'), ('#N00014-13-1-0023', 'ONR'), ('N00014-18-1-2670', 'MURI')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "texto = '''\n",
    "This work was supported in part by ONR awards #W911NF-13-1-0246 and #N00014-13-1-0023, and in part by TerraSwarm. This research was supported in part by the Office of Naval Research under the MURI grant N00014-18-1-2670.\n",
    "'''\n",
    "\n",
    "# Definimos un patrÃ³n para buscar nombres y sus respectivos identificadores con #\n",
    "patron_awards = r'([A-Z\\d&-]+)\\s*awards\\s*((?:#?[A-Z\\d-]+(?:-\\d+){3,}(?:\\s*and\\s*#?[A-Z\\d-]+(?:-\\d+){3,})*)+)\\b'\n",
    "\n",
    "# Definimos un patrÃ³n para buscar nombres y sus respectivos identificadores con grant\n",
    "patron_grant = r'([A-Z\\d&-]+)\\s*grant\\s*((?:#?[A-Z\\d-]+(?:-\\d+){3,}))\\b'\n",
    "\n",
    "# Encontramos todas las coincidencias con awards\n",
    "resultados_awards = re.findall(patron_awards, texto)\n",
    "\n",
    "# Encontramos todas las coincidencias con grant\n",
    "resultados_grant = re.findall(patron_grant, texto)\n",
    "\n",
    "# Creamos una lista de tuplas para almacenar los resultados\n",
    "resultado_final = []\n",
    "\n",
    "# FunciÃ³n para agregar resultados a la lista final\n",
    "def agregar_resultados(resultados):\n",
    "    for match in resultados:\n",
    "        nombre = match[0]\n",
    "        identificadores = match[1].split(' and ')\n",
    "        for identificador in identificadores:\n",
    "            resultado_final.append((identificador, nombre))\n",
    "\n",
    "# Agregamos los resultados de awards\n",
    "agregar_resultados(resultados_awards)\n",
    "\n",
    "# Agregamos los resultados de grant\n",
    "for match in resultados_grant:\n",
    "    nombre = match[0]\n",
    "    identificador = match[1]\n",
    "    resultado_final.append((identificador, nombre))\n",
    "\n",
    "print(resultado_final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'TerraSwarm', 'score': 0.99291795, 'type': 'ORG'},\n",
       " {'name': 'STARnet', 'score': 0.87451047, 'type': 'ORG'},\n",
       " {'name': 'Semiconductor Research Corporation',\n",
       "  'score': 0.9271029,\n",
       "  'type': 'ORG'},\n",
       " {'name': 'MARCO', 'score': 0.9814992, 'type': 'ORG'},\n",
       " {'name': 'DARPA', 'score': 0.976103, 'type': 'ORG'}]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_ners(files,i,pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks to Jakub Pachocki and Szymon Sidor for suggesting tasks, and Greg Brockman, Michael Petrov, Brooke Chan, and Chelsea Voss for helping run evaluations on OpenAI's infrastructure. Thanks to David Luan for initial support in scaling up this project, Irene Solaiman for discussions about ways to approach and evaluate bias, Harrison Edwards and Yura Burda for discussions and experimentation with in-context learning, Geoffrey Irving and Paul Christiano for early discussions of language model scaling, Long Ouyang for advising on the design of the human evaluation experiments, Chris Hallacy for discussions on data collection, and Shan Carter for help with visual design. Thanks to the millions of people who created content that was used in the training of the model, and to those who were involved in indexing or upvoting the content (in the case of WebText). Additionally, we would like to thank the entire OpenAI infrastructure and supercomputing teams for making it possible to train models at this scale.\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_element_soup(files[i],dir,element,type)[0].p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': 0.99979836,\n",
       "  'index': 15,\n",
       "  'word': 'Ryan',\n",
       "  'start': 49,\n",
       "  'end': 53},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9997751,\n",
       "  'index': 16,\n",
       "  'word': 'Lowe',\n",
       "  'start': 54,\n",
       "  'end': 58},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9995851,\n",
       "  'index': 30,\n",
       "  'word': 'J',\n",
       "  'start': 122,\n",
       "  'end': 123},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.93610704,\n",
       "  'index': 31,\n",
       "  'word': '##ak',\n",
       "  'start': 123,\n",
       "  'end': 125},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.994306,\n",
       "  'index': 32,\n",
       "  'word': '##ub',\n",
       "  'start': 125,\n",
       "  'end': 127},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9998161,\n",
       "  'index': 33,\n",
       "  'word': 'Pac',\n",
       "  'start': 128,\n",
       "  'end': 131},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99935037,\n",
       "  'index': 34,\n",
       "  'word': '##ho',\n",
       "  'start': 131,\n",
       "  'end': 133},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99779344,\n",
       "  'index': 35,\n",
       "  'word': '##cki',\n",
       "  'start': 133,\n",
       "  'end': 136},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9995432,\n",
       "  'index': 37,\n",
       "  'word': 'S',\n",
       "  'start': 141,\n",
       "  'end': 142},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9842854,\n",
       "  'index': 38,\n",
       "  'word': '##zy',\n",
       "  'start': 142,\n",
       "  'end': 144},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9810836,\n",
       "  'index': 39,\n",
       "  'word': '##mon',\n",
       "  'start': 144,\n",
       "  'end': 147},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99975055,\n",
       "  'index': 40,\n",
       "  'word': 'Sid',\n",
       "  'start': 148,\n",
       "  'end': 151},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.96134436,\n",
       "  'index': 41,\n",
       "  'word': '##or',\n",
       "  'start': 151,\n",
       "  'end': 153},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.99978906,\n",
       "  'index': 47,\n",
       "  'word': 'Greg',\n",
       "  'start': 180,\n",
       "  'end': 184},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99977845,\n",
       "  'index': 48,\n",
       "  'word': 'Brock',\n",
       "  'start': 185,\n",
       "  'end': 190},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.999348,\n",
       "  'index': 49,\n",
       "  'word': '##man',\n",
       "  'start': 190,\n",
       "  'end': 193},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9998023,\n",
       "  'index': 51,\n",
       "  'word': 'Michael',\n",
       "  'start': 195,\n",
       "  'end': 202},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9997756,\n",
       "  'index': 52,\n",
       "  'word': 'Pet',\n",
       "  'start': 203,\n",
       "  'end': 206},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99942243,\n",
       "  'index': 53,\n",
       "  'word': '##rov',\n",
       "  'start': 206,\n",
       "  'end': 209},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.99975866,\n",
       "  'index': 55,\n",
       "  'word': 'Brooke',\n",
       "  'start': 211,\n",
       "  'end': 217},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99973685,\n",
       "  'index': 56,\n",
       "  'word': 'Chan',\n",
       "  'start': 218,\n",
       "  'end': 222},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9997445,\n",
       "  'index': 59,\n",
       "  'word': 'Chelsea',\n",
       "  'start': 228,\n",
       "  'end': 235},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9996543,\n",
       "  'index': 60,\n",
       "  'word': 'V',\n",
       "  'start': 236,\n",
       "  'end': 237},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9994543,\n",
       "  'index': 61,\n",
       "  'word': '##oss',\n",
       "  'start': 237,\n",
       "  'end': 240},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.99268204,\n",
       "  'index': 68,\n",
       "  'word': 'Open',\n",
       "  'start': 272,\n",
       "  'end': 276},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.97998464,\n",
       "  'index': 69,\n",
       "  'word': '##A',\n",
       "  'start': 276,\n",
       "  'end': 277},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9895727,\n",
       "  'index': 70,\n",
       "  'word': '##I',\n",
       "  'start': 277,\n",
       "  'end': 278},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.99977833,\n",
       "  'index': 77,\n",
       "  'word': 'David',\n",
       "  'start': 307,\n",
       "  'end': 312},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99973184,\n",
       "  'index': 78,\n",
       "  'word': 'Lu',\n",
       "  'start': 313,\n",
       "  'end': 315},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.991676,\n",
       "  'index': 79,\n",
       "  'word': '##an',\n",
       "  'start': 315,\n",
       "  'end': 317},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9997328,\n",
       "  'index': 91,\n",
       "  'word': 'Irene',\n",
       "  'start': 366,\n",
       "  'end': 371},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99973327,\n",
       "  'index': 92,\n",
       "  'word': 'Sol',\n",
       "  'start': 372,\n",
       "  'end': 375},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99529296,\n",
       "  'index': 93,\n",
       "  'word': '##ai',\n",
       "  'start': 375,\n",
       "  'end': 377},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99432445,\n",
       "  'index': 94,\n",
       "  'word': '##man',\n",
       "  'start': 377,\n",
       "  'end': 380},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9997532,\n",
       "  'index': 105,\n",
       "  'word': 'Harrison',\n",
       "  'start': 439,\n",
       "  'end': 447},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9996686,\n",
       "  'index': 106,\n",
       "  'word': 'Edwards',\n",
       "  'start': 448,\n",
       "  'end': 455},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9996786,\n",
       "  'index': 108,\n",
       "  'word': 'Yu',\n",
       "  'start': 460,\n",
       "  'end': 462},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.99922764,\n",
       "  'index': 109,\n",
       "  'word': '##ra',\n",
       "  'start': 462,\n",
       "  'end': 464},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99965084,\n",
       "  'index': 110,\n",
       "  'word': 'B',\n",
       "  'start': 465,\n",
       "  'end': 466},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.96306777,\n",
       "  'index': 111,\n",
       "  'word': '##ur',\n",
       "  'start': 466,\n",
       "  'end': 468},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9759704,\n",
       "  'index': 112,\n",
       "  'word': '##da',\n",
       "  'start': 468,\n",
       "  'end': 470},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.99976736,\n",
       "  'index': 123,\n",
       "  'word': 'Geoffrey',\n",
       "  'start': 533,\n",
       "  'end': 541},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99972445,\n",
       "  'index': 124,\n",
       "  'word': 'Irving',\n",
       "  'start': 542,\n",
       "  'end': 548},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.99974984,\n",
       "  'index': 126,\n",
       "  'word': 'Paul',\n",
       "  'start': 553,\n",
       "  'end': 557},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9997532,\n",
       "  'index': 127,\n",
       "  'word': 'Christian',\n",
       "  'start': 558,\n",
       "  'end': 567},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.92473614,\n",
       "  'index': 128,\n",
       "  'word': '##o',\n",
       "  'start': 567,\n",
       "  'end': 568},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.99978465,\n",
       "  'index': 139,\n",
       "  'word': 'Long',\n",
       "  'start': 618,\n",
       "  'end': 622},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9996934,\n",
       "  'index': 140,\n",
       "  'word': 'O',\n",
       "  'start': 623,\n",
       "  'end': 624},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99843997,\n",
       "  'index': 141,\n",
       "  'word': '##uy',\n",
       "  'start': 624,\n",
       "  'end': 626},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99650395,\n",
       "  'index': 142,\n",
       "  'word': '##ang',\n",
       "  'start': 626,\n",
       "  'end': 629},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9997676,\n",
       "  'index': 154,\n",
       "  'word': 'Chris',\n",
       "  'start': 694,\n",
       "  'end': 699},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9997309,\n",
       "  'index': 155,\n",
       "  'word': 'Hall',\n",
       "  'start': 700,\n",
       "  'end': 704},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99950194,\n",
       "  'index': 156,\n",
       "  'word': '##acy',\n",
       "  'start': 704,\n",
       "  'end': 707},\n",
       " {'entity': 'B-PER',\n",
       "  'score': 0.9997645,\n",
       "  'index': 164,\n",
       "  'word': 'Shan',\n",
       "  'start': 748,\n",
       "  'end': 752},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99975485,\n",
       "  'index': 165,\n",
       "  'word': 'Carter',\n",
       "  'start': 753,\n",
       "  'end': 759},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.98398834,\n",
       "  'index': 211,\n",
       "  'word': 'Web',\n",
       "  'start': 969,\n",
       "  'end': 972},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9741401,\n",
       "  'index': 212,\n",
       "  'word': '##T',\n",
       "  'start': 972,\n",
       "  'end': 973},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.91015553,\n",
       "  'index': 213,\n",
       "  'word': '##ex',\n",
       "  'start': 973,\n",
       "  'end': 975},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.81520927,\n",
       "  'index': 214,\n",
       "  'word': '##t',\n",
       "  'start': 975,\n",
       "  'end': 976},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.980161,\n",
       "  'index': 226,\n",
       "  'word': 'Open',\n",
       "  'start': 1027,\n",
       "  'end': 1031},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.94922477,\n",
       "  'index': 227,\n",
       "  'word': '##A',\n",
       "  'start': 1031,\n",
       "  'end': 1032},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.96556985,\n",
       "  'index': 228,\n",
       "  'word': '##I',\n",
       "  'start': 1032,\n",
       "  'end': 1033}]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acno = extract_element_soup(files[i],None,\"div\",\"acknowledgement\")\n",
    "ners=[]\n",
    "if len(acno)>0:\n",
    "    for elements_text in acno:\n",
    "        ners.extend(pipe(elements_text.text))\n",
    "ners"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
